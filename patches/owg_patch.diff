diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/0demo.py OWG-main/0demo.py
--- OWG-upstream/0demo.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/0demo.py	2025-10-25 18:41:21.900405901 +0800
@@ -0,0 +1,46 @@
+from owg_robot.ui import RobotEnvUI
+from owg.utils.config import load_config
+import argparse
+import os, sys
+
+# å°† grconvnet ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„
+grconvnet_path = os.path.join(os.path.dirname(__file__), "third_party/grconvnet")
+if grconvnet_path not in sys.path:
+    sys.path.insert(0, grconvnet_path)
+
+# ========== âœ… æ–°å¢å‘½ä»¤è¡Œå‚æ•° ==========
+parser = argparse.ArgumentParser(description="Run OWG demo without Tkinter popup.")
+parser.add_argument('--n_objects', type=int, help='Number of objects to load', default=None)
+parser.add_argument('--seed', type=int, help='Random seed', default=None)
+parser.add_argument('--vis', type=int, help='Enable PyBullet visualizer', default=None)
+parser.add_argument('--verbose', type=int, help='Set verbosity level', default=None)
+parser.add_argument('--prompt', type=str, help='Language instruction for the grasp task', default="grasp the red cup")
+kwargs = vars(parser.parse_args())
+
+# ========== âœ… åŠ è½½é…ç½®æ–‡ä»¶ ==========
+cfg = load_config('./config/pyb/env.yaml')
+cfg.n_objects = kwargs['n_objects'] or cfg.n_objects
+cfg.seed = kwargs['seed'] or cfg.seed
+cfg.policy.vis = kwargs['vis'] or cfg.policy.vis
+cfg.policy.verbose = kwargs['verbose'] or cfg.policy.verbose
+
+# âœ… å°† prompt æ³¨å…¥é…ç½®ï¼ˆç»™åç»­ ui.py ä½¿ç”¨ï¼‰
+cfg.prompt = kwargs['prompt']
+print("\n=== Loaded Configuration ===")
+print(cfg)
+print("============================\n")
+
+# ========== âœ… è¿è¡Œ UI æ§åˆ¶å™¨ ==========
+demo = RobotEnvUI(cfg)
+
+# ğŸ”¹ Monkey patch: ç›´æ¥æ³¨å…¥ promptï¼Œé¿å…è°ƒç”¨ Tkinter å¼¹çª—
+if hasattr(demo, "user_input") is False:
+    demo.user_input = cfg.prompt
+
+if hasattr(demo, "set_user_prompt"):
+    demo.set_user_prompt(cfg.prompt)
+
+print(f"[INFO] Using prompt: \"{cfg.prompt}\"")
+
+demo.run()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/ai_yyds_request.py OWG-main/ai_yyds_request.py
--- OWG-upstream/ai_yyds_request.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/ai_yyds_request.py	2025-10-17 22:41:57.174873287 +0800
@@ -0,0 +1,26 @@
+import requests
+
+# è®¾ç½® API å¯†é’¥
+api_key = 'sk-A9q5TscQFLIV7ZTAB29f5c93E1D44f4880F91c24FcAa4eDd'  # è¯·æ›¿æ¢ä¸ºä½ çš„ AI-YYDS API å¯†é’¥
+
+headers = {
+    'Authorization': f'Bearer {api_key}',
+    'Content-Type': 'application/json'
+}
+
+data = {
+    'model': 'gpt-4o',  # å¯ä»¥æ¢æˆå…¶ä»–æ”¯æŒçš„æ¨¡å‹
+    'messages': [
+        {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚'},
+        {'role': 'user', 'content': 'ä½ å¥½ï¼ŒAIï¼'}
+    ]
+}
+
+response = requests.post('https://api.ai-yyds.com/v1/chat/completions', json=data, headers=headers)
+
+if response.status_code == 200:
+    result = response.json()
+    print(f"AI å›å¤: {result['choices'][0]['message']['content']}")
+else:
+    print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{response.text}")
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/analysis_logs.py OWG-main/analysis_logs.py
--- OWG-upstream/analysis_logs.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/analysis_logs.py	2025-11-24 23:09:27.862670650 +0800
@@ -0,0 +1,67 @@
+# analysis_logs.py
+# ç”¨äºåˆ†æ logs/ui_nl_exec.jsonl çš„ grasp æˆåŠŸç‡ï¼ˆæ•´ä½“ + æŒ‰ç‰©ä½“ï¼‰
+
+import json
+from collections import Counter
+from pathlib import Path
+
+LOG_PATH = Path("logs/ui_nl_exec.jsonl")
+
+# ä½ å…³å¿ƒçš„å‡ ä¸ªç‰©ä½“å…³é”®è¯ï¼ˆå’Œä½ å®éªŒä¿æŒä¸€è‡´ï¼‰
+TARGET_QUERIES = [
+    "campbell's soup can",
+    "hammer",
+    "scissors",
+]
+
+def load_rows(path=LOG_PATH):
+    rows = []
+    with open(path, "r") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            rows.append(json.loads(line))
+    return rows
+
+def stat(rows, filter_fn, name):
+    subset = [r for r in rows if filter_fn(r)]
+    if not subset:
+        print(f"{name}: no data")
+        return
+    succ = sum(1 for r in subset if r.get("success"))
+    rate = succ / len(subset)
+    print(f"{name:30s}: {succ:3d}/{len(subset):3d} = {rate:.2f}")
+
+def main():
+    print(f"[INFO] Loading logs from {LOG_PATH} ...")
+    rows = load_rows()
+    print(f"[INFO] Total records: {len(rows)}\n")
+
+    # 1) æ€»ä½“æˆåŠŸç‡
+    stat(rows, lambda r: not r.get("use_lggsn", False), "Baseline - ALL")
+    stat(rows, lambda r: r.get("use_lggsn", False),     "With LGGSN - ALL")
+    print()
+
+    # 2) ç»Ÿè®¡æ¯ä¸ª queryï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
+    for q in TARGET_QUERIES:
+        stat(rows,
+             lambda r, q=q: r["query"].strip().lower() == q
+                           and not r.get("use_lggsn", False),
+             f"Baseline - {q}")
+        stat(rows,
+             lambda r, q=q: r["query"].strip().lower() == q
+                           and r.get("use_lggsn", False),
+             f"With LGGSN - {q}")
+        print()
+
+    # 3)ï¼ˆå¯é€‰ï¼‰çœ‹çœ‹æœ€å¸¸è§çš„ query æ˜¯ä»€ä¹ˆ
+    qs = [r["query"].strip().lower() for r in rows]
+    cnt = Counter(qs)
+    print("Top queries:")
+    for q, c in cnt.most_common(10):
+        print(f"  {c:3d}x  {q!r}")
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/analysis_semantic_pc.py OWG-main/analysis_semantic_pc.py
--- OWG-upstream/analysis_semantic_pc.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/analysis_semantic_pc.py	2025-11-25 13:16:55.250026004 +0800
@@ -0,0 +1,74 @@
+# analysis_semantic_pc.py
+import numpy as np
+import glob
+from collections import Counter
+import os
+
+SEM_DIR = "semantic_pc"
+
+def load_all_semantic_pc(sem_dir=SEM_DIR):
+    paths = sorted(glob.glob(os.path.join(sem_dir, "*.npz")))
+    print(f"[INFO] Found {len(paths)} semantic_pc samples in {sem_dir}")
+    data = []
+    for p in paths:
+        d = np.load(p, allow_pickle=True)
+        pc = d["pc"]                # (N,3)
+        q = str(d["query"]).strip().lower()
+        meta = d["metas"].item() if "metas" in d.files else {}
+        data.append((p, pc, q, meta))
+    return data
+
+# ä½ ä¹‹å‰çš„ 11 ä¸ªç‰©ä½“ï¼Œç”¨å…³é”®è¯ç²—ç•¥å½’ç±»
+KEYWORDS = {
+    "tennis ball": ["tennis"],
+    "hammer": ["hammer"],
+    "blue container": ["blue can", "blue container"],
+    "green clamp": ["clamp"],
+    "eraser": ["eraser"],
+    "pringles can": ["pringles", "chips"],
+    "scissors": ["scissors"],
+    "mustard bottle": ["mustard", "yellow bottle"],
+    "soup can": ["campbell", "soup"],
+    "cheez-it box": ["cheez", "cracker box", "cheez-it"],
+    "rectangular tin": ["tin", "green box"],
+}
+
+def match_category(query: str) -> str:
+    for name, kws in KEYWORDS.items():
+        if any(kw in query for kw in kws):
+            return name
+    return "other"
+
+def main():
+    data = load_all_semantic_pc()
+    if not data:
+        print("[WARN] No semantic_pc found.")
+        return
+
+    # 1) ç‚¹æ•°ç»Ÿè®¡
+    lens = [pc.shape[0] for _, pc, _, _ in data]
+    print("\n[STATS] Points per semantic point cloud:")
+    print("  N samples   :", len(lens))
+    print("  min points  :", min(lens))
+    print("  mean points :", sum(lens) / len(lens))
+    print("  max points  :", max(lens))
+
+    # 2) æŒ‰ç‰©ä½“ç±»åˆ«ç»Ÿè®¡
+    cats = [match_category(q) for _, _, q, _ in data]
+    cnt = Counter(cats)
+    print("\n[STATS] Per-object sample counts:")
+    for name, num in cnt.items():
+        print(f"  {name:15s}: {num}")
+
+    # 3) éšæœºçœ‹å‡ æ¡ç¤ºä¾‹ï¼ˆæ–¹ä¾¿ä½ æ‰‹åŠ¨ sanity checkï¼‰
+    print("\n[EXAMPLES]")
+    for i, (path, pc, q, meta) in enumerate(data[:5]):
+        print(f"  {i+1}. {os.path.basename(path)}")
+        print(f"     query = {q}")
+        print(f"     pc.shape = {pc.shape}")
+        if meta:
+            print(f"     meta keys = {list(meta.keys())}")
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/analyze_ranking_vs_random.py OWG-main/analyze_ranking_vs_random.py
--- OWG-upstream/analyze_ranking_vs_random.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/analyze_ranking_vs_random.py	2025-11-29 21:26:01.988420012 +0800
@@ -0,0 +1,156 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+"""
+Ranking vs Random for geometric grasps on YCB objects.
+
+For each validated grasps file data/grasps_val/*_geom_val.json:
+- Load the list of grasps (each has 'success' and usually 'score').
+- Compute SR@k when grasps are sorted by 'score' (geom ranking).
+- Compute SR@k when grasps are in random order, averaged over many shuffles.
+Outputs:
+- Prints per-object SR@k (geom vs random) for k in {1,3,5}.
+- Prints average improvement across objects.
+- Saves a figure 'results/ranking_vs_random.png' with mean SR@k curves.
+"""
+
+import glob
+import json
+import os
+import random
+from pathlib import Path
+
+import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+
+# top-k list
+K_LIST = [1, 3, 5, 10, 20, 32, 64]
+N_SHUFFLES = 50  # how many random permutations per object
+
+
+def load_grasps(path):
+    with open(path, "r", encoding="utf-8") as f:
+        data = json.load(f)
+    if not isinstance(data, list):
+        raise ValueError(f"{path} does not contain a list")
+    return data
+
+
+def sr_at_k(success_array, k):
+    """success_array: 1D numpy array of 0/1"""
+    n = len(success_array)
+    kk = min(k, n)
+    if kk <= 0:
+        return 0.0
+    return float(success_array[:kk].mean())
+
+
+def compute_geom_and_random_sr(grasps, k_list, n_shuffles=50):
+    """
+    grasps: list of dicts with 'success' and optionally 'score'.
+    Returns:
+        sr_geom: dict{k: value}
+        sr_rand_mean: dict{k: value}
+        sr_rand_std: dict{k: value}
+    """
+    n = len(grasps)
+    if n == 0:
+        zero = {k: 0.0 for k in k_list}
+        return zero, zero, zero
+
+    # 1) Geom ranking: sort by 'score' descending if exists, otherwise keep as is
+    if "score" in grasps[0]:
+        sorted_grasps = sorted(grasps, key=lambda g: g.get("score", 0.0), reverse=True)
+    else:
+        sorted_grasps = list(grasps)
+
+    succ_geom = np.array([1.0 if g.get("success") else 0.0 for g in sorted_grasps], dtype=float)
+    sr_geom = {k: sr_at_k(succ_geom, k) for k in k_list}
+
+    # 2) Random ranking: shuffle multiple times, average SR@k
+    succ_orig = np.array([1.0 if g.get("success") else 0.0 for g in grasps], dtype=float)
+    sr_rand_all = {k: [] for k in k_list}
+
+    idx = np.arange(n)
+    for _ in range(n_shuffles):
+        np.random.shuffle(idx)
+        shuffled = succ_orig[idx]
+        for k in k_list:
+            sr_rand_all[k].append(sr_at_k(shuffled, k))
+
+    sr_rand_mean = {k: float(np.mean(sr_rand_all[k])) for k in k_list}
+    sr_rand_std = {k: float(np.std(sr_rand_all[k])) for k in k_list}
+    return sr_geom, sr_rand_mean, sr_rand_std
+
+
+def main():
+    val_files = sorted(glob.glob("data/grasps_val/*_geom_val.json"))
+    if not val_files:
+        print("[ERROR] No files matched data/grasps_val/*_geom_val.json")
+        return
+
+    rows = []
+    print(f"[INFO] Found {len(val_files)} validated grasp files.")
+
+    for f in val_files:
+        obj_id = Path(f).stem.replace("_geom_val", "")
+        grasps = load_grasps(f)
+        sr_geom, sr_rand_mean, sr_rand_std = compute_geom_and_random_sr(
+            grasps, K_LIST, n_shuffles=N_SHUFFLES
+        )
+
+        row = {"obj_id": obj_id}
+        for k in K_LIST:
+            row[f"geom_SR@{k}"] = sr_geom[k]
+            row[f"rand_SR@{k}"] = sr_rand_mean[k]
+            row[f"rand_std@{k}"] = sr_rand_std[k]
+        rows.append(row)
+
+    df = pd.DataFrame(rows).sort_values("obj_id").reset_index(drop=True)
+
+    os.makedirs("results", exist_ok=True)
+    out_csv = "results/ycb_ranking_vs_random.csv"
+    df.to_csv(out_csv, index=False)
+    print(f"[INFO] Saved per-object SR@k (geom vs random) to {out_csv}\n")
+
+    # æ‰“å°æ¯ä¸ªç‰©ä½“åœ¨ k=1,3,5 çš„å¯¹æ¯”
+    print("Per-object SR@k (geom vs random), k in {1,3,5}:")
+    display_cols = ["obj_id"]
+    for k in [1, 3, 5]:
+        display_cols += [f"geom_SR@{k}", f"rand_SR@{k}"]
+    print(df[display_cols].to_string(index=False))
+
+    # è®¡ç®—è·¨ç‰©ä½“å¹³å‡
+    mean_row = {"obj_id": "MEAN"}
+    for k in K_LIST:
+        mean_row[f"geom_SR@{k}"] = df[f"geom_SR@{k}"].mean()
+        mean_row[f"rand_SR@{k}"] = df[f"rand_SR@{k}"].mean()
+    df_mean = pd.DataFrame([mean_row])
+
+    print("\nMean SR@k across objects:")
+    cols_mean = ["obj_id"]
+    for k in [1, 3, 5, 10, 20, 32, 64]:
+        cols_mean += [f"geom_SR@{k}", f"rand_SR@{k}"]
+    print(df_mean[cols_mean].to_string(index=False))
+
+    # ç”»ä¸€å¼ å¹³å‡ SR@k æ›²çº¿å›¾ï¼ˆæ‰€æœ‰ç‰©ä½“å¹³å‡ï¼‰
+    ks = K_LIST
+    geom_mean = [mean_row[f"geom_SR@{k}"] for k in ks]
+    rand_mean = [mean_row[f"rand_SR@{k}"] for k in ks]
+
+    plt.figure()
+    plt.plot(ks, geom_mean, marker="o", label="Geom ranking")
+    plt.plot(ks, rand_mean, marker="s", linestyle="--", label="Random ranking")
+    plt.xlabel("k")
+    plt.ylabel("SR@k")
+    plt.title("Average SR@k across YCB objects (geom vs random ranking)")
+    plt.grid(True, linestyle="--", alpha=0.3)
+    plt.legend()
+    out_fig = "results/ranking_vs_random.png"
+    plt.savefig(out_fig, dpi=200, bbox_inches="tight")
+    print(f"\n[INFO] Saved ranking vs random figure to {out_fig}")
+
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/analyze_ycb_grasps.py OWG-main/analyze_ycb_grasps.py
--- OWG-upstream/analyze_ycb_grasps.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/analyze_ycb_grasps.py	2025-11-29 10:58:17.125156250 +0800
@@ -0,0 +1,108 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+"""
+åˆ†æ YCB å¤šç‰©ä½“ 6-DoF å‡ ä½•æŠ“å–ç»“æœï¼š
+- ä» data/grasps_val/*_geom_val.json é‡Œè¯»å–æ¯ä¸ªæŠ“å–çš„ success
+- æŒ‰ score é™åºè®¡ç®— SR@kï¼ˆk = 1,3,5,10,20,32,64ï¼‰
+- æ±‡æ€»æˆ results/ycb_sr_table.csv
+- å¯é€‰ï¼šç”»ä¸€å¼  SR@k æ›²çº¿å›¾ results/ycb_sr_curves.png
+"""
+
+import os
+import json
+import glob
+from pathlib import Path
+
+import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+
+# æƒ³çœ‹çš„ k å€¼
+K_LIST = [1, 3, 5, 10, 20, 32, 64]
+
+
+def compute_sr_for_file(json_path: str) -> dict:
+    """å¯¹å•ä¸ª *_geom_val.json è®¡ç®—æ•´ä½“æˆåŠŸç‡å’Œ SR@k"""
+    with open(json_path, "r", encoding="utf-8") as f:
+        grasps = json.load(f)
+
+    if not isinstance(grasps, list):
+        raise ValueError(f"{json_path} is not a list")
+
+    n = len(grasps)
+    if n == 0:
+        return {"n_grasps": 0, "success_rate_json": 0.0, **{f"SR@{k}": 0.0 for k in K_LIST}}
+
+    # æŒ‰ score é™åºï¼ˆå¦‚æœæœ‰ï¼‰
+    if "score" in grasps[0]:
+        grasps = sorted(grasps, key=lambda g: g.get("score", 0.0), reverse=True)
+
+    succ = np.array([1.0 if g.get("success") else 0.0 for g in grasps], dtype=float)
+    overall = float(succ.mean())
+
+    res = {"n_grasps": int(n), "success_rate_json": overall}
+    for k in K_LIST:
+        kk = min(k, n)
+        res[f"SR@{k}"] = float(succ[:kk].mean()) if kk > 0 else 0.0
+    return res
+
+
+def main():
+    # 1) æ‰¾åˆ°æ‰€æœ‰ *_geom_val.json
+    val_files = sorted(glob.glob("data/grasps_val/*_geom_val.json"))
+    if not val_files:
+        print("[ERROR] No files matched data/grasps_val/*_geom_val.json")
+        return
+
+    rows = []
+    for f in val_files:
+        stem = Path(f).stem  # e.g. "cracker_box_geom_val"
+        obj_id = stem.replace("_geom_val", "")
+        stats = compute_sr_for_file(f)
+        stats["obj_id"] = obj_id
+        stats["file"] = f
+        rows.append(stats)
+
+    df_sr = pd.DataFrame(rows)
+    df_sr = df_sr.sort_values("obj_id").reset_index(drop=True)
+
+    # 2) å¦‚æœæœ‰ summary_ycb.csvï¼ŒæŠŠ summary çš„æ•´ä½“æˆåŠŸç‡ä¹Ÿ merge è¿›æ¥
+    summary_path = "results/summary_ycb.csv"
+    if os.path.exists(summary_path):
+        df_sum = pd.read_csv(summary_path)
+        # ä» obj è·¯å¾„é‡Œæå–ç®€å•åå­—ï¼Œæ¯”å¦‚ "YCB_Dataset/ycb/cracker_box.urdf" -> "cracker_box"
+        df_sum["obj_id"] = df_sum["obj"].apply(
+            lambda s: Path(str(s)).stem if isinstance(s, str) else str(s)
+        )
+        df_sum_small = df_sum[["obj_id", "success_rate"]].rename(
+            columns={"success_rate": "success_rate_summary"}
+        )
+        df_sr = df_sr.merge(df_sum_small, on="obj_id", how="left")
+
+    os.makedirs("results", exist_ok=True)
+    out_csv = "results/ycb_sr_table.csv"
+    df_sr.to_csv(out_csv, index=False)
+    print(f"[INFO] Saved SR table to {out_csv}\n")
+    print(df_sr.to_string(index=False))
+
+    # 3) ç”» SR@k æ›²çº¿å›¾ï¼ˆæ‰€æœ‰ç‰©ä½“å…±ç”¨ä¸€å¼ ï¼‰
+    try:
+        plt.figure()
+        for _, row in df_sr.iterrows():
+            y = [row[f"SR@{k}"] for k in K_LIST]
+            plt.plot(K_LIST, y, marker="o", label=row["obj_id"])
+        plt.xlabel("k")
+        plt.ylabel("SR@k (success rate)")
+        plt.title("YCB objects: SR@k curves (geometric grasps)")
+        plt.legend()
+        plt.grid(True, linestyle="--", alpha=0.3)
+        out_fig = "results/ycb_sr_curves.png"
+        plt.savefig(out_fig, dpi=200, bbox_inches="tight")
+        print(f"[INFO] Saved SR@k curves figure to {out_fig}")
+    except Exception as e:
+        print(f"[WARN] Failed to plot SR@k curves: {e}")
+
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/analyze_ycb_tuning.py OWG-main/analyze_ycb_tuning.py
--- OWG-upstream/analyze_ycb_tuning.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/analyze_ycb_tuning.py	2025-11-29 21:06:14.985272190 +0800
@@ -0,0 +1,80 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+
+"""
+åˆ†æ results/summary_ycb_tune.csvï¼š
+- å¯¹æ¯ä¸ª YCB ç‰©ä½“ï¼Œæ‰¾åˆ° success_rate æœ€å¤§çš„é‚£ä¸€ç»„ (ds, vc, sq) ä½œä¸º tuned
+- æ‰¾åˆ°é»˜è®¤å‚æ•°ç»„ (0.0015, 0.8, 0.8) çš„ success_rate ä½œä¸º default
+- è¾“å‡º ycb_tuning_results.csvï¼ŒåŒ…å«ï¼šobj_id, SR_default, SR_tuned, best_ds, best_vc, best_sq, delta
+"""
+
+import pandas as pd
+from pathlib import Path
+
+SUMMARY_TUNE = "results/summary_ycb_tune.csv"
+
+# é»˜è®¤å‚æ•°ï¼ˆè¦å’Œè„šæœ¬é‡Œç”¨çš„ä¸€è‡´ï¼‰
+DS_DEFAULT = 0.0015
+VC_DEFAULT = 0.8
+SQ_DEFAULT = 0.8
+
+def main():
+    df = pd.read_csv(SUMMARY_TUNE)
+
+    # ä» obj è·¯å¾„ä¸­æŠ½å–ç®€å•å¯¹è±¡åï¼ˆä¾‹å¦‚ "YCB_Dataset/ycb/cracker_box.urdf" -> "cracker_box"ï¼‰
+    df["obj_id"] = df["obj"].apply(lambda s: Path(str(s)).stem.replace(".urdf", ""))
+
+    # ä¿ç•™æˆ‘ä»¬å…³å¿ƒçš„åˆ—
+    # ä½ å¯ä»¥å…ˆ print(df.columns) çœ‹ä¸€çœ¼ï¼Œå¦‚æœåˆ—åä¸åŒå°±ç¨å¾®æ”¹ä¸€ä¸‹
+    cols_needed = ["obj_id", "success_rate", "descent_step", "vel_close", "squeeze"]
+    for c in cols_needed:
+        if c not in df.columns:
+            raise ValueError(f"Column '{c}' not found in {SUMMARY_TUNE}, actual columns: {df.columns.tolist()}")
+
+    df_small = df[cols_needed].copy()
+
+    results = []
+
+    for obj_id, group in df_small.groupby("obj_id"):
+        # æ‰¾ tunedï¼šsuccess_rate æœ€å¤§çš„é‚£ä¸€è¡Œ
+        best_idx = group["success_rate"].idxmax()
+        best_row = group.loc[best_idx]
+
+        SR_tuned = float(best_row["success_rate"])
+        best_ds = float(best_row["descent_step"])
+        best_vc = float(best_row["vel_close"])
+        best_sq = float(best_row["squeeze"])
+
+        # æ‰¾ defaultï¼šå‚æ•°ç­‰äºé»˜è®¤é‚£ä¸€è¡Œ
+        cond = (
+            (abs(group["descent_step"] - DS_DEFAULT) < 1e-9) &
+            (abs(group["vel_close"]   - VC_DEFAULT) < 1e-9) &
+            (abs(group["squeeze"]     - SQ_DEFAULT) < 1e-9)
+        )
+        g_def = group[cond]
+        if len(g_def) == 0:
+            # ä¸‡ä¸€æ²¡æ‰«åˆ°é»˜è®¤å‚æ•°ï¼ˆç†è®ºä¸Šä¸ä¼šï¼‰ï¼Œå°±è®¾æˆ NaN
+            SR_default = float("nan")
+        else:
+            SR_default = float(g_def["success_rate"].iloc[0])
+
+        results.append({
+            "obj_id": obj_id,
+            "SR_default": SR_default,
+            "SR_tuned": SR_tuned,
+            "best_ds": best_ds,
+            "best_vc": best_vc,
+            "best_sq": best_sq,
+            "delta": SR_tuned - SR_default
+        })
+
+    df_res = pd.DataFrame(results).sort_values("obj_id").reset_index(drop=True)
+    print(df_res.to_string(index=False))
+
+    out_path = "results/ycb_tuning_results.csv"
+    df_res.to_csv(out_path, index=False)
+    print(f"\n[INFO] Saved tuning results to {out_path}")
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/compare_geom_random.py OWG-main/compare_geom_random.py
--- OWG-upstream/compare_geom_random.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/compare_geom_random.py	2025-11-29 15:15:57.301046462 +0800
@@ -0,0 +1,27 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+"""
+æŠŠå‡ ä½• (summary_ycb.csv) å’Œ éšæœº (summary_ycb_random.csv) çš„ success_rate åˆå¹¶æˆä¸€å¼ è¡¨
+"""
+
+import pandas as pd
+
+geom = pd.read_csv("results/summary_ycb.csv")
+rand = pd.read_csv("results/summary_ycb_random.csv")
+
+# æå– obj_idï¼šä» obj è·¯å¾„é‡Œå–æ–‡ä»¶å
+geom["obj_id"] = geom["obj"].apply(lambda s: str(s).split("/")[-1].replace(".urdf",""))
+rand["obj_id"] = rand["obj"].apply(lambda s: str(s).split("/")[-1].replace(".urdf",""))
+
+g = geom[["obj_id", "success_rate"]].rename(columns={"success_rate": "SR_geom"})
+r = rand[["obj_id", "success_rate"]].rename(columns={"success_rate": "SR_random"})
+
+df = g.merge(r, on="obj_id", how="inner")
+df["delta"] = df["SR_geom"] - df["SR_random"]
+
+df = df.sort_values("obj_id").reset_index(drop=True)
+print(df.to_string(index=False))
+
+df.to_csv("results/ycb_geom_vs_random.csv", index=False)
+print("\n[INFO] Saved comparison to results/ycb_geom_vs_random.csv")
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/config/pyb/0env.yaml OWG-main/config/pyb/0env.yaml
--- OWG-upstream/config/pyb/0env.yaml	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/config/pyb/0env.yaml	2025-05-11 05:27:58.000000000 +0800
@@ -0,0 +1,24 @@
+n_objects: 11
+
+n_grasp_attempts: 4
+n_action_attempts: 3
+finger_length: 0.06
+seed: 19
+
+camera:
+  center_x: 0.05
+  center_y: -0.52
+  center_z: 1.9
+  target_x: 0.05
+  target_y: -0.52
+  target_z: 0.785
+  znear: 0.2
+  zfar: 2.0
+  img_size: 448
+  fov: 40
+
+policy:
+  config_path: './config/pyb/OWG.yaml'
+  verbose: True
+  vis: True
+  use_grasp_ranker: False
\ No newline at end of file
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/config/pyb/env.yaml OWG-main/config/pyb/env.yaml
--- OWG-upstream/config/pyb/env.yaml	2025-12-25 13:40:46.502903512 +0800
+++ OWG-main/config/pyb/env.yaml	2025-11-23 22:25:54.370081710 +0800
@@ -21,4 +21,14 @@
   config_path: './config/pyb/OWG.yaml'
   verbose: True
   vis: True
-  use_grasp_ranker: False
\ No newline at end of file
+  use_grasp_ranker: True
+
+  # ========= âœ… æ–°å¢éƒ¨åˆ† =========
+  # å¯ç”¨é˜¶æ®µä¸€ï¼šæŠ“å–é‡‡æ · + ç‰©ç†éªŒè¯
+  enable_grasp_sampling: true
+
+  # å¯é€‰å‚æ•°ï¼ˆä½ å¯è‡ªè¡Œè°ƒæ•´ï¼‰
+  grasp_samples: 100        # æŠ“å–é‡‡æ ·æ•°é‡
+  grasp_obj_path: "/home/lina/owg_env/lib/python3.10/site-packages/pybullet_data/sphere_smooth.obj"
+  output_dir: "grasp_6dof/dataset"
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/cornell-randsplit-rgbd-grconvnet3-drop1-ch32 OWG-main/cornell-randsplit-rgbd-grconvnet3-drop1-ch32
--- OWG-upstream/cornell-randsplit-rgbd-grconvnet3-drop1-ch32	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/cornell-randsplit-rgbd-grconvnet3-drop1-ch32	2025-10-17 18:56:55.447448614 +0800
@@ -0,0 +1,2138 @@
+
+
+
+
+
+
+<!DOCTYPE html>
+<html
+  lang="en"
+  
+  data-color-mode="auto" data-light-theme="light" data-dark-theme="dark"
+  data-a11y-animated-images="system" data-a11y-link-underlines="true"
+  
+  >
+
+
+
+
+  <head>
+    <meta charset="utf-8">
+  <link rel="dns-prefetch" href="https://github.githubassets.com">
+  <link rel="dns-prefetch" href="https://avatars.githubusercontent.com">
+  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com">
+  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">
+  <link rel="preconnect" href="https://github.githubassets.com" crossorigin>
+  <link rel="preconnect" href="https://avatars.githubusercontent.com">
+
+  
+
+
+  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/light-44e67b0cd5d5.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/light_high_contrast-b51c2fae25e8.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/dark-cb035ed575b8.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/dark_high_contrast-99e9b1169976.css" /><link data-color-theme="light" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light-44e67b0cd5d5.css" /><link data-color-theme="light_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_high_contrast-b51c2fae25e8.css" /><link data-color-theme="light_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_colorblind-dadcba82130c.css" /><link data-color-theme="light_colorblind_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_colorblind_high_contrast-cdc36145225e.css" /><link data-color-theme="light_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_tritanopia-0ca195e3b5f3.css" /><link data-color-theme="light_tritanopia_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_tritanopia_high_contrast-f9fb5556a83f.css" /><link data-color-theme="dark" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark-cb035ed575b8.css" /><link data-color-theme="dark_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_high_contrast-99e9b1169976.css" /><link data-color-theme="dark_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_colorblind-9541c4141757.css" /><link data-color-theme="dark_colorblind_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_colorblind_high_contrast-bc604fc65912.css" /><link data-color-theme="dark_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_tritanopia-384776200fd7.css" /><link data-color-theme="dark_tritanopia_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_tritanopia_high_contrast-489c70dedd0a.css" /><link data-color-theme="dark_dimmed" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_dimmed-1545a2e9e540.css" /><link data-color-theme="dark_dimmed_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_dimmed_high_contrast-4c1792a987c3.css" />
+
+  <style type="text/css">
+    :root {
+      --tab-size-preference: 4;
+    }
+
+    pre, code {
+      tab-size: var(--tab-size-preference);
+    }
+  </style>
+
+    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-primitives-15839d47b75d.css" />
+    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-a5c85403da8c.css" />
+    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/global-4d11e88b2383.css" />
+    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/github-73fd10e24e0c.css" />
+  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/repository-5d735668c600.css" />
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/code-9c9b8dc61e74.css" />
+
+  
+
+  <script type="application/json" id="client-env">{"locale":"en","featureFlags":["alternate_user_config_repo","api_insights_show_missing_data_banner","attestations_filtering","attestations_sorting","billing_unfiltered_discounts","client_version_header","codespaces_prebuild_region_target_update","contact_sales_locale_utm_medium","contentful_lp_footnotes","copilot_agent_cli_public_preview","copilot_agent_tasks_btn_code_nav","copilot_agent_tasks_btn_code_view","copilot_agent_tasks_btn_code_view_lines","copilot_api_agentic_issue_marshal_yaml","copilot_api_github_draft_update_issue_skill","copilot_bing_search_use_azure_ai_agent_service","copilot_bing_search_use_grounding_ui","copilot_chat_attach_multiple_images","copilot_chat_file_redirect","copilot_chat_reduce_quota_checks","copilot_chat_search_bar_redirect","copilot_chat_selection_attachments","copilot_chat_vision_in_claude","copilot_chat_vision_skip_thread_create","copilot_custom_copilots","copilot_custom_copilots_feature_preview","copilot_duplicate_thread","copilot_extensions_deprecation_notice","copilot_features_raycast_logo","copilot_file_block_ref_matching","copilot_free_to_paid_telem","copilot_ftp_hyperspace_upgrade_prompt","copilot_ftp_settings_upgrade","copilot_ftp_upgrade_to_pro_from_models","copilot_ftp_your_copilot_settings","copilot_generate_commit_message_dry_regenerate","copilot_immersive_structured_model_picker","copilot_no_floating_button","copilot_read_shared_conversation","copilot_show_copilot_sub_issues_button_on_issues_page","copilot_spaces_as_attachments","copilot_spaces_ga","copilot_spark_loading_webgl","copilot_spark_progressive_error_handling","copilot_spark_read_iteration_history_from_git_v2","copilot_spark_single_user_iteration","copilot_spark_use_billing_headers","copilot_spark_write_iteration_history_to_git","copilot_stable_conversation_view","copilot_workbench_agent_seed_tool","copilot_workbench_cache","copilot_workbench_connection_reload_banner","copilot_workbench_preview_analytics","copilot_workbench_refresh_on_wsod","copilot_workbench_skip_repo_on_codespace","copilot_workbench_use_single_prompt","direct_to_salesforce","dotcom_chat_client_side_skills","failbot_report_error_react_apps_on_page","ghas_licensing_card_header_cta","ghost_pilot_confidence_truncation_25","ghost_pilot_confidence_truncation_40","global_search_multi_orgs","hpc_improve_dom_insertion_observer","inp_reduced_threshold","insert_before_patch","issue_fields_report_usage","issues_copilot_cross_repo_assign","issues_react_blur_item_picker_on_close","issues_react_bots_timeline_pagination","issues_react_prohibit_title_fallback","issues_react_remove_placeholders","issues_sticky_sidebar","item_picker_milestone_tsq_migration","kb_convert_to_space","lifecycle_label_name_updates","link_contact_sales_swp_marketo","marketing_pages_search_explore_provider","mcp_registry_install","memex_mwl_filter_field_delimiter","migrate_toasts_to_banners_web_notifications","new_traffic_page_banner","override_pulse_legacy_url","pinned_issue_fields","primer_react_segmented_control_tooltip","primer_react_unified_portal_root","record_sso_banner_metrics","ref_selector_create_tag_dialog","releases_update_ref_selector","remove_child_patch","repos_insights_remove_new_url","sample_network_conn_type","scheduled_reminders_updated_limits","site_homepage_collaborate_video","site_homepage_contentful","site_msbuild_webgl_hero","spark_commit_on_default_branch","spark_force_push_after_checkout","spark_show_data_access_on_publish","spark_sync_repository_after_iteration","viewscreen_sandbox","webp_support","workbench_store_readonly"],"copilotApiOverrideUrl":"https://api.githubcopilot.com"}</script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/high-contrast-cookie-f3788027bd8d.js"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/wp-runtime-e3ee4770eb15.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_oddbird_popover-polyfill_dist_popover-fn_js-468bf7cab607.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_stacktrace-parser_dist_stack-trace-parser_esm_js-node_modules_github_bro-2f4e04-280c10ec004d.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/environment-b4e74adb6411.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_index_mjs-3eee64e5ddf0.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-9ab93471824e.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-c98257dc79a7.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_auto-complete-element_dist_index_js-node_modules_github_catalyst_-0d7d60-ad3a87b2f0eb.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_text-expander-element_dist_index_js-754f5b5e9e7e.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-665e70-ac788066c220.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_markdown-toolbar-element_dist_index_js-d41270eb61be.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_view-co-777ce2-9ec8c103bf42.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/github-elements-9e1d42c09c62.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/element-registry-c3d8708e5707.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_braintree_browser-detection_dist_browser-detection_js-node_modules_githu-bb80ec-f11c694928ba.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-9012bef51135.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_morphdom_dist_morphdom-e-c1896e-ba47f43192a8.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-893f9f-9ba0881c72fb.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-8eb9b2209bcd.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_hotkey_dist_index_js-node_modules_github_hydro-analytics-client_d-dd3ec8-1f3d5f90de2b.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_quote-selection_dist_index_js-node_modules_github_session-resume_-31b9f3-e00a737a5ea6.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_document-metadata_document-metadata_ts-packages_failbot_failbot_ts-06156f7d8d1a.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_updatable-content_updatable-content_ts-38f5e2f7c2a7.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_details-6493f1-cecb020e2bb7.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_task-list_ts-app_assets_modules_github_throttled-input_ts-047775-cfe8770908d1.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-d8c5bfe37d1d.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/behaviors-2c0a177b1bbe.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-ef6d0f-20d6767cecc0.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/notifications-global-d89a4ddd4532.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_virtualized-list_es_index_js-node_modules_github_template-parts_lib_inde-f69fd1-ead47121f2f7.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-970f7d-c4a1e7dca262.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-63ecfa2887c1.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/codespaces-b163c00b86b6.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-3eebbd-7f6bf4b8b391.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_decorators_js-node_modules_delegated-events_di-e161aa-0f5deee7bb6d.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_remote--abdaf7-71f92102de66.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/repositories-efdd5372a3aa.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_catalyst_lib_inde-96937f-70732ff56a20.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/code-menu-614eb4e0c016.js" defer="defer"></script>
+  
+  <script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/primer-react-08acf876eb7c.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-lib-25ef56e89e94.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-core-ca4f004636c1.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/octicons-react-dfcd3f5e8531.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_emotion_is-prop-valid_dist_emotion-is-prop-valid_esm_js-node_modules_emo-5b2f05-e03e95da9abb.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_hydro-analytics-c-c228f9-e9af1d9bff76.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_tanstack_query-core_build_modern_mutation_js-node_modules_tanstack_query-9bf7e4-2866c135221a.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_swc_helpers_esm__class_private_method_get_js-node_modules_swc_helpers_es-d6b1a6-ea538d0fdafa.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_dompurify_dist_purify_es_mjs-0294cfa498e7.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_tanstack_query-core_build_modern_queryObserver_js-node_modules_tanstack_-defd52-b021f9f0d92e.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_lodash-es__Stack_js-node_modules_lodash-es__Uint8Array_js-node_modules_l-4faaa6-fa61bd48d446.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_hydro-analytics-client_dist_analytics-client_js-node_modules_gith-c7919d-70dcea79f7a9.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_tanstack_react-virtual_dist_esm_index_js-4f7c027617ef.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_decorators_js-node_modules_accname_dist_access-690142-387fd2fa9b38.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_fzy_js_index_js-node_modules_lit-labs_react_index_js-node_modules_primer-f2dcb3-705cdc996f67.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_catalyst_lib_index_js-node_modules_swc_helpers_esm__class_check_p-2c3c06-e97dfd69b8bb.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_react-reverse-portal_dist_web_index_js-node_modules_swc_helpers_esm__cla-3af928-d148e6552274.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_paths_index_ts-b2d80b245113.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_history_history_ts-packages_promise-with-resolvers-polyfill_promise-with-resolvers-p-2cdd04-a13a722d5075.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_ref-selector_RefSelector_tsx-9df0a8f768c9.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_copilot-chat_utils_copilot-local-storage_ts-9059a6197314.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_copilot-chat_components_tracing_TraceProvider_tsx-3bf4bd1c2170.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_copilot-chat_utils_CopilotChatContext_tsx-packages_safe-html_VerifiedHTML_tsx-f92485305fef.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_commit-attribution_index_ts-packages_commit-checks-status_index_ts-packages_current--18e2dc-36936e806be4.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_code-view-shared_hooks_use-canonical-object_ts-packages_code-view-shared_hooks_use-f-37800a-6c8f00a8ce1c.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_app-uuid_app-uuid_ts-packages_repos-file-tree-view_repos-file-tree-view_ts-0977800e9d2d.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/app_assets_modules_react-code-view_utilities_lines_ts-2feff25dd2a7.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_code-view-shared_utilities_web-worker_ts-packages_code-view-shared_worker-jobs_debou-a85645-b1eb8311b77e.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-code-view-39ceeaba7a58.js" defer="defer"></script>
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.c916beccff42f23950ab.module.css" />
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/react-code-view.9d3b72df4543ce88ddf6.module.css" />
+
+  <script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/packages_notifications-subscriptions-menu_entry_ts-packages_promise-with-resolvers-polyfill_p-15cb60-d3324ce89707.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/notifications-subscriptions-menu-c6e2d0703961.js" defer="defer"></script>
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.c916beccff42f23950ab.module.css" />
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/notifications-subscriptions-menu.bab8822ac328fb95c70d.module.css" />
+
+
+  <title>robotic-grasping/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32 at master Â· skumra/robotic-grasping Â· GitHub</title>
+
+
+
+  <meta name="route-pattern" content="/:user_id/:repository/tree/*name(/*path)" data-turbo-transient>
+  <meta name="route-controller" content="files" data-turbo-transient>
+  <meta name="route-action" content="disambiguate" data-turbo-transient>
+  <meta name="fetch-nonce" content="v2:efe89c02-1e51-251c-8658-f3b66a23ab93">
+
+    
+  <meta name="current-catalog-service-hash" content="f3abb0cc802f3d7b95fc8762b94bdcb13bf39634c40c357301c4aa1d67a256fb">
+
+
+  <meta name="request-id" content="9E0C:25D977:B6C2EC:CD299B:68F220F6" data-pjax-transient="true"/><meta name="html-safe-nonce" content="2b8cc70165e379f42fc72d7635104b0b8061cf3bc424dc50f5840ffcc4b68428" data-pjax-transient="true"/><meta name="visitor-payload" content="eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiI5RTBDOjI1RDk3NzpCNkMyRUM6Q0QyOTlCOjY4RjIyMEY2IiwidmlzaXRvcl9pZCI6Ijg2ODEzNjcxNDE3NTMxNjgxMTgiLCJyZWdpb25fZWRnZSI6InNvdXRoZWFzdGFzaWEiLCJyZWdpb25fcmVuZGVyIjoic291dGhlYXN0YXNpYSJ9" data-pjax-transient="true"/><meta name="visitor-hmac" content="30428cb24e67b7d823aafb356dc29ff14f32b8a7aa667a74af859a9058925578" data-pjax-transient="true"/>
+
+
+    <meta name="hovercard-subject-tag" content="repository:200533757" data-turbo-transient>
+
+
+  <meta name="github-keyboard-shortcuts" content="repository,source-code,file-tree,copilot" data-turbo-transient="true" />
+  
+
+  <meta name="selected-link" value="repo_source" data-turbo-transient>
+  <link rel="assets" href="https://github.githubassets.com/">
+
+    <meta name="google-site-verification" content="Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I">
+
+<meta name="octolytics-url" content="https://collector.github.com/github/collect" />
+
+  <meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;/files/disambiguate" data-turbo-transient="true" />
+
+  
+
+
+
+
+    <meta name="user-login" content="">
+
+  
+
+    <meta name="viewport" content="width=device-width">
+
+    
+
+      <meta name="description" content="Antipodal Robotic Grasping using GR-ConvNet. IROS 2020. - robotic-grasping/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32 at master Â· skumra/robotic-grasping">
+
+      <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">
+
+    <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
+    <meta property="fb:app_id" content="1401488693436528">
+    <meta name="apple-itunes-app" content="app-id=1477376905, app-argument=https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32" />
+
+      <meta name="twitter:image" content="https://opengraph.githubassets.com/6475db1392d729ebe2ef7dc954a67c7cafb0eb5db8b2145a3ad1bc7c7e39b152/skumra/robotic-grasping" /><meta name="twitter:site" content="@github" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="robotic-grasping/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32 at master Â· skumra/robotic-grasping" /><meta name="twitter:description" content="Antipodal Robotic Grasping using GR-ConvNet. IROS 2020. - skumra/robotic-grasping" />
+  <meta property="og:image" content="https://opengraph.githubassets.com/6475db1392d729ebe2ef7dc954a67c7cafb0eb5db8b2145a3ad1bc7c7e39b152/skumra/robotic-grasping" /><meta property="og:image:alt" content="Antipodal Robotic Grasping using GR-ConvNet. IROS 2020. - skumra/robotic-grasping" /><meta property="og:image:width" content="1200" /><meta property="og:image:height" content="600" /><meta property="og:site_name" content="GitHub" /><meta property="og:type" content="object" /><meta property="og:title" content="robotic-grasping/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32 at master Â· skumra/robotic-grasping" /><meta property="og:url" content="https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32" /><meta property="og:description" content="Antipodal Robotic Grasping using GR-ConvNet. IROS 2020. - skumra/robotic-grasping" />
+  
+
+
+
+
+      <meta name="hostname" content="github.com">
+
+
+
+        <meta name="expected-hostname" content="github.com">
+
+
+  <meta http-equiv="x-pjax-version" content="4d8268352d959b9d9cebdb2659b39a042dc20eb46e85bdf968810c979d2cbab7" data-turbo-track="reload">
+  <meta http-equiv="x-pjax-csp-version" content="c922ef32c4ab94f8b870c62883f3e41755ec705db76ec4efb0d343458f1e28c7" data-turbo-track="reload">
+  <meta http-equiv="x-pjax-css-version" content="d18b3c9fa72cd490c825769fe7da15241f27234ff3cead1c3655abad201ff181" data-turbo-track="reload">
+  <meta http-equiv="x-pjax-js-version" content="1a668571d83100d6f595b750b9f0b4b0f8b0631fc3e39ad171715b7adb695184" data-turbo-track="reload">
+
+  <meta name="turbo-cache-control" content="no-preview" data-turbo-transient="">
+
+      <meta name="turbo-cache-control" content="no-cache" data-turbo-transient>
+
+    <meta data-hydrostats="publish">
+
+  <meta name="go-import" content="github.com/skumra/robotic-grasping git https://github.com/skumra/robotic-grasping.git">
+
+  <meta name="octolytics-dimension-user_id" content="42680431" /><meta name="octolytics-dimension-user_login" content="skumra" /><meta name="octolytics-dimension-repository_id" content="200533757" /><meta name="octolytics-dimension-repository_nwo" content="skumra/robotic-grasping" /><meta name="octolytics-dimension-repository_public" content="true" /><meta name="octolytics-dimension-repository_is_fork" content="false" /><meta name="octolytics-dimension-repository_network_root_id" content="200533757" /><meta name="octolytics-dimension-repository_network_root_nwo" content="skumra/robotic-grasping" />
+
+
+
+    
+
+    <meta name="turbo-body-classes" content="logged-out env-production page-responsive">
+
+
+  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">
+
+  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">
+
+  <meta name="release" content="d22160f252e762d096f40ac820a628f5ba9334e7">
+  <meta name="ui-target" content="full">
+
+  <link rel="mask-icon" href="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" color="#000000">
+  <link rel="alternate icon" class="js-site-favicon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png">
+  <link rel="icon" class="js-site-favicon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" data-base-href="https://github.githubassets.com/favicons/favicon">
+
+<meta name="theme-color" content="#1e2327">
+<meta name="color-scheme" content="light dark" />
+
+
+  <link rel="manifest" href="/manifest.json" crossOrigin="use-credentials">
+
+  </head>
+
+  <body class="logged-out env-production page-responsive" style="word-wrap: break-word;">
+    <div data-turbo-body class="logged-out env-production page-responsive" style="word-wrap: break-word;">
+      
+
+
+
+    <div class="position-relative header-wrapper js-header-wrapper ">
+      <a href="#start-of-content" data-skip-target-assigned="false" class="px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content">Skip to content</a>
+
+      <span data-view-component="true" class="progress-pjax-loader Progress position-fixed width-full">
+    <span style="width: 0%;" data-view-component="true" class="Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis"></span>
+</span>      
+      
+      <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.c916beccff42f23950ab.module.css" />
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/keyboard-shortcuts-dialog.2de9c7d6456a311fce49.module.css" />
+
+<react-partial
+  partial-name="keyboard-shortcuts-dialog"
+  data-ssr="false"
+  data-attempted-ssr="false"
+  data-react-profiling="false"
+>
+  
+  <script type="application/json" data-target="react-partial.embeddedData">{"props":{"docsUrl":"https://docs.github.com/get-started/accessibility/keyboard-shortcuts"}}</script>
+  <div data-target="react-partial.reactRoot"></div>
+</react-partial>
+
+
+
+
+
+      
+
+          
+
+              
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_gsap_index_js-23c9606618ce.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-94fd67-dc050877d1bf.js" defer="defer"></script>
+<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/sessions-917229b8a853.js" defer="defer"></script>
+
+<header class="HeaderMktg header-logged-out js-details-container js-header Details f4 py-3" role="banner" data-is-top="true" data-color-mode=light data-light-theme=light data-dark-theme=dark>
+  <h2 class="sr-only">Navigation Menu</h2>
+
+  <button type="button" class="HeaderMktg-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target" aria-label="Toggle navigation">
+    <span class="d-none">Toggle navigation</span>
+  </button>
+
+  <div class="d-flex flex-column flex-lg-row flex-items-center px-3 px-md-4 px-lg-5 height-full position-relative z-1">
+    <div class="d-flex flex-justify-between flex-items-center width-full width-lg-auto">
+      <div class="flex-1">
+        <button aria-label="Toggle navigation" aria-expanded="false" type="button" data-view-component="true" class="js-details-target js-nav-padding-recalculate js-header-menu-toggle Button--link Button--medium Button d-lg-none color-fg-inherit p-1">  <span class="Button-content">
+    <span class="Button-label"><div class="HeaderMenu-toggle-bar rounded my-1"></div>
+            <div class="HeaderMenu-toggle-bar rounded my-1"></div>
+            <div class="HeaderMenu-toggle-bar rounded my-1"></div></span>
+  </span>
+</button>
+      </div>
+
+      <a class="mr-lg-3 color-fg-inherit flex-order-2 js-prevent-focus-on-mobile-nav"
+        href="/"
+        aria-label="Homepage"
+        data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Logomark;ref_loc:Header&quot;}">
+        <svg height="32" aria-hidden="true" viewBox="0 0 24 24" version="1.1" width="32" data-view-component="true" class="octicon octicon-mark-github">
+    <path d="M12 1C5.923 1 1 5.923 1 12c0 4.867 3.149 8.979 7.521 10.436.55.096.756-.233.756-.522 0-.262-.013-1.128-.013-2.049-2.764.509-3.479-.674-3.699-1.292-.124-.317-.66-1.293-1.127-1.554-.385-.207-.936-.715-.014-.729.866-.014 1.485.797 1.691 1.128.99 1.663 2.571 1.196 3.204.907.096-.715.385-1.196.701-1.471-2.448-.275-5.005-1.224-5.005-5.432 0-1.196.426-2.186 1.128-2.956-.111-.275-.496-1.402.11-2.915 0 0 .921-.288 3.024 1.128a10.193 10.193 0 0 1 2.75-.371c.936 0 1.871.123 2.75.371 2.104-1.43 3.025-1.128 3.025-1.128.605 1.513.221 2.64.111 2.915.701.77 1.127 1.747 1.127 2.956 0 4.222-2.571 5.157-5.019 5.432.399.344.743 1.004.743 2.035 0 1.471-.014 2.654-.014 3.025 0 .289.206.632.756.522C19.851 20.979 23 16.854 23 12c0-6.077-4.922-11-11-11Z"></path>
+</svg>
+      </a>
+
+      <div class="d-flex flex-1 flex-order-2 text-right d-lg-none gap-2 flex-justify-end">
+          <a
+            href="/login?return_to=https%3A%2F%2Fgithub.com%2Fskumra%2Frobotic-grasping%2Ftree%2Fmaster%2Ftrained-models%2Fcornell-randsplit-rgbd-grconvnet3-drop1-ch32"
+            class="HeaderMenu-link HeaderMenu-button d-inline-flex f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit js-prevent-focus-on-mobile-nav"
+            data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="1261c07c92eafb9ca98404ea77b9fddadfdf2539b37cdc40ca65dcab649d5beb"
+            data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to Sign in&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}"
+          >
+            Sign in
+          </a>
+              <div class="AppHeader-appearanceSettings">
+    <react-partial-anchor>
+      <button data-target="react-partial-anchor.anchor" id="icon-button-4a6f85f5-4cbc-4982-b7cd-617be1970f30" aria-labelledby="tooltip-95fd364e-4436-4aab-bb7a-086954be506b" type="button" disabled="disabled" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium AppHeader-button HeaderMenu-link border cursor-wait">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-sliders Button-visual">
+    <path d="M15 2.75a.75.75 0 0 1-.75.75h-4a.75.75 0 0 1 0-1.5h4a.75.75 0 0 1 .75.75Zm-8.5.75v1.25a.75.75 0 0 0 1.5 0v-4a.75.75 0 0 0-1.5 0V2H1.75a.75.75 0 0 0 0 1.5H6.5Zm1.25 5.25a.75.75 0 0 0 0-1.5h-6a.75.75 0 0 0 0 1.5h6ZM15 8a.75.75 0 0 1-.75.75H11.5V10a.75.75 0 1 1-1.5 0V6a.75.75 0 0 1 1.5 0v1.25h2.75A.75.75 0 0 1 15 8Zm-9 5.25v-2a.75.75 0 0 0-1.5 0v1.25H1.75a.75.75 0 0 0 0 1.5H4.5v1.25a.75.75 0 0 0 1.5 0v-2Zm9 0a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h6a.75.75 0 0 1 .75.75Z"></path>
+</svg>
+</button><tool-tip id="tooltip-95fd364e-4436-4aab-bb7a-086954be506b" for="icon-button-4a6f85f5-4cbc-4982-b7cd-617be1970f30" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Appearance settings</tool-tip>
+
+      <template data-target="react-partial-anchor.template">
+        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.c916beccff42f23950ab.module.css" />
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.6c63a6de228d6520804d.module.css" />
+
+<react-partial
+  partial-name="appearance-settings"
+  data-ssr="false"
+  data-attempted-ssr="false"
+  data-react-profiling="false"
+>
+  
+  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
+  <div data-target="react-partial.reactRoot"></div>
+</react-partial>
+
+
+      </template>
+    </react-partial-anchor>
+  </div>
+
+      </div>
+    </div>
+
+
+    <div class="HeaderMenu js-header-menu height-fit position-lg-relative d-lg-flex flex-column flex-auto top-0">
+      <div class="HeaderMenu-wrapper d-flex flex-column flex-self-start flex-lg-row flex-auto rounded rounded-lg-0">
+          <nav class="HeaderMenu-nav" aria-label="Global">
+            <ul class="d-lg-flex list-style-none">
+                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
+      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
+        Platform
+        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
+    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
+</svg>
+      </button>
+
+      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 dropdown-menu-wide">
+        <div class="d-lg-flex dropdown-menu-wide">
+            <div class="HeaderMenu-column px-lg-4">
+                <div class="">
+
+                  <ul class="list-style-none f5" >
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}" href="https://github.com/features/copilot">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-copilot color-fg-subtle mr-3">
+    <path d="M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z"></path><path d="M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          GitHub Copilot
+
+        </div>
+
+        Write better code with AI
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}" href="https://github.com/features/spark">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-sparkle-fill color-fg-subtle mr-3">
+    <path d="M11.296 1.924c.24-.656 1.168-.656 1.408 0l.717 1.958a11.25 11.25 0 0 0 6.697 6.697l1.958.717c.657.24.657 1.168 0 1.408l-1.958.717a11.25 11.25 0 0 0-6.697 6.697l-.717 1.958c-.24.657-1.168.657-1.408 0l-.717-1.958a11.25 11.25 0 0 0-6.697-6.697l-1.958-.717c-.656-.24-.656-1.168 0-1.408l1.958-.717a11.25 11.25 0 0 0 6.697-6.697l.717-1.958Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          GitHub Spark
+
+            <span class="HeaderMenu-label">
+              New
+            </span>
+        </div>
+
+        Build and deploy intelligent apps
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}" href="https://github.com/features/models">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-ai-model color-fg-subtle mr-3">
+    <path d="M19.375 8.5a3.25 3.25 0 1 1-3.163 4h-3a3.252 3.252 0 0 1-4.443 2.509L7.214 17.76a3.25 3.25 0 1 1-1.342-.674l1.672-2.957A3.238 3.238 0 0 1 6.75 12c0-.907.371-1.727.97-2.316L6.117 6.846A3.253 3.253 0 0 1 1.875 3.75a3.25 3.25 0 1 1 5.526 2.32l1.603 2.836A3.25 3.25 0 0 1 13.093 11h3.119a3.252 3.252 0 0 1 3.163-2.5ZM10 10.25a1.75 1.75 0 1 0-.001 3.499A1.75 1.75 0 0 0 10 10.25ZM5.125 2a1.75 1.75 0 1 0 0 3.5 1.75 1.75 0 0 0 0-3.5Zm12.5 9.75a1.75 1.75 0 1 0 3.5 0 1.75 1.75 0 0 0-3.5 0Zm-14.25 8.5a1.75 1.75 0 1 0 3.501-.001 1.75 1.75 0 0 0-3.501.001Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          GitHub Models
+
+            <span class="HeaderMenu-label">
+              New
+            </span>
+        </div>
+
+        Manage and compare prompts
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}" href="https://github.com/security/advanced-security">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-shield-check color-fg-subtle mr-3">
+    <path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z"></path><path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          GitHub Advanced Security
+
+        </div>
+
+        Find and fix vulnerabilities
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}" href="https://github.com/features/actions">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-workflow color-fg-subtle mr-3">
+    <path d="M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Actions
+
+        </div>
+
+        Automate any workflow
+      </div>
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+            <div class="HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right">
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 pb-3">
+
+                  <ul class="list-style-none f5" >
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}" href="https://github.com/features/codespaces">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-codespaces color-fg-subtle mr-3">
+    <path d="M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z"></path><path d="M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Codespaces
+
+        </div>
+
+        Instant dev environments
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}" href="https://github.com/features/issues">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-issue-opened color-fg-subtle mr-3">
+    <path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Issues
+
+        </div>
+
+        Plan and track work
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}" href="https://github.com/features/code-review">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-code-review color-fg-subtle mr-3">
+    <path d="M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z"></path><path d="M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Code Review
+
+        </div>
+
+        Manage code changes
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_platform_navbar&quot;}" href="https://github.com/features/discussions">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-comment-discussion color-fg-subtle mr-3">
+    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z"></path><path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Discussions
+
+        </div>
+
+        Collaborate outside of code
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_platform_navbar&quot;}" href="https://github.com/features/code-search">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-code-square color-fg-subtle mr-3">
+    <path d="M10.3 8.24a.75.75 0 0 1-.04 1.06L7.352 12l2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z"></path><path d="M2 3.75C2 2.784 2.784 2 3.75 2h16.5c.966 0 1.75.784 1.75 1.75v16.5A1.75 1.75 0 0 1 20.25 22H3.75A1.75 1.75 0 0 1 2 20.25Zm1.75-.25a.25.25 0 0 0-.25.25v16.5c0 .138.112.25.25.25h16.5a.25.25 0 0 0 .25-.25V3.75a.25.25 0 0 0-.25-.25Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Code Search
+
+        </div>
+
+        Find more, search less
+      </div>
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+            <div class="HeaderMenu-column px-lg-4">
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 mb-3 pb-3">
+
+                      <span class="d-block h4 color-fg-default my-1" id="platform-explore-heading">Explore</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="platform-explore-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;why_github&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;why_github_link_platform_navbar&quot;}" href="https://github.com/why-github">
+      Why GitHub
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external" target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;documentation&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;documentation_link_platform_navbar&quot;}" href="https://docs.github.com">
+      Documentation
+
+    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
+    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
+</svg>
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external" target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_skills&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_skills_link_platform_navbar&quot;}" href="https://skills.github.com">
+      GitHub Skills
+
+    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
+    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
+</svg>
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external" target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;blog&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;blog_link_platform_navbar&quot;}" href="https://github.blog">
+      Blog
+
+    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
+    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
+</svg>
+</a></li>
+
+                  </ul>
+                </div>
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 pb-3">
+
+                      <span class="d-block h4 color-fg-default my-1" id="platform-integrations-heading">Integrations</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="platform-integrations-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_marketplace&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_marketplace_link_platform_navbar&quot;}" href="https://github.com/marketplace">
+      GitHub Marketplace
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;mcp_registry&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;mcp_registry_link_platform_navbar&quot;}" href="https://github.com/mcp">
+      MCP Registry
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+        </div>
+
+          <div class="HeaderMenu-trailing-link rounded-bottom-2 mt-lg-4 px-lg-4 py-4 py-lg-3 f5 text-semibold">
+            <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_features&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}" href="https://github.com/features">
+              View all features
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-right HeaderMenu-trailing-link-icon">
+    <path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+</a>          </div>
+      </div>
+</li>
+
+
+                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
+      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
+        Solutions
+        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
+    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
+</svg>
+      </button>
+
+      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 dropdown-menu-wide">
+        <div class="d-lg-flex dropdown-menu-wide">
+            <div class="HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right">
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 pb-3">
+
+                      <span class="d-block h4 color-fg-default my-1" id="solutions-by-company-size-heading">By company size</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="solutions-by-company-size-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprises&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprises_link_solutions_navbar&quot;}" href="https://github.com/enterprise">
+      Enterprises
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;small_and_medium_teams&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;small_and_medium_teams_link_solutions_navbar&quot;}" href="https://github.com/team">
+      Small and medium teams
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;startups&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;startups_link_solutions_navbar&quot;}" href="https://github.com/enterprise/startups">
+      Startups
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;nonprofits&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;nonprofits_link_solutions_navbar&quot;}" href="/solutions/industry/nonprofits">
+      Nonprofits
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+            <div class="HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right">
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 pb-3">
+
+                      <span class="d-block h4 color-fg-default my-1" id="solutions-by-use-case-heading">By use case</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="solutions-by-use-case-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;app_modernization&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;app_modernization_link_solutions_navbar&quot;}" href="/solutions/use-case/app-modernization">
+      App Modernization
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;devsecops&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;devsecops_link_solutions_navbar&quot;}" href="/solutions/use-case/devsecops">
+      DevSecOps
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;devops&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;devops_link_solutions_navbar&quot;}" href="/solutions/use-case/devops">
+      DevOps
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ci_cd&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ci_cd_link_solutions_navbar&quot;}" href="/solutions/use-case/ci-cd">
+      CI/CD
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_use_cases&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_use_cases_link_solutions_navbar&quot;}" href="/solutions/use-case">
+      View all use cases
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+            <div class="HeaderMenu-column px-lg-4">
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 pb-3">
+
+                      <span class="d-block h4 color-fg-default my-1" id="solutions-by-industry-heading">By industry</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="solutions-by-industry-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;healthcare&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;healthcare_link_solutions_navbar&quot;}" href="/solutions/industry/healthcare">
+      Healthcare
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;financial_services&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;financial_services_link_solutions_navbar&quot;}" href="/solutions/industry/financial-services">
+      Financial services
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;manufacturing&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;manufacturing_link_solutions_navbar&quot;}" href="/solutions/industry/manufacturing">
+      Manufacturing
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;government&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;government_link_solutions_navbar&quot;}" href="/solutions/industry/government">
+      Government
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_industries&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_industries_link_solutions_navbar&quot;}" href="/solutions/industry">
+      View all industries
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+        </div>
+
+          <div class="HeaderMenu-trailing-link rounded-bottom-2 mt-lg-4 px-lg-4 py-4 py-lg-3 f5 text-semibold">
+            <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_solutions&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_solutions_link_solutions_navbar&quot;}" href="/solutions">
+              View all solutions
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-right HeaderMenu-trailing-link-icon">
+    <path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+</a>          </div>
+      </div>
+</li>
+
+
+                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
+      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
+        Resources
+        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
+    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
+</svg>
+      </button>
+
+      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 pb-2 pb-lg-4 dropdown-menu-wide">
+        <div class="d-lg-flex dropdown-menu-wide">
+            <div class="HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right">
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 pb-3">
+
+                      <span class="d-block h4 color-fg-default my-1" id="resources-topics-heading">Topics</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="resources-topics-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ai&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ai_link_resources_navbar&quot;}" href="/resources/articles?topic=ai">
+      AI
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;devops&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;devops_link_resources_navbar&quot;}" href="/resources/articles?topic=devops">
+      DevOps
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_resources_navbar&quot;}" href="/resources/articles?topic=security">
+      Security
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;software_development&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;software_development_link_resources_navbar&quot;}" href="/resources/articles?topic=software-development">
+      Software Development
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_link_resources_navbar&quot;}" href="/resources/articles">
+      View all
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+            <div class="HeaderMenu-column px-lg-4">
+                <div class="border-bottom border-lg-bottom-0 pb-lg-0 border-bottom-0">
+
+                      <span class="d-block h4 color-fg-default my-1" id="resources-explore-heading">Explore</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="resources-explore-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external" target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
+      Learning Pathways
+
+    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle">
+    <path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path>
+</svg>
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://github.com/resources/events">
+      Events &amp; Webinars
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
+      Ebooks &amp; Whitepapers
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
+      Customer Stories
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://github.com/partners">
+      Partners
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
+      Executive Insights
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+        </div>
+
+      </div>
+</li>
+
+
+                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
+      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
+        Open Source
+        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
+    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
+</svg>
+      </button>
+
+      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 pb-2 pb-lg-4">
+        <div class="d-lg-flex dropdown-menu-wide">
+            <div class="HeaderMenu-column px-lg-4">
+                <div class="border-bottom mb-3 mb-lg-3 pb-3">
+
+                  <ul class="list-style-none f5" >
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="/sponsors">
+      
+      <div>
+        <div class="color-fg-default h4">
+          GitHub Sponsors
+
+        </div>
+
+        Fund open source developers
+      </div>
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+                <div class="border-bottom mb-3 mb-lg-3 pb-3">
+
+                  <ul class="list-style-none f5" >
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
+      
+      <div>
+        <div class="color-fg-default h4">
+          The ReadME Project
+
+        </div>
+
+        GitHub community articles
+      </div>
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+                <div class="border-bottom border-bottom-0">
+
+                      <span class="d-block h4 color-fg-default my-1" id="open-source-repositories-heading">Repositories</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="open-source-repositories-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;topics&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;topics_link_open_source_navbar&quot;}" href="https://github.com/topics">
+      Topics
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;trending&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;trending_link_open_source_navbar&quot;}" href="https://github.com/trending">
+      Trending
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;collections&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;collections_link_open_source_navbar&quot;}" href="https://github.com/collections">
+      Collections
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+        </div>
+
+      </div>
+</li>
+
+
+                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
+      <button type="button" class="HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target" aria-expanded="false">
+        Enterprise
+        <svg opacity="0.5" aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-chevron-down HeaderMenu-icon ml-1">
+    <path d="M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z"></path>
+</svg>
+      </button>
+
+      <div class="HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 pb-2 pb-lg-4">
+        <div class="d-lg-flex dropdown-menu-wide">
+            <div class="HeaderMenu-column px-lg-4">
+                <div class="border-bottom mb-3 mb-lg-3 pb-3">
+
+                  <ul class="list-style-none f5" >
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="/enterprise">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-stack color-fg-subtle mr-3">
+    <path d="M11.063 1.456a1.749 1.749 0 0 1 1.874 0l8.383 5.316a1.751 1.751 0 0 1 0 2.956l-8.383 5.316a1.749 1.749 0 0 1-1.874 0L2.68 9.728a1.751 1.751 0 0 1 0-2.956Zm1.071 1.267a.25.25 0 0 0-.268 0L3.483 8.039a.25.25 0 0 0 0 .422l8.383 5.316a.25.25 0 0 0 .268 0l8.383-5.316a.25.25 0 0 0 0-.422Z"></path><path d="M1.867 12.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z"></path><path d="M1.867 16.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Enterprise platform
+
+        </div>
+
+        AI-powered developer platform
+      </div>
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+                <div class="border-bottom border-bottom-0">
+
+                      <span class="d-block h4 color-fg-default my-1" id="enterprise-available-add-ons-heading">Available add-ons</span>
+
+                  <ul class="list-style-none f5" aria-labelledby="enterprise-available-add-ons-heading">
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_enterprise_navbar&quot;}" href="https://github.com/security/advanced-security">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-shield-check color-fg-subtle mr-3">
+    <path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z"></path><path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          GitHub Advanced Security
+
+        </div>
+
+        Enterprise-grade security features
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;copilot_for_business&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;copilot_for_business_link_enterprise_navbar&quot;}" href="/features/copilot/copilot-business">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-copilot color-fg-subtle mr-3">
+    <path d="M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z"></path><path d="M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Copilot for business
+
+        </div>
+
+        Enterprise-grade AI features
+      </div>
+
+    
+</a></li>
+
+                      <li>
+  <a class="HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;premium_support&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;premium_support_link_enterprise_navbar&quot;}" href="/premium-support">
+      <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-comment-discussion color-fg-subtle mr-3">
+    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z"></path><path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z"></path>
+</svg>
+      <div>
+        <div class="color-fg-default h4">
+          Premium Support
+
+        </div>
+
+        Enterprise-grade 24/7 support
+      </div>
+
+    
+</a></li>
+
+                  </ul>
+                </div>
+            </div>
+        </div>
+
+      </div>
+</li>
+
+
+                <li class="HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item">
+    <a class="HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;platform&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;platform_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
+</li>
+
+            </ul>
+          </nav>
+
+        <div class="d-flex flex-column flex-lg-row width-full flex-justify-end flex-lg-items-center text-center mt-3 mt-lg-0 text-lg-left ml-lg-3">
+                
+
+
+<qbsearch-input class="search-input" data-scope="repo:skumra/robotic-grasping" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Md4yBFlQx90gjUEgNo2WHiC1zniHPmRK2nRnbx2-0GPrTAfESAO-sr40Z5D8w0ATdchIKkGNAmo6SpZrckVOKg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="skumra/robotic-grasping" data-current-org="" data-current-owner="skumra" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
+  <div
+    class="search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded"
+    data-action="click:qbsearch-input#searchInputContainerClicked"
+  >
+      <button
+        type="button"
+        class="header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none"
+        data-target="qbsearch-input.inputButton"
+        aria-label="Search or jump toâ€¦"
+        aria-haspopup="dialog"
+        placeholder="Search or jump to..."
+        data-hotkey=s,/
+        autocapitalize="off"
+        data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;searchbar&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;input&quot;,&quot;label&quot;:&quot;searchbar_input_global_navbar&quot;}"
+        data-action="click:qbsearch-input#handleExpand"
+      >
+        <div class="mr-2 color-fg-muted">
+          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search">
+    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
+</svg>
+        </div>
+        <span class="flex-1" data-target="qbsearch-input.inputButtonText">Search or jump to...</span>
+          <div class="d-flex" data-target="qbsearch-input.hotkeyIndicator">
+            <svg xmlns="http://www.w3.org/2000/svg" width="22" height="20" aria-hidden="true" class="mr-1"><path fill="none" stroke="#979A9C" opacity=".4" d="M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z"></path><path fill="#979A9C" d="M11.8 6L8 15.1h-.9L10.8 6h1z"></path></svg>
+          </div>
+      </button>
+
+    <input type="hidden" name="type" class="js-site-search-type-field">
+
+    
+<div class="Overlay--hidden " data-modal-dialog-overlay>
+  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true" class="Overlay Overlay--width-large Overlay--height-auto">
+      <h1 id="search-suggestions-dialog-header" class="sr-only">Search code, repositories, users, issues, pull requests...</h1>
+    <div class="Overlay-body Overlay-body--paddingNone">
+      
+          <div data-view-component="true">        <div class="search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container"
+          style="border-radius: 12px;"
+          data-target="qbsearch-input.queryBuilderContainer"
+          hidden
+        >
+          <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="query-builder-test-form" action="" accept-charset="UTF-8" method="get">
+  <query-builder data-target="qbsearch-input.queryBuilder" id="query-builder-query-builder-test" data-filter-key=":" data-view-component="true" class="QueryBuilder search-query-builder">
+    <div class="FormControl FormControl--fullWidth">
+      <label id="query-builder-test-label" for="query-builder-test" class="FormControl-label sr-only">
+        Search
+      </label>
+      <div
+        class="QueryBuilder-StyledInput width-fit "
+        data-target="query-builder.styledInput"
+      >
+          <span id="query-builder-test-leadingvisual-wrap" class="FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap">
+            <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search FormControl-input-leadingVisual">
+    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
+</svg>
+          </span>
+        <div data-target="query-builder.styledInputContainer" class="QueryBuilder-StyledInputContainer">
+          <div
+            aria-hidden="true"
+            class="QueryBuilder-StyledInputContent"
+            data-target="query-builder.styledInputContent"
+          ></div>
+          <div class="QueryBuilder-InputWrapper">
+            <div aria-hidden="true" class="QueryBuilder-Sizer" data-target="query-builder.sizer"></div>
+            <input id="query-builder-test" name="query-builder-test" value="" autocomplete="off" type="text" role="combobox" spellcheck="false" aria-expanded="false" aria-describedby="validation-38e0a95e-cbd0-4d4e-a21a-e3ab8dd4bcd7" data-target="query-builder.input" data-action="
+          input:query-builder#inputChange
+          blur:query-builder#inputBlur
+          keydown:query-builder#inputKeydown
+          focus:query-builder#inputFocus
+        " data-view-component="true" class="FormControl-input QueryBuilder-Input FormControl-medium" />
+          </div>
+        </div>
+          <span class="sr-only" id="query-builder-test-clear">Clear</span>
+          <button role="button" id="query-builder-test-clear-button" aria-labelledby="query-builder-test-clear query-builder-test-label" data-target="query-builder.clearButton" data-action="
+                click:query-builder#clear
+                focus:query-builder#clearButtonFocus
+                blur:query-builder#clearButtonBlur
+              " variant="small" hidden="hidden" type="button" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x-circle-fill Button-visual">
+    <path d="M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z"></path>
+</svg>
+</button>
+
+      </div>
+      <template id="search-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search">
+    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
+</svg>
+</template>
+
+<template id="code-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code">
+    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+</template>
+
+<template id="file-code-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-file-code">
+    <path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path>
+</svg>
+</template>
+
+<template id="history-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-history">
+    <path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z"></path>
+</svg>
+</template>
+
+<template id="repo-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo">
+    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
+</svg>
+</template>
+
+<template id="bookmark-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bookmark">
+    <path d="M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z"></path>
+</svg>
+</template>
+
+<template id="plus-circle-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-plus-circle">
+    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z"></path>
+</svg>
+</template>
+
+<template id="circle-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-dot-fill">
+    <path d="M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z"></path>
+</svg>
+</template>
+
+<template id="trash-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-trash">
+    <path d="M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z"></path>
+</svg>
+</template>
+
+<template id="team-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-people">
+    <path d="M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z"></path>
+</svg>
+</template>
+
+<template id="project-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-project">
+    <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z"></path>
+</svg>
+</template>
+
+<template id="pencil-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-pencil">
+    <path d="M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z"></path>
+</svg>
+</template>
+
+<template id="copilot-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copilot">
+    <path d="M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z"></path><path d="M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z"></path>
+</svg>
+</template>
+
+<template id="copilot-error-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copilot-error">
+    <path d="M16 11.24c0 .112-.072.274-.21.467L13 9.688V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-.198 0-.388-.009-.571-.029L6.833 5.226a4.01 4.01 0 0 0 .17-.782c.117-.935-.037-1.395-.241-1.614-.193-.206-.637-.413-1.682-.297-.683.076-1.115.231-1.395.415l-1.257-.91c.579-.564 1.413-.877 2.485-.996 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095Zm-5.083-8.707c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Zm2.511 11.074c-1.393.776-3.272 1.428-5.43 1.428-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.18-.455-.241-.963-.252-1.475L.31 4.107A.747.747 0 0 1 0 3.509V3.49a.748.748 0 0 1 .625-.73c.156-.026.306.047.435.139l14.667 10.578a.592.592 0 0 1 .227.264.752.752 0 0 1 .046.249v.022a.75.75 0 0 1-1.19.596Zm-1.367-.991L5.635 7.964a5.128 5.128 0 0 1-.889.073c-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433 1.539 0 3.089-.505 4.063-.934Z"></path>
+</svg>
+</template>
+
+<template id="workflow-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-workflow">
+    <path d="M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z"></path>
+</svg>
+</template>
+
+<template id="book-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-book">
+    <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path>
+</svg>
+</template>
+
+<template id="code-review-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code-review">
+    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+</template>
+
+<template id="codespaces-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-codespaces">
+    <path d="M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z"></path><path d="M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z"></path>
+</svg>
+</template>
+
+<template id="comment-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-comment">
+    <path d="M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
+</svg>
+</template>
+
+<template id="comment-discussion-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-comment-discussion">
+    <path d="M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z"></path>
+</svg>
+</template>
+
+<template id="organization-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-organization">
+    <path d="M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z"></path>
+</svg>
+</template>
+
+<template id="rocket-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-rocket">
+    <path d="M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
+</svg>
+</template>
+
+<template id="shield-check-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield-check">
+    <path d="m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
+</svg>
+</template>
+
+<template id="heart-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-heart">
+    <path d="m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z"></path>
+</svg>
+</template>
+
+<template id="server-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-server">
+    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z"></path>
+</svg>
+</template>
+
+<template id="globe-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-globe">
+    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z"></path>
+</svg>
+</template>
+
+<template id="issue-opened-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened">
+    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
+</svg>
+</template>
+
+<template id="device-mobile-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-device-mobile">
+    <path d="M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path>
+</svg>
+</template>
+
+<template id="package-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-package">
+    <path d="m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z"></path>
+</svg>
+</template>
+
+<template id="credit-card-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-credit-card">
+    <path d="M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z"></path><path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z"></path>
+</svg>
+</template>
+
+<template id="play-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play">
+    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
+</svg>
+</template>
+
+<template id="gift-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-gift">
+    <path d="M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z"></path>
+</svg>
+</template>
+
+<template id="code-square-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code-square">
+    <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
+</svg>
+</template>
+
+<template id="device-desktop-icon">
+  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-device-desktop">
+    <path d="M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z"></path>
+</svg>
+</template>
+
+        <div class="position-relative">
+                <ul
+                  role="listbox"
+                  class="ActionListWrap QueryBuilder-ListWrap"
+                  aria-label="Suggestions"
+                  data-action="
+                    combobox-commit:query-builder#comboboxCommit
+                    mousedown:query-builder#resultsMousedown
+                  "
+                  data-target="query-builder.resultsList"
+                  data-persist-list=false
+                  id="query-builder-test-results"
+                  tabindex="-1"
+                ></ul>
+        </div>
+      <div class="FormControl-inlineValidation" id="validation-38e0a95e-cbd0-4d4e-a21a-e3ab8dd4bcd7" hidden="hidden">
+        <span class="FormControl-inlineValidation--visual">
+          <svg aria-hidden="true" height="12" viewBox="0 0 12 12" version="1.1" width="12" data-view-component="true" class="octicon octicon-alert-fill">
+    <path d="M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z"></path>
+</svg>
+        </span>
+        <span></span>
+</div>    </div>
+    <div data-target="query-builder.screenReaderFeedback" aria-live="polite" aria-atomic="true" class="sr-only"></div>
+</query-builder></form>
+          <div class="d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt">
+            <a target="_blank" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax" data-view-component="true" class="Link color-fg-accent text-normal ml-2">Search syntax tips</a>            <div class="d-flex flex-1"></div>
+          </div>
+        </div>
+</div>
+
+    </div>
+</modal-dialog></div>
+  </div>
+  <div data-action="click:qbsearch-input#retract" class="dark-backdrop position-fixed" hidden data-target="qbsearch-input.darkBackdrop"></div>
+  <div class="color-fg-default">
+    
+<dialog-helper>
+  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade Overlay--disableScroll">
+    <div data-view-component="true" class="Overlay-header">
+  <div class="Overlay-headerContentWrap">
+    <div class="Overlay-titleWrap">
+      <h1 class="Overlay-title " id="feedback-dialog-title">
+        Provide feedback
+      </h1>
+        
+    </div>
+    <div class="Overlay-actionWrap">
+      <button data-close-dialog-id="feedback-dialog" aria-label="Close" aria-label="Close" type="button" data-view-component="true" class="close-button Overlay-closeButton"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
+    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
+</svg></button>
+    </div>
+  </div>
+  
+</div>
+      <scrollable-region data-labelled-by="feedback-dialog-title">
+        <div data-view-component="true" class="Overlay-body">        <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="code-search-feedback-form" data-turbo="false" action="/search/feedback" accept-charset="UTF-8" method="post"><input type="hidden" data-csrf="true" name="authenticity_token" value="w0deUgQQTJmqIOlBjyvMpY5M1KE825w+aeJBHhvIHZxl3xDfde/Js+h9vw6FJJFUMQOXREtuhhns+aPoej4YaQ==" />
+          <p>We read every piece of feedback, and take your input very seriously.</p>
+          <textarea name="feedback" class="form-control width-full mb-2" style="height: 120px" id="feedback"></textarea>
+          <input name="include_email" id="include_email" aria-label="Include my email address so I can be contacted" class="form-control mr-2" type="checkbox">
+          <label for="include_email" style="font-weight: normal">Include my email address so I can be contacted</label>
+</form></div>
+      </scrollable-region>
+      <div data-view-component="true" class="Overlay-footer Overlay-footer--alignEnd">          <button data-close-dialog-id="feedback-dialog" type="button" data-view-component="true" class="btn">    Cancel
+</button>
+          <button form="code-search-feedback-form" data-action="click:qbsearch-input#submitFeedback" type="submit" data-view-component="true" class="btn-primary btn">    Submit feedback
+</button>
+</div>
+</dialog></dialog-helper>
+
+    <custom-scopes data-target="qbsearch-input.customScopesManager">
+    
+<dialog-helper>
+  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade Overlay--disableScroll">
+    <div data-view-component="true" class="Overlay-header Overlay-header--divided">
+  <div class="Overlay-headerContentWrap">
+    <div class="Overlay-titleWrap">
+      <h1 class="Overlay-title " id="custom-scopes-dialog-title">
+        Saved searches
+      </h1>
+        <h2 id="custom-scopes-dialog-description" class="Overlay-description">Use saved searches to filter your results more quickly</h2>
+    </div>
+    <div class="Overlay-actionWrap">
+      <button data-close-dialog-id="custom-scopes-dialog" aria-label="Close" aria-label="Close" type="button" data-view-component="true" class="close-button Overlay-closeButton"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
+    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
+</svg></button>
+    </div>
+  </div>
+  
+</div>
+      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
+        <div data-view-component="true" class="Overlay-body">        <div data-target="custom-scopes.customScopesModalDialogFlash"></div>
+
+        <div hidden class="create-custom-scope-form" data-target="custom-scopes.createCustomScopeForm">
+        <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="custom-scopes-dialog-form" data-turbo="false" action="/search/custom_scopes" accept-charset="UTF-8" method="post"><input type="hidden" data-csrf="true" name="authenticity_token" value="tW6WwsL7Ro0E/hEWXa+a7k4GzTlmtbmYJ3T0002T9ksdCSUVlHIG05omR71kxQ1qZk65lwSgl1pP3Dfc2M2pEA==" />
+          <div data-target="custom-scopes.customScopesModalDialogFlash"></div>
+
+          <input type="hidden" id="custom_scope_id" name="custom_scope_id" data-target="custom-scopes.customScopesIdField">
+
+          <div class="form-group">
+            <label for="custom_scope_name">Name</label>
+            <auto-check src="/search/custom_scopes/check_name" required>
+              <input
+                type="text"
+                name="custom_scope_name"
+                id="custom_scope_name"
+                data-target="custom-scopes.customScopesNameField"
+                class="form-control"
+                autocomplete="off"
+                placeholder="github-ruby"
+                required
+                maxlength="50">
+              <input type="hidden" data-csrf="true" value="nx8Yjkc064QysOIUkv12fIaksT11olYi7Jbvs76Z7Ffj3QgsK5Mjg5WrBuIENW1Gu+JdSvM56O68BCfuefHJtg==" />
+            </auto-check>
+          </div>
+
+          <div class="form-group">
+            <label for="custom_scope_query">Query</label>
+            <input
+              type="text"
+              name="custom_scope_query"
+              id="custom_scope_query"
+              data-target="custom-scopes.customScopesQueryField"
+              class="form-control"
+              autocomplete="off"
+              placeholder="(repo:mona/a OR repo:mona/b) AND lang:python"
+              required
+              maxlength="500">
+          </div>
+
+          <p class="text-small color-fg-muted">
+            To see all available qualifiers, see our <a class="Link--inTextBlock" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax">documentation</a>.
+          </p>
+</form>        </div>
+
+        <div data-target="custom-scopes.manageCustomScopesForm">
+          <div data-target="custom-scopes.list"></div>
+        </div>
+
+</div>
+      </scrollable-region>
+      <div data-view-component="true" class="Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided">          <button data-action="click:custom-scopes#customScopesCancel" type="button" data-view-component="true" class="btn">    Cancel
+</button>
+          <button form="custom-scopes-dialog-form" data-action="click:custom-scopes#customScopesSubmit" data-target="custom-scopes.customScopesSubmitButton" type="submit" data-view-component="true" class="btn-primary btn">    Create saved search
+</button>
+</div>
+</dialog></dialog-helper>
+    </custom-scopes>
+  </div>
+</qbsearch-input>
+
+
+            <div class="position-relative HeaderMenu-link-wrap d-lg-inline-block">
+              <a
+                href="/login?return_to=https%3A%2F%2Fgithub.com%2Fskumra%2Frobotic-grasping%2Ftree%2Fmaster%2Ftrained-models%2Fcornell-randsplit-rgbd-grconvnet3-drop1-ch32"
+                class="HeaderMenu-link HeaderMenu-link--sign-in HeaderMenu-button flex-shrink-0 no-underline d-none d-lg-inline-flex border border-lg-0 rounded px-2 py-1"
+                style="margin-left: 12px;"
+                data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="1261c07c92eafb9ca98404ea77b9fddadfdf2539b37cdc40ca65dcab649d5beb"
+                data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}"
+              >
+                Sign in
+              </a>
+            </div>
+
+              <a href="/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Ffiles%2Fdisambiguate&amp;source=header-repo&amp;source_repo=skumra%2Frobotic-grasping"
+                class="HeaderMenu-link HeaderMenu-link--sign-up HeaderMenu-button flex-shrink-0 d-flex d-lg-inline-flex no-underline border color-border-default rounded px-2 py-1"
+                data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="1261c07c92eafb9ca98404ea77b9fddadfdf2539b37cdc40ca65dcab649d5beb"
+                data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;/files/disambiguate;ref_cta:Sign up;ref_loc:header logged out&quot;}"
+              >
+                Sign up
+              </a>
+
+                <div class="AppHeader-appearanceSettings">
+    <react-partial-anchor>
+      <button data-target="react-partial-anchor.anchor" id="icon-button-62f29f3e-e947-424e-b393-f7895b2f3c77" aria-labelledby="tooltip-71060e65-0d12-4026-98a0-5044cb051df9" type="button" disabled="disabled" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium AppHeader-button HeaderMenu-link border cursor-wait">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-sliders Button-visual">
+    <path d="M15 2.75a.75.75 0 0 1-.75.75h-4a.75.75 0 0 1 0-1.5h4a.75.75 0 0 1 .75.75Zm-8.5.75v1.25a.75.75 0 0 0 1.5 0v-4a.75.75 0 0 0-1.5 0V2H1.75a.75.75 0 0 0 0 1.5H6.5Zm1.25 5.25a.75.75 0 0 0 0-1.5h-6a.75.75 0 0 0 0 1.5h6ZM15 8a.75.75 0 0 1-.75.75H11.5V10a.75.75 0 1 1-1.5 0V6a.75.75 0 0 1 1.5 0v1.25h2.75A.75.75 0 0 1 15 8Zm-9 5.25v-2a.75.75 0 0 0-1.5 0v1.25H1.75a.75.75 0 0 0 0 1.5H4.5v1.25a.75.75 0 0 0 1.5 0v-2Zm9 0a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h6a.75.75 0 0 1 .75.75Z"></path>
+</svg>
+</button><tool-tip id="tooltip-71060e65-0d12-4026-98a0-5044cb051df9" for="icon-button-62f29f3e-e947-424e-b393-f7895b2f3c77" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Appearance settings</tool-tip>
+
+      <template data-target="react-partial-anchor.template">
+        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.c916beccff42f23950ab.module.css" />
+<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.6c63a6de228d6520804d.module.css" />
+
+<react-partial
+  partial-name="appearance-settings"
+  data-ssr="false"
+  data-attempted-ssr="false"
+  data-react-profiling="false"
+>
+  
+  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
+  <div data-target="react-partial.reactRoot"></div>
+</react-partial>
+
+
+      </template>
+    </react-partial-anchor>
+  </div>
+
+          <button type="button" class="sr-only js-header-menu-focus-trap d-block d-lg-none">Resetting focus</button>
+        </div>
+      </div>
+    </div>
+  </div>
+</header>
+
+      <div hidden="hidden" data-view-component="true" class="js-stale-session-flash stale-session-flash flash flash-warn flash-full">
+  
+        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
+    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
+</svg>
+        <span class="js-stale-session-flash-signed-in" hidden>You signed in with another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>
+        <span class="js-stale-session-flash-signed-out" hidden>You signed out in another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>
+        <span class="js-stale-session-flash-switched" hidden>You switched accounts on another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>
+
+    <button id="icon-button-de71748d-b027-4121-9ead-afc6269e78fb" aria-labelledby="tooltip-0b911745-2e0e-42f7-931b-682990b5a41c" type="button" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x Button-visual">
+    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+</button><tool-tip id="tooltip-0b911745-2e0e-42f7-931b-682990b5a41c" for="icon-button-de71748d-b027-4121-9ead-afc6269e78fb" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Dismiss alert</tool-tip>
+
+
+  
+</div>
+    </div>
+
+  <div id="start-of-content" class="show-on-focus"></div>
+
+
+
+
+
+
+
+
+    <div id="js-flash-container" class="flash-container" data-turbo-replace>
+
+
+
+
+  <template class="js-flash-template">
+    
+<div class="flash flash-full   {{ className }}">
+  <div >
+    <button autofocus class="flash-close js-flash-close" type="button" aria-label="Dismiss this message">
+      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
+    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+    </button>
+    <div aria-atomic="true" role="alert" class="js-flash-alert">
+      
+      <div>{{ message }}</div>
+
+    </div>
+  </div>
+</div>
+  </template>
+</div>
+
+
+    
+
+
+
+
+
+
+  <div
+    class="application-main "
+    data-commit-hovercards-enabled
+    data-discussion-hovercards-enabled
+    data-issue-and-pr-hovercards-enabled
+    data-project-hovercards-enabled
+  >
+        <div itemscope itemtype="http://schema.org/SoftwareSourceCode" class="">
+    <main id="js-repo-pjax-container" >
+      
+      
+
+
+
+
+
+
+  
+  <div id="repository-container-header"  class="pt-3 hide-full-screen" style="background-color: var(--page-header-bgColor, var(--color-page-header-bg));" data-turbo-replace>
+
+      <div class="d-flex flex-nowrap flex-justify-end mb-3  px-3 px-lg-5" style="gap: 1rem;">
+
+        <div class="flex-auto min-width-0 width-fit">
+            
+  <div class=" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal">
+      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo color-fg-muted mr-2">
+    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
+</svg>
+    
+    <span class="author flex-self-stretch" itemprop="author">
+      <a class="url fn" rel="author" data-hovercard-type="user" data-hovercard-url="/users/skumra/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/skumra">
+        skumra
+</a>    </span>
+    <span class="mx-1 flex-self-stretch color-fg-muted">/</span>
+    <strong itemprop="name" class="mr-2 flex-self-stretch">
+      <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="/skumra/robotic-grasping">robotic-grasping</a>
+    </strong>
+
+    <span></span><span class="Label Label--secondary v-align-middle mr-1">Public</span>
+  </div>
+
+
+        </div>
+
+        <div id="repository-details-container" class="flex-shrink-0" data-turbo-replace style="max-width: 70%;">
+            <ul class="pagehead-actions flex-shrink-0 d-none d-md-inline" style="padding: 2px 0;">
+    
+      
+
+  <li>
+            <a href="/login?return_to=%2Fskumra%2Frobotic-grasping" rel="nofollow" id="repository-details-watch-button" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="79caaf263607eae4f85bef0b58e5bc0ea5afbc16269149c6eb056f55c4e2985c" aria-label="You must be signed in to change notification settings" data-view-component="true" class="btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bell mr-2">
+    <path d="M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z"></path>
+</svg>Notifications
+</a>    <tool-tip id="tooltip-c228e65c-67bd-436f-b3c3-492b30e11b53" for="repository-details-watch-button" popover="manual" data-direction="s" data-type="description" data-view-component="true" class="sr-only position-absolute">You must be signed in to change notification settings</tool-tip>
+
+  </li>
+
+  <li>
+          <a icon="repo-forked" id="fork-button" href="/login?return_to=%2Fskumra%2Frobotic-grasping" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:200533757,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="3c4da6e57cae53092fd2ed626dbc181a74b1e63ed141806acb85ac585bc40c61" data-view-component="true" class="btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo-forked mr-2">
+    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
+</svg>Fork
+    <span id="repo-network-counter" data-pjax-replace="true" data-turbo-replace="true" title="137" data-view-component="true" class="Counter">137</span>
+</a>
+  </li>
+
+  <li>
+        <div data-view-component="true" class="BtnGroup d-flex">
+        <a href="/login?return_to=%2Fskumra%2Frobotic-grasping" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:200533757,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="10a8a462706559802d5f6608dade7414add94b291342c63a7195722941b7538d" aria-label="You must be signed in to star a repository" data-view-component="true" class="tooltipped tooltipped-sw btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-star v-align-text-bottom d-inline-block mr-2">
+    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
+</svg><span data-view-component="true" class="d-inline">
+          Star
+</span>          <span id="repo-stars-counter-star" aria-label="587 users starred this repository" data-singular-suffix="user starred this repository" data-plural-suffix="users starred this repository" data-turbo-replace="true" title="587" data-view-component="true" class="Counter js-social-count">587</span>
+</a></div>
+  </li>
+
+</ul>
+
+        </div>
+      </div>
+
+        <div id="responsive-meta-container" data-turbo-replace>
+</div>
+
+
+          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true" class="js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5">
+
+  <ul data-view-component="true" class="UnderlineNav-body list-style-none">
+      <li data-view-component="true" class="d-inline-flex">
+  <a id="code-tab" href="/skumra/robotic-grasping" data-tab-item="i0code-tab" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /skumra/robotic-grasping" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g c" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" aria-current="page" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected">
+    
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code UnderlineNav-octicon d-none d-sm-inline">
+    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+        <span data-content="Code">Code</span>
+          <span id="code-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>
+
+
+    
+</a></li>
+      <li data-view-component="true" class="d-inline-flex">
+  <a id="issues-tab" href="/skumra/robotic-grasping/issues" data-tab-item="i1issues-tab" data-selected-links="repo_issues repo_labels repo_milestones /skumra/robotic-grasping/issues" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g i" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
+    
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline">
+    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
+</svg>
+        <span data-content="Issues">Issues</span>
+          <span id="issues-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="40" data-view-component="true" class="Counter">40</span>
+
+
+    
+</a></li>
+      <li data-view-component="true" class="d-inline-flex">
+  <a id="pull-requests-tab" href="/skumra/robotic-grasping/pulls" data-tab-item="i2pull-requests-tab" data-selected-links="repo_pulls checks /skumra/robotic-grasping/pulls" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g p" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
+    
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline">
+    <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z"></path>
+</svg>
+        <span data-content="Pull requests">Pull requests</span>
+          <span id="pull-requests-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="0" hidden="hidden" data-view-component="true" class="Counter">0</span>
+
+
+    
+</a></li>
+      <li data-view-component="true" class="d-inline-flex">
+  <a id="actions-tab" href="/skumra/robotic-grasping/actions" data-tab-item="i3actions-tab" data-selected-links="repo_actions /skumra/robotic-grasping/actions" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g a" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
+    
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play UnderlineNav-octicon d-none d-sm-inline">
+    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
+</svg>
+        <span data-content="Actions">Actions</span>
+          <span id="actions-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>
+
+
+    
+</a></li>
+      <li data-view-component="true" class="d-inline-flex">
+  <a id="security-tab" href="/skumra/robotic-grasping/security" data-tab-item="i4security-tab" data-selected-links="security overview alerts policy token_scanning code_scanning /skumra/robotic-grasping/security" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g s" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
+    
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline">
+    <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
+</svg>
+        <span data-content="Security">Security</span>
+          <include-fragment src="/skumra/robotic-grasping/security/overall-count" accept="text/fragment+html" data-nonce="v2:efe89c02-1e51-251c-8658-f3b66a23ab93" data-view-component="true">
+  
+  <div data-show-on-forbidden-error hidden>
+    <div class="Box">
+  <div class="blankslate-container">
+    <div data-view-component="true" class="blankslate blankslate-spacious color-bg-default rounded-2">
+      
+
+      <h3 data-view-component="true" class="blankslate-heading">        Uh oh!
+</h3>
+      <p data-view-component="true">        <p class="color-fg-muted my-2 mb-2 ws-normal">There was an error while loading. <a class="Link--inTextBlock" data-turbo="false" href="" aria-label="Please reload this page">Please reload this page</a>.</p>
+</p>
+
+</div>  </div>
+</div>  </div>
+</include-fragment>
+
+    
+</a></li>
+      <li data-view-component="true" class="d-inline-flex">
+  <a id="insights-tab" href="/skumra/robotic-grasping/pulse" data-tab-item="i5insights-tab" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /skumra/robotic-grasping/pulse" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
+    
+              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline">
+    <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
+</svg>
+        <span data-content="Insights">Insights</span>
+          <span id="insights-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>
+
+
+    
+</a></li>
+</ul>
+    <div style="visibility:hidden;" data-view-component="true" class="UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0">      <action-menu data-select-variant="none" data-view-component="true">
+  <focus-group direction="vertical" mnemonics retain>
+    <button id="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-button" popovertarget="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-overlay" aria-controls="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-list" aria-haspopup="true" aria-labelledby="tooltip-3c6ecea3-bf05-418e-b8a2-5b162d1b85cd" type="button" data-view-component="true" class="Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-kebab-horizontal Button-visual">
+    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
+</svg>
+</button><tool-tip id="tooltip-3c6ecea3-bf05-418e-b8a2-5b162d1b85cd" for="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-button" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Additional navigation options</tool-tip>
+
+
+<anchored-position data-target="action-menu.overlay" id="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-overlay" anchor="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-button" align="start" side="outside-bottom" anchor-offset="normal" popover="auto" data-view-component="true">
+  <div data-view-component="true" class="Overlay Overlay--size-auto">
+    
+      <div data-view-component="true" class="Overlay-body Overlay-body--paddingNone">          <action-list>
+  <div data-view-component="true">
+    <ul aria-labelledby="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-button" id="action-menu-b82d328a-0b5c-43b1-89ba-8c65b6fe3e0d-list" role="menu" data-view-component="true" class="ActionListWrap--inset ActionListWrap">
+        <li hidden="hidden" data-menu-item="i0code-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
+    
+    
+    <a tabindex="-1" id="item-209c8382-8ada-457f-8c9e-9fcf99e9b891" href="/skumra/robotic-grasping" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
+        <span class="ActionListItem-visual ActionListItem-visual--leading">
+          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code">
+    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+        </span>
+      
+        <span data-view-component="true" class="ActionListItem-label">
+          Code
+</span>      
+</a>
+  
+</li>
+        <li hidden="hidden" data-menu-item="i1issues-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
+    
+    
+    <a tabindex="-1" id="item-5da4289d-2d59-4857-9003-eec67e20ad21" href="/skumra/robotic-grasping/issues" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
+        <span class="ActionListItem-visual ActionListItem-visual--leading">
+          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened">
+    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
+</svg>
+        </span>
+      
+        <span data-view-component="true" class="ActionListItem-label">
+          Issues
+</span>      
+</a>
+  
+</li>
+        <li hidden="hidden" data-menu-item="i2pull-requests-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
+    
+    
+    <a tabindex="-1" id="item-db234af3-1fd8-4135-a71a-47ca8b24bbea" href="/skumra/robotic-grasping/pulls" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
+        <span class="ActionListItem-visual ActionListItem-visual--leading">
+          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-pull-request">
+    <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z"></path>
+</svg>
+        </span>
+      
+        <span data-view-component="true" class="ActionListItem-label">
+          Pull requests
+</span>      
+</a>
+  
+</li>
+        <li hidden="hidden" data-menu-item="i3actions-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
+    
+    
+    <a tabindex="-1" id="item-ec9c8549-1b13-4e56-a7f1-1814fcb49aab" href="/skumra/robotic-grasping/actions" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
+        <span class="ActionListItem-visual ActionListItem-visual--leading">
+          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play">
+    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
+</svg>
+        </span>
+      
+        <span data-view-component="true" class="ActionListItem-label">
+          Actions
+</span>      
+</a>
+  
+</li>
+        <li hidden="hidden" data-menu-item="i4security-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
+    
+    
+    <a tabindex="-1" id="item-5596ba3d-cb03-4f45-bee8-e9d0a4a7a0b3" href="/skumra/robotic-grasping/security" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
+        <span class="ActionListItem-visual ActionListItem-visual--leading">
+          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield">
+    <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
+</svg>
+        </span>
+      
+        <span data-view-component="true" class="ActionListItem-label">
+          Security
+</span>      
+</a>
+  
+</li>
+        <li hidden="hidden" data-menu-item="i5insights-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
+    
+    
+    <a tabindex="-1" id="item-498e8f5b-4ff9-4039-8d78-d403b17cdade" href="/skumra/robotic-grasping/pulse" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
+        <span class="ActionListItem-visual ActionListItem-visual--leading">
+          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-graph">
+    <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
+</svg>
+        </span>
+      
+        <span data-view-component="true" class="ActionListItem-label">
+          Insights
+</span>      
+</a>
+  
+</li>
+</ul>    
+</div></action-list>
+
+
+</div>
+      
+</div></anchored-position>  </focus-group>
+</action-menu></div>
+</nav>
+
+  </div>
+
+  
+
+
+
+<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance" class="">
+    <div id="repo-content-pjax-container" class="repository-content " >
+    
+
+
+
+    
+      
+    
+
+
+
+
+
+
+
+
+<react-app
+  app-name="react-code-view"
+  initial-path="/skumra/robotic-grasping/tree/master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32"
+    style="display: block; min-height: calc(100vh - 64px);"
+  data-attempted-ssr="false"
+  data-ssr="false"
+  data-lazy="false"
+  data-alternate="false"
+  data-data-router-enabled="false"
+  data-react-profiling="false"
+>
+  
+  <script type="application/json" data-target="react-app.embeddedData">{"payload":{"allShortcutsEnabled":false,"path":"trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32","repo":{"id":200533757,"defaultBranch":"master","name":"robotic-grasping","ownerLogin":"skumra","currentUserCanPush":false,"isFork":false,"isEmpty":false,"createdAt":"2019-08-04T19:34:02.000Z","ownerAvatar":"https://avatars.githubusercontent.com/u/42680431?v=4","public":true,"private":false,"isOrgOwned":false},"currentUser":null,"refInfo":{"name":"master","listCacheKey":"v0:1632510933.126199","canEdit":false,"refType":"branch","currentOid":"183c6f68c44c1c7ff0f07707e2db6fcfd6840d2d"},"tree":{"items":[{"name":"arch.txt","path":"trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/arch.txt","contentType":"file"},{"name":"epoch_13_iou_0.96","path":"trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_13_iou_0.96","contentType":"file"},{"name":"epoch_15_iou_0.97","path":"trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_15_iou_0.97","contentType":"file"},{"name":"epoch_19_iou_0.98","path":"trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_19_iou_0.98","contentType":"file"}],"templateDirectorySuggestionUrl":null,"readme":null,"totalCount":4,"showBranchInfobar":false},"fileTree":{"trained-models":{"items":[{"name":"cornell-randsplit-rgbd-grconvnet3-drop1-ch16","path":"trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16","contentType":"directory"},{"name":"cornell-randsplit-rgbd-grconvnet3-drop1-ch32","path":"trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32","contentType":"directory"},{"name":"jacquard-d-grconvnet3-drop0-ch32","path":"trained-models/jacquard-d-grconvnet3-drop0-ch32","contentType":"directory"},{"name":"jacquard-rgbd-grconvnet3-drop0-ch32","path":"trained-models/jacquard-rgbd-grconvnet3-drop0-ch32","contentType":"directory"}],"totalCount":4},"":{"items":[{"name":"hardware","path":"hardware","contentType":"directory"},{"name":"inference","path":"inference","contentType":"directory"},{"name":"trained-models","path":"trained-models","contentType":"directory"},{"name":"utils","path":"utils","contentType":"directory"},{"name":".gitattributes","path":".gitattributes","contentType":"file"},{"name":".gitignore","path":".gitignore","contentType":"file"},{"name":"LICENSE","path":"LICENSE","contentType":"file"},{"name":"README.md","path":"README.md","contentType":"file"},{"name":"_config.yml","path":"_config.yml","contentType":"file"},{"name":"cleanup.sh","path":"cleanup.sh","contentType":"file"},{"name":"evaluate.py","path":"evaluate.py","contentType":"file"},{"name":"requirements.txt","path":"requirements.txt","contentType":"file"},{"name":"run_calibration.py","path":"run_calibration.py","contentType":"file"},{"name":"run_grasp_generator.py","path":"run_grasp_generator.py","contentType":"file"},{"name":"run_offline.py","path":"run_offline.py","contentType":"file"},{"name":"run_realtime.py","path":"run_realtime.py","contentType":"file"},{"name":"train_network.py","path":"train_network.py","contentType":"file"}],"totalCount":17}},"fileTreeProcessingTime":4.740084,"foldersToFetch":[],"treeExpanded":true,"symbolsExpanded":false,"copilotSWEAgentEnabled":false,"csrf_tokens":{"/skumra/robotic-grasping/branches":{"post":"NcqsiTT-25k70X1c2UlMRBfBD_PuBCYfzaD_-9CFmMOcXDhS233OAVMGuvcYmazpIHvq-iIBrIe2pCP4CH8bSg"},"/skumra/robotic-grasping/branches/fetch_and_merge/master":{"post":"gTPBeMUs2emgCfU8OA4IDsuawYDR-qZ6Ud_IPDYx6drDRk9_uhaqOqs7k9mNkl5zAqM-aMAlNexPxNVfgbCxoA"},"/skumra/robotic-grasping/branches/fetch_and_merge/master?discard_changes=true":{"post":"HPTs6R49u_DtVSnWi-3vLqUMdROCPRlmmIB_i08mVrVegWLuYQfII-ZnTzM-cblTbDWK-5PiivCGm2Lo-KcOzw"}}},"title":"robotic-grasping/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32 at master Â· skumra/robotic-grasping","appPayload":{"helpUrl":"https://docs.github.com","findFileWorkerPath":"/assets-cdn/worker/find-file-worker-9bd411a8e273.js","findInFileWorkerPath":"/assets-cdn/worker/find-in-file-worker-4747241b1152.js","githubDevUrl":null,"enabled_features":{"code_nav_ui_events":false,"react_blob_overlay":false,"accessible_code_button":true}}}</script>
+  <div data-target="react-app.reactRoot"></div>
+</react-app>
+</turbo-frame>
+
+
+
+  </div>
+
+</turbo-frame>
+
+    </main>
+  </div>
+
+  </div>
+
+          <footer class="footer pt-8 pb-6 f6 color-fg-muted p-responsive" role="contentinfo" >
+  <h2 class='sr-only'>Footer</h2>
+
+  
+
+
+  <div class="d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap">
+    <div class="d-flex flex-items-center flex-shrink-0 mx-2">
+      <a aria-label="GitHub Homepage" class="footer-octicon mr-2" href="https://github.com">
+        <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-mark-github">
+    <path d="M12 1C5.923 1 1 5.923 1 12c0 4.867 3.149 8.979 7.521 10.436.55.096.756-.233.756-.522 0-.262-.013-1.128-.013-2.049-2.764.509-3.479-.674-3.699-1.292-.124-.317-.66-1.293-1.127-1.554-.385-.207-.936-.715-.014-.729.866-.014 1.485.797 1.691 1.128.99 1.663 2.571 1.196 3.204.907.096-.715.385-1.196.701-1.471-2.448-.275-5.005-1.224-5.005-5.432 0-1.196.426-2.186 1.128-2.956-.111-.275-.496-1.402.11-2.915 0 0 .921-.288 3.024 1.128a10.193 10.193 0 0 1 2.75-.371c.936 0 1.871.123 2.75.371 2.104-1.43 3.025-1.128 3.025-1.128.605 1.513.221 2.64.111 2.915.701.77 1.127 1.747 1.127 2.956 0 4.222-2.571 5.157-5.019 5.432.399.344.743 1.004.743 2.035 0 1.471-.014 2.654-.014 3.025 0 .289.206.632.756.522C19.851 20.979 23 16.854 23 12c0-6.077-4.922-11-11-11Z"></path>
+</svg>
+</a>
+      <span>
+        &copy; 2025 GitHub,&nbsp;Inc.
+      </span>
+    </div>
+
+    <nav aria-label="Footer">
+      <h3 class="sr-only" id="sr-footer-heading">Footer navigation</h3>
+
+      <ul class="list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0" aria-labelledby="sr-footer-heading">
+
+          <li class="mx-2">
+            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}" href="https://docs.github.com/site-policy/github-terms/github-terms-of-service" data-view-component="true" class="Link--secondary Link">Terms</a>
+          </li>
+
+          <li class="mx-2">
+            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}" href="https://docs.github.com/site-policy/privacy-policies/github-privacy-statement" data-view-component="true" class="Link--secondary Link">Privacy</a>
+          </li>
+
+          <li class="mx-2">
+            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}" href="https://github.com/security" data-view-component="true" class="Link--secondary Link">Security</a>
+          </li>
+
+          <li class="mx-2">
+            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}" href="https://www.githubstatus.com/" data-view-component="true" class="Link--secondary Link">Status</a>
+          </li>
+
+          <li class="mx-2">
+            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to community&quot;,&quot;label&quot;:&quot;text:community&quot;}" href="https://github.community/" data-view-component="true" class="Link--secondary Link">Community</a>
+          </li>
+
+          <li class="mx-2">
+            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to docs&quot;,&quot;label&quot;:&quot;text:docs&quot;}" href="https://docs.github.com/" data-view-component="true" class="Link--secondary Link">Docs</a>
+          </li>
+
+          <li class="mx-2">
+            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}" href="https://support.github.com?tags=dotcom-footer" data-view-component="true" class="Link--secondary Link">Contact</a>
+          </li>
+
+          <li class="mx-2" >
+  <cookie-consent-link>
+    <button
+      type="button"
+      class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent"
+      data-action="click:cookie-consent-link#showConsentManagement"
+      data-analytics-event="{&quot;location&quot;:&quot;footer&quot;,&quot;action&quot;:&quot;cookies&quot;,&quot;context&quot;:&quot;subfooter&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;cookies_link_subfooter_footer&quot;}"
+    >
+       Manage cookies
+    </button>
+  </cookie-consent-link>
+</li>
+
+<li class="mx-2">
+  <cookie-consent-link>
+    <button
+      type="button"
+      class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent text-left"
+      data-action="click:cookie-consent-link#showConsentManagement"
+      data-analytics-event="{&quot;location&quot;:&quot;footer&quot;,&quot;action&quot;:&quot;dont_share_info&quot;,&quot;context&quot;:&quot;subfooter&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;dont_share_info_link_subfooter_footer&quot;}"
+    >
+      Do not share my personal information
+    </button>
+  </cookie-consent-link>
+</li>
+
+      </ul>
+    </nav>
+  </div>
+</footer>
+
+
+
+    <ghcc-consent id="ghcc" class="position-fixed bottom-0 left-0" style="z-index: 999999"
+      data-locale="en"
+      data-initial-cookie-consent-allowed=""
+      data-cookie-consent-required="false"
+    ></ghcc-consent>
+
+
+
+
+  <div id="ajax-error-message" class="ajax-error-message flash flash-error" hidden>
+    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
+    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
+</svg>
+    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
+      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
+    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+    </button>
+    You canâ€™t perform that action at this time.
+  </div>
+
+    <template id="site-details-dialog">
+  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open>
+    <summary role="button" aria-label="Close dialog"></summary>
+    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
+      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog>
+        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
+    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
+</svg>
+      </button>
+      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
+    </details-dialog>
+  </details>
+</template>
+
+    <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;">
+  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large" style="width:360px;">
+  </div>
+</div>
+
+    <template id="snippet-clipboard-copy-button">
+  <div class="zeroclipboard-container position-absolute right-0 top-0">
+    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
+      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
+    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
+</svg>
+      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
+    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
+</svg>
+    </clipboard-copy>
+  </div>
+</template>
+<template id="snippet-clipboard-copy-button-unpositioned">
+  <div class="zeroclipboard-container">
+    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
+      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
+    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
+</svg>
+      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
+    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
+</svg>
+    </clipboard-copy>
+  </div>
+</template>
+
+
+
+
+    </div>
+    <div id="js-global-screen-reader-notice" class="sr-only mt-n1" aria-live="polite" aria-atomic="true" ></div>
+    <div id="js-global-screen-reader-notice-assertive" class="sr-only mt-n1" aria-live="assertive" aria-atomic="true"></div>
+  </body>
+</html>
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/demo.py OWG-main/demo.py
--- OWG-upstream/demo.py	2025-12-25 13:40:46.502903512 +0800
+++ OWG-main/demo.py	2025-10-27 02:10:33.225886395 +0800
@@ -1,21 +1,65 @@
+import sys, os
+sys.path.append(os.path.dirname(__file__))  # âœ… åŠ å…¥å½“å‰è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ° owg_robot
+
+from grasp_6dof.grasp_sampler import sample_grasps
+from grasp_6dof.grasp_validator_panda import validate_grasps
 from owg_robot.ui import RobotEnvUI
 from owg.utils.config import load_config
-
 import argparse
-parser = argparse.ArgumentParser()
-parser.add_argument('--n_objects', help='Overload number of objects to load', type=int, default=None)
-parser.add_argument('--seed', help='Overload random seed', type=int, default=None)
-parser.add_argument('--vis', help='Overload visualizer plotting', type=int, default=None)
-parser.add_argument('--verbose', help='Overload VLM output verbocity', type=int, default=None)
+
+# å°† grconvnet ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„
+grconvnet_path = os.path.join(os.path.dirname(__file__), "third_party/grconvnet")
+if grconvnet_path not in sys.path:
+    sys.path.insert(0, grconvnet_path)
+
+# ========== âœ… æ–°å¢å‘½ä»¤è¡Œå‚æ•° ==========
+parser = argparse.ArgumentParser(description="Run OWG demo without Tkinter popup.")
+parser.add_argument('--n_objects', type=int, help='Number of objects to load', default=None)
+parser.add_argument('--seed', type=int, help='Random seed', default=None)
+parser.add_argument('--vis', type=int, help='Enable PyBullet visualizer', default=None)
+parser.add_argument('--verbose', type=int, help='Set verbosity level', default=None)
+parser.add_argument('--prompt', type=str, help='Language instruction for the grasp task', default="grasp the red cup")
 kwargs = vars(parser.parse_args())
 
+# ========== âœ… åŠ è½½é…ç½®æ–‡ä»¶ ==========
 cfg = load_config('./config/pyb/env.yaml')
 cfg.n_objects = kwargs['n_objects'] or cfg.n_objects
 cfg.seed = kwargs['seed'] or cfg.seed
 cfg.policy.vis = kwargs['vis'] or cfg.policy.vis
 cfg.policy.verbose = kwargs['verbose'] or cfg.policy.verbose
+
+# âœ… å°† prompt æ³¨å…¥é…ç½®ï¼ˆç»™åç»­ ui.py ä½¿ç”¨ï¼‰
+cfg.prompt = kwargs['prompt']
+print("\n=== Loaded Configuration ===")
 print(cfg)
+print("============================\n")
 
+# ========== âœ… è¿è¡Œ UI æ§åˆ¶å™¨ ==========
 demo = RobotEnvUI(cfg)
 
-demo.run()
\ No newline at end of file
+# ğŸ”¹ Monkey patch: ç›´æ¥æ³¨å…¥ promptï¼Œé¿å…è°ƒç”¨ Tkinter å¼¹çª—
+if hasattr(demo, "user_input") is False:
+    demo.user_input = cfg.prompt
+
+if hasattr(demo, "set_user_prompt"):
+    demo.set_user_prompt(cfg.prompt)
+
+print(f"[INFO] Using prompt: \"{cfg.prompt}\"")
+
+demo.run()
+
+# ========== âœ… é˜¶æ®µä¸€ï¼š6-DoF æŠ“å–é‡‡æ · + éªŒè¯ ==========
+if hasattr(cfg.policy, "enable_grasp_sampling") and cfg.policy.enable_grasp_sampling:
+    print("\n=== Stage 1: 6-DoF Grasp Sampling and Validation ===")
+    
+    # 1ï¸âƒ£ é‡‡æ ·æŠ“å–å§¿æ€
+    obj_path = "/home/lina/owg_env/lib/python3.10/site-packages/pybullet_data/sphere_smooth.obj"
+    sample_file = sample_grasps(obj_path, n_samples=100)
+    
+    # 2ï¸âƒ£ éªŒè¯æŠ“å–å¯è¡Œæ€§
+    validate_grasps(cfg, sample_file)
+
+    print("\nâœ… Stage 1 complete: grasp data generated successfully.")
+    print("Files saved in: grasp_6dof/dataset/")
+
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/env.yaml OWG-main/env.yaml
--- OWG-upstream/env.yaml	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/env.yaml	2025-11-24 21:23:06.992261924 +0800
@@ -0,0 +1,23 @@
+n_objects: 11
+n_grasp_attempts: 4
+n_action_attempts: 3
+finger_length: 0.06
+seed: 19
+
+camera:
+  center_x: 0.05
+  center_y: -0.52
+  center_z: 1.9
+  target_x: 0.05
+  target_y: -0.52
+  target_z: 0.785
+  znear: 0.2
+  zfar: 2.0
+  img_size: 448
+  fov: 40
+
+policy:
+  config_path: './config/pyb/OWG.yaml'
+  verbose: true
+  vis: false
+  use_grasp_ranker: true
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/gen_random_grasps_ycb.py OWG-main/gen_random_grasps_ycb.py
--- OWG-upstream/gen_random_grasps_ycb.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/gen_random_grasps_ycb.py	2025-11-29 11:25:06.887477437 +0800
@@ -0,0 +1,69 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+"""
+ç”Ÿæˆä¸å‡ ä½• grasp åŒæ ¼å¼çš„éšæœº 6-DoF æŠ“å–ï¼Œç”¨äº YCB baselineï¼š
+- è¯»å– objects_ycb.txt
+- æ¯ä¸ªç‰©ä½“ç”Ÿæˆ N=256 ä¸ªéšæœº grasps
+- ä¿å­˜åˆ° data/grasps_raw/{obj_id}_random_K256.json
+"""
+
+import json
+import math
+import os
+import random
+from pathlib import Path
+
+N_PER_OBJ = 256
+OBJ_LIST = "objects_ycb.txt"
+
+# éšæœºèŒƒå›´ï¼Œå¯æ ¹æ®éœ€è¦å¾®è°ƒ
+X_MIN, X_MAX = 0.35, 0.45
+Y_MIN, Y_MAX = -0.10, 0.10
+Z_OFFSET_MIN, Z_OFFSET_MAX = 0.02, 0.10
+WIDTH_MIN, WIDTH_MAX = 0.02, 0.08
+
+def sample_random_grasp(target_h: float):
+    """é‡‡æ ·ä¸€ä¸ªéšæœº graspï¼Œè¿”å›ä¸ validate_grasps_panda å…¼å®¹çš„ dict"""
+    x = random.uniform(X_MIN, X_MAX)
+    y = random.uniform(Y_MIN, Y_MAX)
+    z = random.uniform(target_h + Z_OFFSET_MIN, target_h + Z_OFFSET_MAX)
+    roll = random.uniform(-math.pi, math.pi)
+    pitch = random.uniform(-math.pi, math.pi)
+    yaw = random.uniform(-math.pi, math.pi)
+    width = random.uniform(WIDTH_MIN, WIDTH_MAX)
+
+    return {
+        "position": [x, y, z],
+        "rpy": [roll, pitch, yaw],
+        "width": width,
+        # æ‰“ä¸ªå ä½åˆ†æ•°ï¼Œåé¢ä¸ä¼šç”¨ï¼Œåªæ˜¯å­—æ®µè¦é½å…¨
+        "score": 0.0,
+        "meta": {"type": "random_baseline"}
+    }
+
+
+def main():
+    os.makedirs("data/grasps_raw", exist_ok=True)
+
+    with open(OBJ_LIST, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line or line.startswith("#"):
+                continue
+            oid, urdf_path, target_h = line.split()
+            target_h = float(target_h)
+
+            out_path = Path("data/grasps_raw") / f"{oid}_random_K{N_PER_OBJ}.json"
+            print(f"[INFO] Generating random grasps for {oid} -> {out_path}")
+
+            grasps = [sample_random_grasp(target_h) for _ in range(N_PER_OBJ)]
+
+            with open(out_path, "w", encoding="utf-8") as fout:
+                json.dump(grasps, fout, indent=2)
+
+    print("[INFO] Done.")
+
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/.gitignore OWG-main/.gitignore
--- OWG-upstream/.gitignore	2025-12-25 13:40:46.502903512 +0800
+++ OWG-main/.gitignore	2025-10-30 17:55:45.042092940 +0800
@@ -1,2 +1,25 @@
+# Python
 __pycache__/
-*.py[cod]
+*.pyc
+.venv/
+owg_env/
+
+# IDE
+.vscode/
+.idea/
+
+# PyBullet/Open3D ä¸´æ—¶è¾“å‡º
+grasp_6dof/out/
+grasp_6dof/dataset/*.png
+*.log
+
+# å¤§å‹æ•°æ®/æ¨¡å‹
+checkpoints/
+weights/
+*.pth
+*.pt
+*.onnx
+
+# OS
+.DS_Store
+Thumbs.db
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/.ipynb_checkpoints/lggsn_dataset-checkpoint.csv OWG-main/.ipynb_checkpoints/lggsn_dataset-checkpoint.csv
--- OWG-upstream/.ipynb_checkpoints/lggsn_dataset-checkpoint.csv	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/.ipynb_checkpoints/lggsn_dataset-checkpoint.csv	2025-11-22 23:02:21.843485888 +0800
@@ -0,0 +1,9 @@
+query,x,y,z,roll,pitch,yaw,width,obj_height,success,label,scene,source
+tomato soup,0.4344765342960289,-0.6842599277978338,0.7867337703704833,0.0,0.0,-1.457436442375183,0.0312635079026222,0.0017337799072265,1,1,owg_tray,ui_log_v1
+chips,0.4344765342960289,-0.6842599277978338,0.7867337703704833,0.0,0.0,-1.457436442375183,0.0312635079026222,0.0017337799072265,0,0,owg_tray,ui_log_v1
+chips,-0.0528880866425992,-0.7492418772563176,0.8541484117507934,0.0,0.0,-1.515838384628296,0.02957509085536,0.0691484212875366,1,1,owg_tray,ui_log_v1
+tennis,-0.0528880866425992,-0.7492418772563176,0.8541484117507934,0.0,0.0,-1.515838384628296,0.02957509085536,0.0691484212875366,0,0,owg_tray,ui_log_v1
+tennis,-0.1864620938628158,-0.5976173285198556,0.8410251379013061,0.0,0.0,-0.5004521608352661,0.0380451306700706,0.0560251474380493,1,1,owg_tray,ui_log_v1
+chips,-0.1864620938628158,-0.5976173285198556,0.8410251379013061,0.0,0.0,-0.5004521608352661,0.0380451306700706,0.0560251474380493,0,0,owg_tray,ui_log_v1
+sauce\,0.2214801444043322,-0.7420216606498194,0.8221800565719604,0.0,0.0,-0.4615610837936401,0.0482809133827686,0.0371800661087036,1,1,owg_tray,ui_log_v1
+tennis,0.2214801444043322,-0.7420216606498194,0.8221800565719604,0.0,0.0,-0.4615610837936401,0.0482809133827686,0.0371800661087036,0,0,owg_tray,ui_log_v1
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/.ipynb_checkpoints/patch-checkpoint.diff OWG-main/.ipynb_checkpoints/patch-checkpoint.diff
--- OWG-upstream/.ipynb_checkpoints/patch-checkpoint.diff	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/.ipynb_checkpoints/patch-checkpoint.diff	2025-11-14 21:52:33.562416587 +0800
@@ -0,0 +1,281 @@
+*** a/grasp_6dof/validate_grasps_panda.py
+--- b/grasp_6dof/validate_grasps_panda.py
+***************
+*** 1,6 ****
+  # -*- coding: utf-8 -*-
+  import pybullet as p
+  import pybullet_data
+  import numpy as np
+  import json
+  import time
+--- 1,7 ----
+  # -*- coding: utf-8 -*-
+  import pybullet as p
+  import pybullet_data
+  import numpy as np
+  import json
+  import time
++ import math
+  import argparse
+  import os
+  import random
+  from datetime import datetime
+***************
+*** 107,113 ****
+      LIFT_UP = 0.25
+      LIFT_SUCCESS_DZ = 0.05
+      JOINT_FORCE = float(joint_force)
+      IK_ITERS = int(ik_iters)
+      IK_ATTEMPTS = max(1, int(ik_attempts))
+  
+      # ---------- step å‡½æ•° ----------
+      if step_fn is None:
+          def step_fn(n):
+--- 108,116 ----
+      LIFT_UP = 0.25
+      LIFT_SUCCESS_DZ = 0.05
+      JOINT_FORCE = float(joint_force)
+      IK_ITERS = int(ik_iters)
+      IK_ATTEMPTS = max(1, int(ik_attempts))
++     CONTACT_NEG_MARGIN = 0.05  # æ‰æ¡Œ/æ•°å€¼çˆ†æ‰çš„ä¸‹ç•Œé˜ˆå€¼
+  
+      # ---------- step å‡½æ•° ----------
+      if step_fn is None:
+          def step_fn(n):
+***************
+*** 217,237 ****
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(0.20 * 480))
+  
+      if not contact:
+          cps = p.getContactPoints(bodyA=panda_id, bodyB=obj_id)
+          contact = any(c[3] in finger_ids for c in cps)
+  
+      # ---------- è½»æŠ¬ + äºŒæ¬¡æŒ¤å‹ ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + 0.015], orn=quat_target, steps=120)
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(SQUEEZE_EXTRA_TIME * 480))
+  
+      # ---------- æŠ¬å‡å¹¶åˆ¤å®š ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + LIFT_UP], orn=quat_target, steps=360)
+  
+      now_z = p.getBasePositionAndOrientation(obj_id)[0][2]
+      base_z = (init_base_z if init_base_z is not None else obj_pos0[2])
+!     lifted = (now_z - base_z) > LIFT_SUCCESS_DZ
+!     print(f"[DEBUG] contact={contact}, Î”z={now_z - base_z:.3f}, success={lifted}")
+  
+      # ---------- æ¾æ‰‹ ----------
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.04, force=200)
+      step_fn(int(0.20 * 480))
+  
+      return lifted
+--- 219,255 ----
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(0.20 * 480))
+  
+      if not contact:
+          cps = p.getContactPoints(bodyA=panda_id, bodyB=obj_id)
+          contact = any(c[3] in finger_ids for c in cps)
++     # æ²¡æ¥è§¦å°±ç›´æ¥å¤±è´¥å¹¶è½»æ”¾å›ï¼Œé¿å…â€œç›²æŠ¬â€æ¨ªæ‰«
++     if not contact:
++         for fid in finger_ids:
++             p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.04, force=200)
++         step_fn(int(0.15 * 480))
++         return False
+  
+      # ---------- è½»æŠ¬ + äºŒæ¬¡æŒ¤å‹ ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + 0.015], orn=quat_target, steps=120)
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(SQUEEZE_EXTRA_TIME * 480))
+  
+      # ---------- æŠ¬å‡å¹¶åˆ¤å®š ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + LIFT_UP], orn=quat_target, steps=360)
+  
+      now_z = p.getBasePositionAndOrientation(obj_id)[0][2]
+      base_z = (init_base_z if init_base_z is not None else obj_pos0[2])
+!     dz = float(now_z - base_z)
+!     fell_off = (dz < -CONTACT_NEG_MARGIN) or (abs(dz) > 0.5)
+!     lifted = (dz > LIFT_SUCCESS_DZ) and (not fell_off)
+!     print(f"[DEBUG] contact={contact}, Î”z={dz:.3f}, fell_off={fell_off}, success={lifted}")
+  
+      # ---------- æ¾æ‰‹ ----------
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.04, force=200)
+      step_fn(int(0.20 * 480))
+  
+      return lifted
+***************
+*** 287,300 ****
+      physicsClient = p.connect(p.GUI if (args.vis and not args.fast) else p.DIRECT)
+      p.setAdditionalSearchPath(pybullet_data.getDataPath())
+      p.setGravity(0, 0, -9.8)
+  
+      save_env_snapshot()
+  
+      # æ­¥è¿›å‡½æ•°ï¼šfast æ¨¡å¼å…³é—­ sleep ä¸”æŒ‰ scale å‡å°‘æ­¥æ•°
+      def step(n):
+          steps = int(max(1, n * (args.fast_scale if args.fast else 1.0)))
+          if args.fast or not args.vis:
+              for _ in range(steps):
+                  p.stepSimulation()
+          else:
+              for _ in range(steps):
+                  p.stepSimulation()
+                  time.sleep(1/480.0)
+--- 305,330 ----
+      physicsClient = p.connect(p.GUI if (args.vis and not args.fast) else p.DIRECT)
+      p.setAdditionalSearchPath(pybullet_data.getDataPath())
+      p.setGravity(0, 0, -9.8)
++     # â€”â€” ç‰©ç†ç¨³å®šæ€§/ç¡®å®šæ€§å‚æ•° â€”â€” 
++     p.setPhysicsEngineParameter(
++         deterministicOverlappingPairs=1,
++         collisionFilterMode=1,
++         contactSlop=1e-3,
++         enableConeFriction=1,
++         solverResidualThreshold=1e-7,
++         numSolverIterations=200,
++         erp=0.2, contactERP=0.2, frictionERP=0.2,
++         useSplitImpulse=1, splitImpulsePenetrationThreshold=-0.01
++     )
++     p.setTimeStep(1.0/480.0)
+  
+      save_env_snapshot()
+  
+      # æ­¥è¿›å‡½æ•°ï¼šfast æ¨¡å¼å…³é—­ sleep ä¸”æŒ‰ scale å‡å°‘æ­¥æ•°
+      def step(n):
+          steps = int(max(1, n * (args.fast_scale if args.fast else 1.0)))
+          if args.fast or not args.vis:
+              for _ in range(steps):
+                  p.stepSimulation()
+          else:
+              for _ in range(steps):
+                  p.stepSimulation()
+                  time.sleep(1/480.0)
+***************
+*** 306,311 ****
+--- 336,345 ----
+      table_id = p.loadURDF("table/table.urdf", basePosition=[0.5, 0, -0.63])
+      TABLE_TOP_Z = get_table_top_z(table_id)
+      print(f"[INFO] Detected table top z = {TABLE_TOP_Z:.3f}")
++     # æ¡Œé¢ä¹Ÿç»™é«˜æ‘©æ“¦å’Œä½å›å¼¹ï¼Œå‡å°‘è¢«â€œåˆ®è½æ¡Œâ€çš„æ¦‚ç‡
++     p.changeDynamics(table_id, -1,
++                      lateralFriction=1.6, rollingFriction=0.05, spinningFriction=0.02,
++                      restitution=0.0)
+  
+      # ç‰©ä½“ï¼šé™ç½®åœ¨æ¡Œé¢
+      CUBE_SCALE = float(args.cube_scale)
+      CUBE_HALF_Z = 0.5 * CUBE_SCALE
+***************
+*** 343,354 ****
+      except Exception:
+          grasps = []
+      if args.topk is not None and len(grasps) > 0:
+-         if "score" in grasps[0]:
+-             grasps = sorted(grasps, key=lambda g: g.get("score", 0.0), reverse=True)
+-         grasps = grasps[:args.topk]
++         # å¼ºåˆ¶å°è¯•æŒ‰ score æ’åºï¼Œå¥å£®å…œåº•
++         try:
++             grasps = sorted(grasps, key=lambda g: float(g.get("score", 0.0)), reverse=True)
++         except Exception:
++             pass
++         grasps = grasps[:args.topk]
+  
+      if len(grasps) == 0:
+          cx, cy, cz = p.getBasePositionAndOrientation(obj_id)[0]
+          z_above = cz + 0.12
+***************
+*** 384,392 ****
+          ok = grasp_with_panda(
+              obj_id, grasp, panda_id,
+              end_effector_index=END_EFFECTOR_INDEX,
+              finger_ids=finger_ids,
+              table_top_z=TABLE_TOP_Z,
+              init_base_z=init_base_z,
+              open_width_m=target_open,           # â† ä¼ è¿›å»
+              descent_step=args.descent_step,
+--- 418,427 ----
+          ok = grasp_with_panda(
+              obj_id, grasp, panda_id,
+              end_effector_index=END_EFFECTOR_INDEX,
+              finger_ids=finger_ids,
+              table_top_z=TABLE_TOP_Z,
+              init_base_z=init_base_z,
+              open_width_m=target_open,           # â† ä¼ è¿›å»
+              descent_step=args.descent_step,
+***************
+*** 396,402 ****
+              squeeze=args.squeeze,
+          )
+          out_g = dict(grasp); out_g["success"] = bool(ok)
+          validated.append(out_g)
+          if ok: success_count += 1
+          print(f"[{i+1}/{len(grasps)}] Grasp success = {ok}")
+--- 431,444 ----
+              squeeze=args.squeeze,
+          )
+          out_g = dict(grasp); out_g["success"] = bool(ok)
++         # ä»è°ƒè¯•æ‰“å°ä¸­å›å¡« contact / dz / fell_offï¼ˆå¦‚æœæœ‰å…¨å±€å˜é‡å°±ç•¥è¿‡ï¼Œä¿å®ˆè®¡ç®—ä¸€æ¬¡ï¼‰
++         cps_now = p.getContactPoints(bodyA=panda_id, bodyB=obj_id)
++         out_g["contact_seen"] = any(c[3] in finger_ids for c in cps_now)
++         now_z = p.getBasePositionAndOrientation(obj_id)[0][2]
++         out_g["dz"] = round(float(now_z - init_base_z), 4)
++         out_g["fell_off"] = bool((out_g["dz"] < -0.05) or (abs(out_g["dz"]) > 0.5))
++ 
+          validated.append(out_g)
+          if ok: success_count += 1
+          print(f"[{i+1}/{len(grasps)}] Grasp success = {ok}")
+***************
+*** 416,421 ****
+--- 458,488 ----
+      print(f"[INFO] Saved validated grasps â†’ {args.out}")
+  
+      fields = [
+          "time","obj","cube_scale","topk","seed",
+          "ee_index","ik_iters","ik_attempts","joint_force",
+          "descent_step","descend_clear","vel_close","pos_close","squeeze",
+          "fast","fast_scale",
+          "n_trials","success_count","success_rate"
+      ]
+      values = [
+          datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
+          args.obj, args.cube_scale, args.topk, args.seed,
+          args.ee_index, args.ik_iters, args.ik_attempts, args.joint_force,
+          args.descent_step, args.descend_clear, args.vel_close, args.pos_close, args.squeeze,
+          int(args.fast), args.fast_scale,
+          len(grasps), success_count, round(success_count/max(1,len(grasps)), 4)
+      ]
++     # ç‰©ä½“å½¢çŠ¶æ ‡ç­¾ï¼ˆè®°å½•åˆ° CSVï¼Œä¾¿äºåç»­åˆ†ç»„åˆ†æï¼‰
++     obj_aabb = p.getAABB(obj_id, -1)
++     dx = obj_aabb[1][0] - obj_aabb[0][0]
++     dy = obj_aabb[1][1] - obj_aabb[0][1]
++     dz = obj_aabb[1][2] - obj_aabb[0][2]
++     rx = abs(dx - dy) / max(1e-6, max(dx, dy))
++     ry = abs(dx - dz) / max(1e-6, max(dx, dz))
++     rz = abs(dy - dz) / max(1e-6, max(dy, dz))
++     obj_shape = "sphere_like" if (rx < 0.15 and ry < 0.15 and rz < 0.15) else "other"
++     fields += ["obj_shape"]
++     values += [obj_shape]
++ 
++     # è®°å½•æŠ“å–æ–‡ä»¶å’Œå£å¾„ tagï¼Œä¾¿äºè¿‡æ»¤å¯¹æ¯”
++     fields += ["grasps_path","tag"]
++     base_tag = f"{os.path.basename(args.obj)}_pc{int(args.pos_close)}_sq{args.squeeze}_topk{args.topk}"
++     gfile_tag = os.path.splitext(os.path.basename(args.grasps))[0] if args.grasps else "fallback"
++     values += [args.grasps, f"{base_tag}_{gfile_tag}"]
+  
+      append_summary_row(args.summary_c
+sv, fields, values)
+      print(f"[INFO] Appended summary â†’ {args.summary_csv}")
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/.ipynb_checkpoints/report-checkpoint.md OWG-main/.ipynb_checkpoints/report-checkpoint.md
--- OWG-upstream/.ipynb_checkpoints/report-checkpoint.md	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/.ipynb_checkpoints/report-checkpoint.md	2025-11-02 18:29:02.876162113 +0800
@@ -0,0 +1,61 @@
+# å®éªŒæŠ¥å‘Šï¼šé—¯å…³ç±»æ‰‹æ¸¸ç”¨æˆ·æµå¤±é¢„æµ‹
+
+## ä¸€ã€ä»»åŠ¡
+æ ¹æ® 2.1â€“2.4 çš„äº¤äº’æ—¥å¿—ï¼Œé¢„æµ‹æ¬¡å‘¨ï¼ˆ2.7â€“2.13ï¼‰æ˜¯å¦ç™»å½•ï¼ˆ1=æµå¤±ï¼Œ0=ç•™å­˜ï¼‰ã€‚è¯„ä»·æŒ‡æ ‡ï¼šAUCã€‚
+
+## äºŒã€æ•°æ®
+- `train.csv` / `dev.csv` / `test.csv`ï¼ˆå‡ä¸º **åˆ¶è¡¨ç¬¦åˆ†éš”**ï¼‰
+- `level_seq.csv`ï¼šé€å…³å¡åºåˆ—æ—¥å¿—ï¼Œå« `user_id, level_id, f_success, f_duration, f_reststep, f_help, time`
+- `level_meta.csv`ï¼šå…³å¡å±‚é¢ç»Ÿè®¡ï¼Œå« `level_id, f_avg_passrate, f_avg_duration, f_avg_win_duration, f_avg_retrytimes`
+- ï¼ˆå¯é€‰ï¼‰Groundtruth ç”¨äº Test å¤æ ¸
+
+## ä¸‰ã€æ–¹æ³•æ¦‚è¿°
+**æ€è·¯**ï¼šå°†åŸå§‹åºåˆ—æŒ‰ `user_id` èšåˆä¸ºç”¨æˆ·çº§ç‰¹å¾è¡¨ â†’ ä¼ ç»Ÿæ¨¡å‹å»ºæ¨¡ï¼ˆå¯è§£é‡Šã€é²æ£’ã€æ•ˆç‡é«˜ï¼‰â†’ å¤šæ¨¡å‹é›†æˆæå‡ä¸Šé™ã€‚
+
+### 3.1 ç‰¹å¾å·¥ç¨‹ï¼ˆç”¨æˆ·çº§èšåˆï¼‰
+- **ä½“é‡/å¤šæ ·æ€§**ï¼šæ€»æ¸¸ç©æ¬¡æ•° `plays_total`ã€ä¸åŒå…³å¡æ•° `levels_unique`ã€`level_max`
+- **ç»“æœè¡¨ç°**ï¼šæˆåŠŸæ¬¡æ•°/æˆåŠŸç‡ `succ_cnt/succ_rate`
+- **è¿‡ç¨‹å¼ºåº¦**ï¼šæ—¶é•¿ `dur_sum/dur_mean`ã€å‰©ä½™æ­¥æ•°å‡å€¼ `reststep_mean`ã€æ±‚åŠ©æ¯”ä¾‹ `help_rate`
+- **å…³å¡å…ƒä¿¡æ¯**ï¼ˆä¸ `level_meta` äº¤å‰ï¼‰ï¼šç”¨æˆ·ç»å†å…³å¡çš„å¹³å‡é€šå…³ç‡/å¹³å‡æ—¶é•¿/å¹³å‡èƒœåˆ©æ—¶é•¿/å¹³å‡é‡è¯•æ¬¡æ•°
+- **è¡ç”Ÿæ¯”ç‡**ï¼š`succ_per_play`ã€`dur_per_day` ç­‰
+> æ•°æ®æ¸…æ´—ï¼š`time` è§£æä¸º datetimeï¼Œä»…åœ¨èšåˆæ—¶ä½¿ç”¨ï¼Œæœ€ç»ˆè¾“å…¥æ¨¡å‹çš„å‡ä¸ºæ•°å€¼åˆ—ï¼Œé¿å… datetime ä¸ float æ··å‹å¯¼è‡´çš„ dtype é”™è¯¯ã€‚
+
+### 3.2 æ¨¡å‹ä¸è®­ç»ƒ
+- **åŸºå­¦ä¹ å™¨**ï¼š
+  - HistGradientBoostingï¼ˆæ ‘æ¨¡å‹ï¼Œèƒ½æ•æ‰éçº¿æ€§ã€äº¤äº’ï¼‰
+  - Logistic Regressionï¼ˆé…åˆç¼ºå¤±å€¼å¡«å……ä¸æ ‡å‡†åŒ–ï¼‰
+- **å †å ç­–ç•¥**ï¼š5 æŠ˜ Stratified KFold ç”Ÿæˆ OOFï¼ˆ`hgb_oof`ã€`lr_oof`ï¼‰â†’ ä½œä¸ºå…ƒç‰¹å¾å–‚ç»™ **å…ƒå­¦ä¹ å™¨ LR**
+- **è¯„ä¼°**ï¼šAUCï¼›ç»˜åˆ¶ ROC/PR ä»¥è§‚å¯Ÿé˜ˆå€¼å˜åŒ–ä¸‹çš„æ€§èƒ½
+
+## å››ã€å®éªŒç»“æœ
+- è®­ç»ƒæ ·æœ¬æ•°ï¼š**8158**
+- å¼€å‘é›†æ ·æœ¬æ•°ï¼š**2658**
+- æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š**2773**
+- æœ€ç»ˆç‰¹å¾ç»´åº¦ï¼š**55**
+- **Dev AUCï¼š0.799207**
+- ç”Ÿæˆæäº¤æ–‡ä»¶ï¼š`submission.csv`ï¼ˆç”± Notebook è‡ªåŠ¨å¯¼å‡ºï¼‰
+
+> è¯´æ˜ï¼šAUC åœ¨ 0.79â€“0.80 åŒºé—´çš„è½»å¾®æ³¢åŠ¨å±äºæ­£å¸¸ï¼ˆéšæœºç§å­ã€ç¯å¢ƒã€å¹¶è¡Œçº¿ç¨‹ç­‰ä¼šæœ‰ç»†å¾®å½±å“ï¼‰ã€‚
+
+## äº”ã€è¯¯å·®åˆ†æï¼ˆè¦ç‚¹ï¼‰
+- æ—©æœŸçŸ­æ—¶æ®µæ•°æ®å¯¹â€œé•¿æœŸç•™å­˜â€å­˜åœ¨å¤©ç„¶ä¸ç¡®å®šæ€§ï¼Œæ¨¡å‹å¯¹çŸ­é¢‘å¿«ç”¨æˆ·æ›´æ•æ„Ÿï¼›
+- å…³å¡ç‰¹å¾ä¸ç”¨æˆ·è¡Œä¸ºçš„äº¤äº’å°šæœªå……åˆ†å±•å¼€ï¼ˆä¾‹å¦‚åˆ†æ®µâ€œå¡å…³â€è¡Œä¸ºã€æœ€è¿‘è¡Œä¸ºè¡°å‡ï¼‰ï¼›
+- éƒ¨åˆ†ç”¨æˆ·æ•°æ®è¾ƒç¨€ç–ï¼Œèšåˆç‰¹å¾ä¸ç¨³å®šï¼Œå¯é€šè¿‡ç›®æ ‡ç¼–ç /è´å¶æ–¯å¹³æ»‘ç¼“è§£ã€‚
+
+## å…­ã€ä¸šåŠ¡è½åœ°å»ºè®®
+- ä»¥æ¦‚ç‡åˆ†å±‚ï¼šTop 10/20/30% é«˜é£é™©ç”¨æˆ·è¿›è¡Œä¸åŒåŠ›åº¦å¹²é¢„ï¼ˆå¥–åŠ±ã€çŸ­ä¿¡ã€Pushã€å®¢æœå›è®¿ï¼‰ï¼›
+- A/B éªŒè¯ï¼šä¸åŒå¹²é¢„ç­–ç•¥çš„ **å¢é‡ç•™å­˜ç‡** ä¸ **å•ä½æˆæœ¬**ï¼›
+- åœ¨çº¿ç›‘æ§ï¼šAUC ä¸ PSIï¼ˆç‰¹å¾åˆ†å¸ƒæ¼‚ç§»ï¼‰é¢„è­¦ï¼Œç‰ˆæœ¬æ›´æ–°æˆ–èŠ‚å‡æ—¥å‰åé‡ç‚¹å…³æ³¨ã€‚
+
+## ä¸ƒã€æ”¹è¿›æ–¹å‘ï¼ˆå¯å†²å‡» +0.5~1 ä¸ªç™¾åˆ†ç‚¹ AUCï¼‰
+1. **åºåˆ—ç‰¹å¾**ï¼šæœ€è¿‘ N å±€åŠ æƒã€æœ€é•¿å¤±è´¥è¿ä¸²ã€å¡å…³æ—¶é•¿/è§£é”é€Ÿåº¦ã€æ—¥/æ—¶æ®µèŠ‚å¾‹ï¼›
+2. **æ›´å¤šåŸºæ¨¡å‹**ï¼šLightGBM / CatBoost çº³å…¥å †å ï¼›
+3. **é˜ˆå€¼ä¸æ ¡å‡†**ï¼šPlatt / Isotonic æ ¡å‡†åæŒ‰ä¸šåŠ¡ç›®æ ‡ä¼˜åŒ–é˜ˆå€¼ï¼›
+4. **æ›´ç»†è‡´çš„å†·å¯åŠ¨å¤„ç†**ï¼šå®‰è£…æ¥æºã€è®¾å¤‡ã€æ¸ é“ç‰¹å¾ï¼ˆè‹¥å¯ç”¨ï¼‰ï¼›
+5. **K æŠ˜æ•°ä¸Šè°ƒ + å¤šæ¬¡é‡å¯**ï¼šé™ä½æ–¹å·®ï¼Œæäº¤é›†æˆå¹³å‡ã€‚
+
+## å…«ã€å¯å¤ç°å®éªŒæ–‡ä»¶
+- Notebookï¼š`final_project_notebook.ipynb`ï¼ˆåŒ…å«å®Œæ•´è®­ç»ƒ/è¯„ä¼°/æäº¤å¯¼å‡ºï¼‰
+- æäº¤ï¼š`submission.csv`
+- æŠ¥å‘Šï¼š`report.md`
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/lggsn_dataset.csv OWG-main/lggsn_dataset.csv
--- OWG-upstream/lggsn_dataset.csv	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/lggsn_dataset.csv	2025-11-23 10:49:58.362143136 +0800
@@ -0,0 +1,43 @@
+query,x,y,z,roll,pitch,yaw,width,obj_height,success,label,scene,source
+tomato soup,0.4344765342960289,-0.6842599277978338,0.7867337703704833,0.0,0.0,-1.457436442375183,0.0312635079026222,0.0017337799072265,1,1,owg_tray,ui_log_auto_v1
+chips,-0.0528880866425992,-0.7492418772563176,0.8541484117507934,0.0,0.0,-1.515838384628296,0.02957509085536,0.0691484212875366,1,1,owg_tray,ui_log_auto_v1
+tennis,-0.1864620938628158,-0.5976173285198556,0.8410251379013061,0.0,0.0,-0.5004521608352661,0.0380451306700706,0.0560251474380493,1,1,owg_tray,ui_log_auto_v1
+sauce\,0.2214801444043322,-0.7420216606498194,0.8221800565719604,0.0,0.0,-0.4615610837936401,0.0482809133827686,0.0371800661087036,1,1,owg_tray,ui_log_auto_v1
+a tennis ball,-0.1864620938628158,-0.5976173285198556,0.8410254955291747,0.0,0.0,-0.5004955530166626,0.0380467362701892,0.0560255050659179,1,1,owg_tray,ui_log_auto_v1
+hammer,-0.2153429602888086,-0.5398555956678701,0.7849132061004638,0.0,0.0,0.2275583744049072,0.0429496020078659,-8.678436279296875e-05,1,1,owg_tray,ui_log_auto_v1
+soda can,-0.2117328519855595,-0.4459927797833935,0.8689109325408935,0.0,0.0,0.2028610706329345,0.041665568947792,0.0839109420776367,1,1,owg_tray,ui_log_auto_v1
+tennis ball,-0.1720216606498194,-0.5976173285198556,0.8434596300125121,0.0,0.0,-0.5555682182312012,0.0364583767950534,0.0584596395492553,1,1,owg_tray,ui_log_auto_v1
+hammer,-0.226173285198556,-0.3340794223826715,0.8118072986602782,0.0,0.0,-0.6088788509368896,0.0362338796257972,0.0268073081970214,1,1,owg_tray,ui_log_auto_v1
+a can of potato chips,-0.0528880866425992,-0.763682310469314,0.8541196823120116,0.0,0.0,-1.508307933807373,0.0300462804734706,0.0691196918487548,1,1,owg_tray,ui_log_auto_v1
+black clamp,0.0518050541516245,-0.5037545126353791,0.8215353727340697,0.0,0.0,-0.931288719177246,0.037019096314907,0.0365353822708129,1,1,owg_tray,ui_log_auto_v1
+mustard,0.1240072202166065,-0.3629602888086642,0.8395127296447753,0.0,0.0,-0.6612963676452637,0.0533100813627243,0.0545127391815185,1,1,owg_tray,ui_log_auto_v1
+tomato soup,0.2720216606498195,-0.3449097472924187,0.8670435190200805,0.0,0.0,-0.730776309967041,0.0395803786814212,0.0820435285568237,1,1,owg_tray,ui_log_auto_v1
+scissors,0.178158844765343,-0.7420216606498194,0.8130946397781371,0.0,0.0,-0.2838515043258667,0.0313345380127429,0.0280946493148803,1,1,owg_tray,ui_log_auto_v1
+a small tin can,0.1276173285198556,-0.5867870036101083,0.8582303762435912,0.0,0.0,-0.9863321781158448,0.0335012711584568,0.0732303857803344,1,1,owg_tray,ui_log_auto_v1
+a can of soup,0.4344765342960289,-0.6842599277978338,0.7867293596267699,0.0,0.0,-1.4578100442886353,0.0312478747218847,0.0017293691635131,1,1,owg_tray,ui_log_auto_v1
+crackers,0.2684115523465705,-0.5218050541516245,0.970240557193756,0.0,0.0,-1.4209858179092407,0.0476920418441295,0.1852405667304992,1,1,owg_tray,ui_log_auto_v1
+scissors,0.1203971119133574,-0.7348014440433213,0.7849132061004638,0.0,0.0,-0.5891704559326172,0.0222776476293802,-8.678436279296875e-05,1,1,owg_tray,ui_log_auto_v1
+a black clamp,0.0518050541516245,-0.5037545126353791,0.8215357303619384,0.0,0.0,-0.9312987923622132,0.0370140261948108,0.0365357398986816,1,1,owg_tray,ui_log_auto_v1
+mustard,0.3081227436823104,-0.7059205776173285,0.9067924857139588,0.0,0.0,-0.817054808139801,0.0487855933606624,0.1217924952507019,0,0,owg_tray,ui_log_auto_v1
+tomato soup,0.3911552346570398,-0.7384115523465704,0.8504417181015014,0.0,0.0,0.8161735534667969,0.0267351884394884,0.0654417276382446,1,1,owg_tray,ui_log_auto_v1
+tennis,-0.1864620938628158,-0.5976173285198556,0.8410251379013061,0.0,0.0,-0.5010828971862793,0.0380470044910907,0.0560251474380493,1,1,owg_tray,ui_log_auto_v1
+a hammer,-0.2009025270758123,-0.5254151624548737,0.8097576141357421,0.0,0.0,0.238574743270874,0.0440863743424415,0.0247576236724853,1,1,owg_tray,ui_log_auto_v1
+a soda can,0.1276173285198556,-0.5867870036101083,0.8582613706588744,0.0,0.0,-0.9705398082733154,0.032991986721754,0.0732613801956176,1,1,owg_tray,ui_log_auto_v1
+chips,0.1023465703971119,-0.7745126353790613,0.8435830116271972,0.0,0.0,-0.8780084252357483,0.0343284010887146,0.0585830211639404,1,1,owg_tray,ui_log_auto_v1
+meat,0.3045126353790613,-0.3665703971119133,0.8650737047195434,0.0,0.0,0.3856196403503418,0.0473958104848861,0.0800737142562866,1,1,owg_tray,ui_log_auto_v1
+tomato soup,0.438086642599278,-0.6806498194945848,0.7849132061004638,0.0,0.0,-1.4349985122680664,0.0320225171744823,-8.678436279296875e-05,1,1,owg_tray,ui_log_auto_v1
+scissors,0.1203971119133574,-0.7348014440433213,0.7849132061004638,0.0,0.0,-0.5892026424407959,0.0222722981125116,-8.678436279296875e-05,1,1,owg_tray,ui_log_auto_v1
+mustard,0.3370036101083033,-0.7059205776173285,0.8394084215164184,0.0,0.0,-0.8350943326950073,0.0460659153759479,0.0544084310531616,1,1,owg_tray,ui_log_auto_v1
+black clamp,0.0518050541516245,-0.5037545126353791,0.8215342998504638,0.0,0.0,-0.9313108921051024,0.0370136238634586,0.036534309387207,1,1,owg_tray,ui_log_auto_v1
+a can of potato chips,-0.0528880866425992,-0.7492418772563176,0.8541484117507934,0.0,0.0,-1.515838384628296,0.02957509085536,0.0691484212875366,1,1,owg_tray,ui_log_auto_v1
+hammer,-0.2514440433212996,-0.4965342960288809,0.8118567705154418,0.0,0.0,0.2555391788482666,0.0470304079353809,0.026856780052185,1,1,owg_tray,ui_log_auto_v1
+soda,0.1203971119133574,-0.5831768953068592,0.8560760259628295,0.0,0.0,-0.8874908089637756,0.0344622172415256,0.0710760354995727,1,1,owg_tray,ui_log_auto_v1
+meatmm,0.2250902527075812,-0.427942238267148,0.8319756031036376,0.0,0.0,0.1641242504119873,0.0471047721803188,0.0469756126403808,1,1,owg_tray,ui_log_auto_v1
+the tennis ball,-0.1864620938628158,-0.5976173285198556,0.8410254955291747,0.0,0.0,-0.5004955530166626,0.0380467362701892,0.0560255050659179,1,1,owg_tray,ui_log_auto_v1
+the hammer,-0.2153429602888086,-0.5398555956678701,0.7849132061004638,0.0,0.0,0.2275545597076416,0.0429489724338054,-8.678436279296875e-05,1,1,owg_tray,ui_log_auto_v1
+the blue can,-0.2839350180505414,-0.792563176895307,0.8550154209136962,0.0,0.0,-0.0718905925750732,0.0266040470451116,0.0700154304504394,1,1,owg_tray,ui_log_auto_v1
+the can of potato chips,0.0157039711191336,-0.7203610108303249,0.8514095783233642,0.0,0.0,1.402458906173706,0.0355671532452106,0.0664095878601074,1,1,owg_tray,ui_log_auto_v1
+the black clamp,0.1095667870036101,-0.3051985559566786,0.8300323724746703,0.0,0.0,-0.7305395007133484,0.0174464769661426,0.0450323820114135,1,1,owg_tray,ui_log_auto_v1
+scissors,-0.1828519855595667,-0.6373285198555956,0.8080287218093871,0.0,0.0,-0.6776494383811951,0.0217611845582723,0.0230287313461303,1,1,owg_tray,ui_log_auto_v1
+tomato soup,0.3586642599277979,-0.7131407942238267,0.8503333568572997,0.0,0.0,0.2152206897735595,0.0412148013710975,0.0653333663940429,1,1,owg_tray,ui_log_auto_v1
+lunch meat,0.1167870036101083,-0.5831768953068592,0.8579178094863891,0.0,0.0,-0.8495091199874878,0.0344087332487106,0.0729178190231323,1,1,owg_tray,ui_log_auto_v1
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/lggsn_model.py OWG-main/lggsn_model.py
--- OWG-upstream/lggsn_model.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/lggsn_model.py	2025-11-23 13:35:34.051478514 +0800
@@ -0,0 +1,56 @@
+# lggsn_model.py
+# ç®€åŒ–ç‰ˆ LGGSNï¼šåªç”¨å‡ ä½•ç‰¹å¾åšæŠ“å–å¥½åé¢„æµ‹
+
+import torch
+import torch.nn as nn
+
+
+class LGGSN(nn.Module):
+    """
+    Lightweight Grasp Geometry Scoring Network.
+
+    ç›®å‰åªç”¨å‡ ä½• + è´¨é‡ç‰¹å¾ (geom) æ¥é¢„æµ‹æŠ“å–è´¨é‡ labelã€‚
+    é¢„ç•™äº† query embedding æ¥å£ï¼Œæ–¹ä¾¿ä»¥åæ¥è¯­è¨€/ç±»ä¿¡æ¯ã€‚
+
+    Args:
+        n_queries: æŸ¥è¯¢ id çš„æ€»æ•°ï¼ˆç°åœ¨ç”¨ä¸åˆ°ï¼Œå…ˆç•™æ¥å£ï¼‰
+        geom_dim:  å‡ ä½•ç‰¹å¾ç»´åº¦ï¼ˆ= feature_cols çš„é•¿åº¦ï¼‰
+        query_dim: query embedding ç»´åº¦ï¼ˆç°åœ¨æˆ‘ä»¬è®¾æˆ 0ï¼Œç›¸å½“äºä¸ç”¨ï¼‰
+        hidden_dim: MLP éšè—å±‚ç»´åº¦
+    """
+    def __init__(
+        self,
+        n_queries: int,
+        geom_dim: int = 12,
+        query_dim: int = 0,
+        hidden_dim: int = 40,
+    ):
+        super().__init__()
+        self.use_query = query_dim > 0
+
+        if self.use_query:
+            self.query_emb = nn.Embedding(n_queries, query_dim)
+        else:
+            self.query_emb = None
+
+        in_dim = geom_dim + (query_dim if self.use_query else 0)
+
+        self.mlp = nn.Sequential(
+            nn.Linear(in_dim, hidden_dim),
+            nn.ReLU(inplace=True),
+            nn.Linear(hidden_dim, 1),
+        )
+
+    def forward(self, geom: torch.Tensor, query_id: torch.Tensor):
+        """
+        geom: [B, geom_dim]
+        query_id: [B]ï¼Œå¦‚æœ use_query=False ä¼šè¢«å¿½ç•¥
+        """
+        x = geom
+        if self.use_query:
+            q = self.query_emb(query_id)  # [B, query_dim]
+            x = torch.cat([geom, q], dim=-1)
+
+        logit = self.mlp(x).squeeze(-1)  # [B]
+        return logit
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/lggsn_multimodal.py OWG-main/lggsn_multimodal.py
--- OWG-upstream/lggsn_multimodal.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/lggsn_multimodal.py	2025-11-25 13:31:47.521557859 +0800
@@ -0,0 +1,135 @@
+"""
+lggsn_multimodal.py
+
+Multimodal LG-GSN:
+- Geometry-only grasp features (from existing LGGSN)
+- Semantic 3D point cloud features (Stage 3 semantic_pc)
+- Text features (query embedding, placeholder for now)
+"""
+
+from typing import Tuple
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class SemanticPointEncoder(nn.Module):
+    """
+    Encode semantic point cloud P âˆˆ R^{NÃ—3} into a global feature g_sem âˆˆ R^{D}.
+
+    Very simple PointNet-style encoder:
+        - per-point MLP
+        - global max-pooling
+        - linear projection
+    """
+
+    def __init__(
+        self,
+        in_dim: int = 3,
+        hidden_dims: Tuple[int, int, int] = (64, 128, 256),
+        out_dim: int = 256,
+    ):
+        super().__init__()
+        layers = []
+        last = in_dim
+        for h in hidden_dims:
+            layers.append(nn.Linear(last, h))
+            layers.append(nn.ReLU(inplace=True))
+            last = h
+        self.mlp = nn.Sequential(*layers)
+        self.proj = nn.Linear(last, out_dim)
+
+    def forward(self, pts: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            pts: (B, N, 3) point cloud
+
+        Returns:
+            feat: (B, out_dim)
+        """
+        # (B, N, C)
+        x = self.mlp(pts)
+        # global max-pooling over points -> (B, C)
+        x = x.max(dim=1).values
+        # projection -> (B, out_dim)
+        x = self.proj(x)
+        return x
+
+
+class TextEncoderPlaceholder(nn.Module):
+    """
+    Placeholder text encoder.
+
+    In a real system you would:
+      - either pre-compute CLIP / OpenAI embeddings and feed them here
+      - or replace this with a frozen language model encoder
+
+    For now we just take a precomputed embedding of dim=in_dim and
+    project it to dim=out_dim.
+    """
+
+    def __init__(self, in_dim: int = 512, out_dim: int = 256):
+        super().__init__()
+        self.proj = nn.Linear(in_dim, out_dim)
+
+    def forward(self, text_emb: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            text_emb: (B, in_dim) precomputed text embedding
+
+        Returns:
+            feat: (B, out_dim)
+        """
+        return self.proj(text_emb)
+
+
+class LggsnMultimodal(nn.Module):
+    """
+    Multimodal LG-GSN head.
+
+    It takes three kinds of features:
+      - Geometric grasp feature g_geom âˆˆ R^{Dg}
+      - Semantic point cloud feature g_sem âˆˆ R^{Ds}
+      - Text feature g_text âˆˆ R^{Dt}
+
+    and predicts a scalar success score y_hat âˆˆ (0,1) via a small MLP.
+    """
+
+    def __init__(
+        self,
+        dim_geom: int = 40,
+        dim_sem: int = 256,
+        dim_text: int = 256,
+        hidden_dim: int = 256,
+    ):
+        super().__init__()
+        in_dim = dim_geom + dim_sem + dim_text
+        self.mlp = nn.Sequential(
+            nn.Linear(in_dim, hidden_dim),
+            nn.ReLU(inplace=True),
+            nn.Linear(hidden_dim, hidden_dim),
+            nn.ReLU(inplace=True),
+            nn.Linear(hidden_dim, 1),
+        )
+
+    def forward(
+        self,
+        geom_feat: torch.Tensor,
+        sem_feat: torch.Tensor,
+        text_feat: torch.Tensor,
+    ) -> torch.Tensor:
+        """
+        Args:
+            geom_feat: (B, Dg) grasp geometry feature
+            sem_feat:  (B, Ds) semantic point cloud feature
+            text_feat: (B, Dt) text feature
+
+        Returns:
+            prob: (B,) predicted success probability in [0,1]
+        """
+        x = torch.cat([geom_feat, sem_feat, text_feat], dim=-1)
+        logit = self.mlp(x).squeeze(-1)  # (B,)
+        prob = torch.sigmoid(logit)
+        return prob
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/lggsn_pairs.csv OWG-main/lggsn_pairs.csv
--- OWG-upstream/lggsn_pairs.csv	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/lggsn_pairs.csv	2025-11-22 19:09:05.187609743 +0800
@@ -0,0 +1,9 @@
+query,x,y,z,roll,pitch,yaw,width,obj_height,label
+tomato soup,0.4344765342960289,-0.6842599277978338,0.7867337703704833,0.0,0.0,-1.457436442375183,0.0312635079026222,0.0017337799072265,1
+chips,0.4344765342960289,-0.6842599277978338,0.7867337703704833,0.0,0.0,-1.457436442375183,0.0312635079026222,0.0017337799072265,0
+chips,-0.0528880866425992,-0.7492418772563176,0.8541484117507934,0.0,0.0,-1.515838384628296,0.02957509085536,0.0691484212875366,1
+tennis,-0.0528880866425992,-0.7492418772563176,0.8541484117507934,0.0,0.0,-1.515838384628296,0.02957509085536,0.0691484212875366,0
+tennis,-0.1864620938628158,-0.5976173285198556,0.8410251379013061,0.0,0.0,-0.5004521608352661,0.0380451306700706,0.0560251474380493,1
+chips,-0.1864620938628158,-0.5976173285198556,0.8410251379013061,0.0,0.0,-0.5004521608352661,0.0380451306700706,0.0560251474380493,0
+sauce\,0.2214801444043322,-0.7420216606498194,0.8221800565719604,0.0,0.0,-0.4615610837936401,0.0482809133827686,0.0371800661087036,1
+tennis,0.2214801444043322,-0.7420216606498194,0.8221800565719604,0.0,0.0,-0.4615610837936401,0.0482809133827686,0.0371800661087036,0
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/objects_ycb.txt OWG-main/objects_ycb.txt
--- OWG-upstream/objects_ycb.txt	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/objects_ycb.txt	2025-11-29 15:57:38.623883586 +0800
@@ -0,0 +1,10 @@
+# obj_id        urdf_path                          target_h
+cracker_box     YCB_Dataset/ycb/cracker_box.urdf       0.18
+sugar_box       YCB_Dataset/ycb/sugar_box.urdf         0.17
+tomato_soup     YCB_Dataset/ycb/tomato_soup_can.urdf   0.10
+mustard         YCB_Dataset/ycb/mustard_bottle.urdf    0.18
+potted_meat     YCB_Dataset/ycb/potted_meat_can.urdf   0.10
+bowl            YCB_Dataset/ycb/bowl.urdf              0.07
+mug             YCB_Dataset/ycb/mug.urdf               0.10
+bleach          YCB_Dataset/ycb/bleach_cleanser.urdf   0.22
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg/gpt_utils0.py OWG-main/owg/gpt_utils0.py
--- OWG-upstream/owg/gpt_utils0.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/owg/gpt_utils0.py	2025-05-11 05:27:58.000000000 +0800
@@ -0,0 +1,179 @@
+import os
+import numpy as np
+import base64
+import requests
+from io import BytesIO
+from typing import List, Union, Optional
+from PIL import Image
+
+# Get OpenAI API Key from environment variable
+openai_api_key = os.environ['OPENAI_API_KEY']
+
+API_URL = "https://api.openai.com/v1/chat/completions"
+
+
+def encode_image_to_base64(image) -> str:
+    """
+    Encodes an image into a base64-encoded string in JPEG format.
+
+    Parameters:
+        image (np.ndarray): The image to be encoded. This will be a string
+        of the image path or a PIL image
+
+    Returns:
+        str: A base64-encoded string representing the image in JPEG format.
+    """
+    # Function to encode the image
+    def _encode_image_from_file(image_path):
+        # Function to encode the image
+        with open(image_path, 'rb') as image_file:
+            return base64.b64encode(image_file.read()).decode('utf-8')
+    def _encode_image_from_pil(image):
+        buffered = BytesIO()
+        image.save(buffered, format='JPEG')
+        return base64.b64encode(buffered.getvalue()).decode('utf-8')
+    
+    if isinstance(image, str):
+        return _encode_image_from_file(image)
+    elif isinstance(image, Image.Image):
+        return _encode_image_from_pil(image)
+    elif isinstance(image, np.ndarray):
+        image_pil = Image.fromarray(image)
+        return _encode_image_from_pil(image_pil)
+    else:
+        raise ValueError(f"Unknown option for image {type(image)}")
+
+
+def prepare_prompt(
+    images: List[Union[Image.Image, np.ndarray]],
+    prompt: Optional[str] = None,
+    in_context_examples: Optional[dict] = None
+  ) -> dict:
+  
+  def _append_pair(current_prompt, images, text):
+    # text first if given, then image.
+    if text:
+      current_prompt['content'].append({
+          'type': 'text',
+          'text': text
+        })
+    else:
+      assert len(images) > 0, "Both images and text prompts are empty."
+
+    for image in images:
+      base64_image = encode_image_to_base64(image)
+      current_prompt['content'].append({
+              "type": "image_url",
+              "image_url": {
+                  "url": f"data:image/jpeg;base64,{base64_image}",
+                  #"detail": "low"
+          }
+        })
+    return current_prompt
+
+  set_prompt = {
+    'role': 'user',
+    'content': []
+  }
+
+  # Include in-context examples if provided
+  if in_context_examples:
+    for example in in_context_examples:
+      _append_pair(
+        set_prompt, example['images'], example['prompt'])
+      # interleave response
+      set_prompt['content'].append({
+          'type': 'text',
+          'text': f"The answer should be: {example['response']}\n"
+      })
+    
+  # add user prompt
+  _append_pair(set_prompt, images, prompt)
+
+  return set_prompt
+
+
+# def prepare_prompt(
+#         images: List[np.ndarray], 
+#         prompt: Optional[str] = None, 
+#         detail: str = "auto"
+# ) -> dict:
+#    # text prompt always goes first, then images prompt
+#     set_user_prompt = {
+#         "role": "user",
+#         "content": []
+#     }
+
+#     if not text_prompt:
+#       assert len(images) > 0, "Image and text prompts are both empty."
+#     else:
+#       set_user_prompt["content"].append({
+#         "type": "text",
+#         "text": prompt
+#       })
+
+#     # If there are no images, return a simple text prompt
+#     if not images:
+#         return set_user_prompt
+    
+#     # Otherwise, prepare prompt with images    
+#     for image in images:
+#         base64_image = encode_image_to_base64(image)
+#         image_prompt = {
+#             "type": "image_url",
+#             "image_url": {
+#                 "url": f"data:image/jpeg;base64,{base64_image}",
+#                 "detail": detail
+#             }
+#         }
+#         set_user_prompt["content"].append(image_prompt)
+#     return set_user_prompt
+
+
+def compose_payload(images: List[np.ndarray], prompt: str, system_prompt: str, detail: str, temperature: float, max_tokens: int, n: int, model_name: str = "gpt-4o", return_logprobs: bool = False, in_context_examples: List[dict] = None, seed: Optional[int] = None) -> dict:
+    # Prepare system message
+    system_msg = {
+                "role": "system",
+                "content": system_prompt  # plain text, not a list
+    }
+    messages = [system_msg]
+    # Prepare prompt message, potentially with in-context examples
+    msg = prepare_prompt(
+      images, prompt, in_context_examples)
+    messages.append(msg)
+    
+    payload = {
+        "model": model_name,
+        "messages": messages,
+        "max_tokens": max_tokens,
+        "temperature": temperature,
+        "n": n,
+        "logprobs": return_logprobs,
+    }
+    # reproducable output?
+    if seed is not None:
+      payload["seed"] = seed
+    return payload
+
+
+def request_gpt(images: Union[np.ndarray, List[np.ndarray]], prompt: str, system_prompt: str, detail: str = "auto", temp: float = 0.0, n_tokens: int = 256, n: int = 1, return_logprobs: bool = False, in_context_examples: List[dict] = None, model_name: str = "gpt-4o", seed: Optional[int] = None) -> str:
+    headers = {
+        "Content-Type": "application/json",
+        "Authorization": f"Bearer {openai_api_key}"
+    }
+    # convert single image prompt to multiple for compatibility
+    if not isinstance(images, List):
+        assert isinstance(images, np.ndarray), "Provide either a numpy array, a PIL image, an image path string or a list of the above."
+        images = [images]
+    
+    payload = compose_payload(images=images, prompt=prompt, detail=detail, system_prompt=system_prompt, n=n, temperature=temp, max_tokens=n_tokens, return_logprobs=return_logprobs, in_context_examples=in_context_examples, model_name=model_name)
+    response = requests.post(url=API_URL, headers=headers, json=payload).json()
+    
+    if 'error' in response:
+        raise ValueError(response['error']['message'])
+    
+    response = [r['message']['content'] for r in response['choices']]
+    response = response[0] if n == 1 else response
+    
+    return response
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg/gpt_utils.py OWG-main/owg/gpt_utils.py
--- OWG-upstream/owg/gpt_utils.py	2025-12-25 13:40:46.542883990 +0800
+++ OWG-main/owg/gpt_utils.py	2025-10-18 12:52:20.127695301 +0800
@@ -6,8 +6,49 @@
 from typing import List, Union, Optional
 from PIL import Image
 
+
+
+
+# åœ¨æ–‡ä»¶é¡¶éƒ¨åŠ å…¥
+import re
+import json
+import logging
+
+logging.basicConfig(level=logging.INFO)  # æˆ– DEBUG
+
+def _extract_json_from_text(text: str):
+    """
+    Try to find the first JSON object/array inside a text blob and parse it.
+    Returns parsed object or None.
+    """
+    # 1) Try to find a ```json ... ``` or ``` ... ``` block
+    m = re.search(r"```(?:json)?\s*(\{[\s\S]*?\}|\[[\s\S]*?\])\s*```", text, flags=re.IGNORECASE)
+    if m:
+        candidate = m.group(1)
+        try:
+            return json.loads(candidate)
+        except Exception:
+            pass
+
+    # 2) Try to find first {...} or [...] in the text (greedy edges)
+    m = re.search(r"(\{[\s\S]*\}|\[[\s\S]*\])", text)
+    if m:
+        candidate = m.group(1)
+        # attempt balanced brace extraction: try shrinking end until valid json
+        for end in range(len(candidate), 0, -1):
+            try:
+                return json.loads(candidate[:end])
+            except Exception:
+                continue
+
+    # 3) no JSON found
+    return None
+
+
+
+
 # Get OpenAI API Key from environment variable
-openai_api_key = os.environ['OPENAI_API_KEY']
+openai_api_key = os.environ.get('OPENAI_API_KEY', 'sk-A9q5TscQFLIV7ZTAB29f5c93E1D44f4880F91c24FcAa4eDd')
 
 API_URL = "https://api.openai.com/v1/chat/completions"
 
@@ -157,23 +198,52 @@
 
 
 def request_gpt(images: Union[np.ndarray, List[np.ndarray]], prompt: str, system_prompt: str, detail: str = "auto", temp: float = 0.0, n_tokens: int = 256, n: int = 1, return_logprobs: bool = False, in_context_examples: List[dict] = None, model_name: str = "gpt-4o", seed: Optional[int] = None) -> str:
+    api_key = "sk-A9q5TscQFLIV7ZTAB29f5c93E1D44f4880F91c24FcAa4eDd"
     headers = {
         "Content-Type": "application/json",
-        "Authorization": f"Bearer {openai_api_key}"
+        "Authorization": f"Bearer {api_key}"
     }
+    
     # convert single image prompt to multiple for compatibility
     if not isinstance(images, List):
         assert isinstance(images, np.ndarray), "Provide either a numpy array, a PIL image, an image path string or a list of the above."
         images = [images]
     
-    payload = compose_payload(images=images, prompt=prompt, detail=detail, system_prompt=system_prompt, n=n, temperature=temp, max_tokens=n_tokens, return_logprobs=return_logprobs, in_context_examples=in_context_examples, model_name=model_name)
-    response = requests.post(url=API_URL, headers=headers, json=payload).json()
-    
-    if 'error' in response:
-        raise ValueError(response['error']['message'])
-    
-    response = [r['message']['content'] for r in response['choices']]
-    response = response[0] if n == 1 else response
-    
-    return response
+    payload = compose_payload(
+        images=images,
+        prompt=prompt,
+        detail=detail,
+        system_prompt=system_prompt,
+        n=n,
+        temperature=temp,
+        max_tokens=n_tokens,
+        return_logprobs=return_logprobs,
+        in_context_examples=in_context_examples,
+        model_name=model_name,
+        seed=seed
+    )
+    
+    response = requests.post("https://api.ai-yyds.com/v1/chat/completions", json=payload, headers=headers)
+
+    logging.info("LLM raw status: %s", response.status_code)
+    logging.debug("LLM raw response text:\n%s", response.text)
+
+    if response.status_code == 200:
+        res_json = response.json()
+        try:
+            content = res_json['choices'][0]['message'].get('content', '')
+        except Exception:
+            content = res_json.get('choices')[0].get('text', '') if 'choices' in res_json else ""
+
+        parsed = None
+        if isinstance(content, str) and content.strip():
+            parsed = _extract_json_from_text(content)
+
+        if parsed is not None:
+            return parsed
+        else:
+            return content
+    else:
+        logging.error("LLM request failed: %s", response.text)
+        raise ValueError(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{response.text}")
 
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg/markers/visualizer.py OWG-main/owg/markers/visualizer.py
--- OWG-upstream/owg/markers/visualizer.py	2025-12-25 13:40:46.542883990 +0800
+++ OWG-main/owg/markers/visualizer.py	2025-10-17 20:17:57.953709930 +0800
@@ -168,11 +168,11 @@
     def __init__(
         self,
         as_line: bool = True,
-        grasp_colors: List[Color] = [Color.red(), Color.blue()],
-        mark_color: Color = Color.green(),
+	grasp_colors: List[Color] = [Color(255, 0, 0), Color(0, 0, 255)],
+        mark_color: Color = Color(0, 255, 0),
         line_thickness: int = 2,
-        text_color: Color = Color.white(),
-        text_rect_color: Color = Color.black(),
+        text_color: Color = Color(255, 255, 255),
+        text_rect_color: Color = Color(0, 0, 0),
         text_scale: float = 0.5,
         text_thickness: int = 1,
         text_padding: int = 2,
@@ -419,8 +419,8 @@
 
     def __init__(
         self,
-        color: Color = Color.black(),
-        text_color: Union[Color, ColorPalette] = ColorPalette.default(),
+        color: Color = Color(0, 0, 0),
+        text_color: Union[Color, ColorPalette] = Color(255, 255, 255),
         text_scale: float = 0.5,
         text_thickness: int = 1,
         text_padding: int = 10,
@@ -651,8 +651,8 @@
             color_lookup=sv.ColorLookup.INDEX,
             thickness=line_thickness)
         self.label_annotator = sv.LabelAnnotator(
-            color=sv.Color.black(),
-            text_color=sv.Color.white(),
+            color=sv.Color(0, 0, 0),
+            text_color=sv.Color(255, 255, 255),
             color_lookup=sv.ColorLookup.INDEX,
             text_position=sv.Position.CENTER_OF_MASS,
             text_scale=text_scale)
@@ -720,7 +720,7 @@
     if cfg.label.text_include:
         vis.label_annotator = LabelAnnotator(
             text_color = MyColorPalette.default(),
-            color = sv.Color.black(), # background rectangle
+            color = sv.Color(0, 0, 0), # background rectangle
             color_lookup = sv.ColorLookup.INDEX,
             text_position = getattr(sv.Position, cfg.label.text_position),
             text_scale = cfg.label.text_scale,
@@ -791,4 +791,4 @@
             thickness = cfg.polygon.polygon_thickness,
             color_lookup = sv.ColorLookup.INDEX,
         )
-    return vis
\ No newline at end of file
+    return vis
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg/policy0.py OWG-main/owg/policy0.py
--- OWG-upstream/owg/policy0.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/owg/policy0.py	2025-10-17 18:01:23.669105000 +0800
@@ -0,0 +1,102 @@
+from typing import List, Dict, Any, Optional
+from owg.visual_prompt import VisualPrompterPlanning, VisualPrompterGrounding, VisualPrompterGraspRanking
+from owg.utils.image import display_image
+from owg.utils.grasp import Grasp2D
+from owg.utils.pointcloud import to_o3d
+import numpy as np
+from PIL import Image
+
+
+class OwgPolicy:
+    """
+    Implement OWG algorithm to predict next gripper action given user query and observation
+    """
+
+    def __init__(self,
+                 config_path: str,
+                 verbose: bool = False,
+                 vis: bool = False,
+                 use_grasp_ranker: bool = True):
+        self.vis = vis
+        self.grounder = VisualPrompterGrounding(config_path, debug=verbose)
+        self.planner = VisualPrompterPlanning(config_path, debug=verbose)
+        if use_grasp_ranker:
+            self.grasp_ranker = VisualPrompterGraspRanking(config_path,
+                                                           debug=verbose)
+        self.use_grasp_ranker = use_grasp_ranker
+
+    def predict(self,
+                obs: Dict[str, Any],
+                user_query: str,
+                grasps: Optional[List[tuple]] = None):
+        image, seg = obs['image'], obs['seg']
+        # per object mask and Grasp2D objects
+        obj_ids = np.unique(seg)[1:]
+        all_masks = np.stack([seg == objID for objID in obj_ids])
+        marker_data = {'masks': all_masks, 'labels': obj_ids}
+
+        # grounding
+        if self.vis:
+            # Visualize grounding visual prompt
+            visual_promppt, _ = self.grounder.prepare_image_prompt(
+                image.copy(), marker_data)
+            marked_image_grounding = visual_promppt[-1]
+            Image.fromarray(marked_image_grounding).show()
+
+        dets, target_mask, target_ids = self.grounder.request(text_query=user_query,
+                                                    image=image.copy(),
+                                                    data=marker_data)
+        try:
+            target_mask_index = list(dets.keys())[0]
+            target_id = target_ids[0]  # assume single correct object
+        except IndexError:
+            print(f'Object not found: {user_query}')
+            return {'action': 'fail'}
+
+        # planning
+        plan = self.planner.request(text_query=target_id,
+                                    image=image.copy(),
+                                    data=marker_data)
+        action = plan[0]
+        action['target_id'] = target_id
+        action['grasps'] = list(range(len(grasps[action['input']])))
+
+        try:
+            obj_grasps = grasps[action['input']]
+            if action['action'] == 'pick':
+                obj_mask = target_mask
+            else:
+                obj_mask = all_masks[obj_ids.tolist().index(action['input'])]
+        except IndexError:
+            print(f'Object {action["input"]} not detected in image.')
+            return {'action': 'fail'}
+
+        # grasp ranking
+        if self.use_grasp_ranker:
+
+            if not self.grasp_ranker.use_3d_prompt:
+                req_data = {'grasps': obj_grasps, 'mask': obj_mask}
+                inp_prompt = image.copy()
+
+            else:
+                points = obs['points']
+                colors = image.reshape(-1, 3) / 255.
+                cloud = to_o3d(points, colors)
+                req_data = {
+                    'grasps': obj_grasps,
+                }
+                inp_prompt = cloud
+
+            if self.vis:
+                # Visualize grounding visual prompt
+                visual_prompt, _ = self.grasp_ranker.prepare_image_prompt(
+                    inp_prompt, req_data)
+                marked_image_grasping = visual_prompt[-1]
+                Image.fromarray(marked_image_grasping).show()
+
+            _, _, grasp_indices = self.grasp_ranker.request(
+                inp_prompt, req_data)
+
+            action['grasps'] = grasp_indices
+
+        return action
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg/policy.py OWG-main/owg/policy.py
--- OWG-upstream/owg/policy.py	2025-12-25 13:40:46.542883990 +0800
+++ OWG-main/owg/policy.py	2025-11-25 11:39:46.272386855 +0800
@@ -5,98 +5,305 @@
 from owg.utils.pointcloud import to_o3d
 import numpy as np
 from PIL import Image
+import json, ast, re
+from owg_robot.grasp_ranker_lggsn import LggsnGraspRanker
+import os, time
+
+GROUND_LOG_DIR = "logs/grounding_examples"
+os.makedirs(GROUND_LOG_DIR, exist_ok=True)
+
+def _safe_get_from_raw(resp: dict, *keys):
+    """
+    Safely return the first present, non-None value for keys from resp dict.
+    Avoid evaluating truthiness of numpy arrays (which raises ValueError).
+    """
+    if not isinstance(resp, dict):
+        return None
+    for k in keys:
+        if k in resp:
+            v = resp[k]
+            # treat None as missing, but accept numpy arrays, lists, scalars (even empty)
+            if v is not None:
+                return v
+    return None
+
+
+
+
+def safe_json_parse(raw_resp):
+    """Robustly parse LLM JSON-like responses, even with code blocks or single quotes."""
+    if raw_resp is None:
+        return None
+
+    if not isinstance(raw_resp, str):
+        raw_resp = str(raw_resp)
+
+    # å»æ‰ ``` æˆ– ```json çš„ä»£ç å—æ ‡è¯†
+    raw_resp = raw_resp.strip()
+    raw_resp = re.sub(r"^```[a-zA-Z]*\s*", "", raw_resp)
+    raw_resp = re.sub(r"\s*```$", "", raw_resp)
+
+    # å°†å•å¼•å·æ”¹æˆåŒå¼•å·ï¼Œä½†åªå¯¹æœ€å¤–å±‚JSONæœ‰æ•ˆ
+    if raw_resp.startswith("[") or raw_resp.startswith("{"):
+        raw_resp = raw_resp.replace("'", '"')
+
+    try:
+        return json.loads(raw_resp)
+    except Exception:
+        try:
+            return ast.literal_eval(raw_resp)
+        except Exception as e2:
+            print("âš ï¸ safe_json_parse failed:", e2)
+            print("ğŸ”¹ åŸå§‹å“åº”:", raw_resp)
+            return None
+
+
+
+
+
 
 
 class OwgPolicy:
     """
-    Implement OWG algorithm to predict next gripper action given user query and observation
+    Debug version of OWG policy with detailed logging and robust JSON parsing.
     """
 
-    def __init__(self,
-                 config_path: str,
-                 verbose: bool = False,
-                 vis: bool = False,
-                 use_grasp_ranker: bool = True):
+    def __init__(self, config_path: str, verbose: bool = True, vis: bool = False, use_grasp_ranker: bool = True):
         self.vis = vis
+        self.verbose = verbose
         self.grounder = VisualPrompterGrounding(config_path, debug=verbose)
         self.planner = VisualPrompterPlanning(config_path, debug=verbose)
-        if use_grasp_ranker:
-            self.grasp_ranker = VisualPrompterGraspRanking(config_path,
-                                                           debug=verbose)
+
         self.use_grasp_ranker = use_grasp_ranker
+        self.grasp_ranker = None
 
-    def predict(self,
-                obs: Dict[str, Any],
-                user_query: str,
-                grasps: Optional[List[tuple]] = None):
+        if use_grasp_ranker:
+            # âœ… ä¼˜å…ˆå°è¯•ç”¨ LG-GSN å‡ ä½• ranker
+            try:
+                self.grasp_ranker = LggsnGraspRanker(
+                    model_path="grasp_6dof/models/lggsn_geom_only.pt",
+                    device="cuda",
+                )
+                if verbose:
+                    print("ğŸŸ¢ Using LGGSN geometry grasp ranker.")
+            except Exception as e:
+                print("âš ï¸ Failed to init LGGSN ranker, disable grasp ranker:", e)
+                self.grasp_ranker = None
+                self.use_grasp_ranker = False   # å…³é”®ï¼šç›´æ¥å…³æ‰ï¼Œä¸å† fallback
+    def predict(self, obs: Dict[str, Any], user_query: str, grasps: Optional[Dict[int, list]] = None):
+        """
+        Robust predict method for OwgPolicy. Replace the original predict with this.
+        """
         image, seg = obs['image'], obs['seg']
-        # per object mask and Grasp2D objects
-        obj_ids = np.unique(seg)[1:]
-        all_masks = np.stack([seg == objID for objID in obj_ids])
+        # å°è¯•æ‹¿ä¸€ä¸‹æ·±åº¦å’Œç›¸æœºå‚æ•°ï¼ˆæ²¡æœ‰å°±ç®—äº†ï¼Œä¸ä¼šæŠ¥é”™ï¼‰
+        depth = obs.get('depth') if isinstance(obs, dict) else None
+        K = obs.get('K') if isinstance(obs, dict) else None
+        pose_cam = obs.get('cam_pose') if isinstance(obs, dict) else None
+
+        # ---------------- Object masks ----------------
+        # å…ˆæ‹¿åˆ°æ‰€æœ‰ IDï¼Œç„¶åå»æ‰èƒŒæ™¯ 0
+        obj_ids = np.unique(seg)
+        obj_ids = obj_ids[obj_ids != 0]
+
+        # ğŸ§± é˜²æ­¢ np.stack([]) å´©æ‰ï¼šå¦‚æœæ²¡æ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“ï¼Œç›´æ¥ fail
+        if obj_ids.size == 0:
+            if getattr(self, "verbose", False):
+                print("âš ï¸ No objects detected in segmentation. Aborting this action.")
+            return {'action': 'fail'}
+
+        # æ­£å¸¸æƒ…å†µï¼šå¯¹æ¯ä¸ª obj_id ç”Ÿæˆä¸€ä¸ª maskï¼Œç„¶å stack
+        all_masks = np.stack([seg == objID for objID in obj_ids], axis=0)
         marker_data = {'masks': all_masks, 'labels': obj_ids}
 
-        # grounding
-        if self.vis:
-            # Visualize grounding visual prompt
-            visual_promppt, _ = self.grounder.prepare_image_prompt(
-                image.copy(), marker_data)
-            marked_image_grounding = visual_promppt[-1]
-            Image.fromarray(marked_image_grounding).show()
-
-        dets, target_mask, target_ids = self.grounder.request(text_query=user_query,
-                                                    image=image.copy(),
-                                                    data=marker_data)
+
+        if getattr(self, "verbose", False):
+            print("ğŸ”¹ obj_ids:", obj_ids)
+            print("ğŸ”¹ marker_data keys:", marker_data.keys())
+
+        if getattr(self, "vis", False):
+            visual_prompt, _ = self.grounder.prepare_image_prompt(image.copy(), marker_data)
+            Image.fromarray(visual_prompt[-1]).show()
+
+        # ---------------- Grounding ----------------
+        raw_resp = self.grounder.request(
+            text_query=user_query,
+            image=image.copy(),
+            data=marker_data
+        )
+        if getattr(self, "verbose", False):
+            print("ğŸŸ¢ Grounder raw response:", raw_resp)
+
+        # ---- ç»Ÿä¸€è§£æ Grounder çš„è¿”å›ï¼ˆæ”¯æŒ dict æˆ– tupleï¼‰----
+        dets = None
+        target_mask = None
+        target_ids = None
+
+        if isinstance(raw_resp, dict):
+            # ä¸è¦å¯¹ numpy æ•°ç»„ç”¨ "or"ï¼Œåˆ†æ­¥éª¤å¤„ç†æ›´å®‰å…¨
+            dets = raw_resp.get("outputs", None)
+            if dets is None:
+                dets = raw_resp.get("dets", None)
+            if dets is None:
+                dets = raw_resp.get("detections", None)
+
+            if "mask" in raw_resp:
+                target_mask = raw_resp["mask"]
+            elif "target_mask" in raw_resp:
+                target_mask = raw_resp["target_mask"]
+            else:
+                target_mask = None
+
+            if "ids" in raw_resp:
+                target_ids = raw_resp["ids"]
+            elif "target_ids" in raw_resp:
+                target_ids = raw_resp["target_ids"]
+            else:
+                target_ids = None
+
+        elif isinstance(raw_resp, (list, tuple, np.ndarray)) and len(raw_resp) >= 3:
+            dets, target_mask, target_ids = raw_resp[:3]
+        else:
+            print("âŒ Grounding returned unsupported format:", type(raw_resp), raw_resp)
+            return {"action": "fail"}
+
+        # ---- åªæ ¹æ® target_ids æ¥åˆ¤æ–­æ˜¯å¦æˆåŠŸ ----
+        if target_ids is None or (
+            isinstance(target_ids, (list, tuple, np.ndarray)) and len(target_ids) == 0
+        ):
+            print("âŒ Grounding failed. Raw response:", raw_resp)
+            return {"action": "fail"}
+
+        # ç»Ÿä¸€æˆ list[int]
+        if isinstance(target_ids, (list, tuple, np.ndarray)):
+            target_ids = [int(i) for i in target_ids]
+        else:
+            target_ids = [int(target_ids)]
+
+        if getattr(self, "verbose", False):
+            print("âœ… target_ids:", target_ids)
+
+        # ---------- è¿™é‡Œå¼€å§‹ï¼šä¿å­˜ grounding æ—¥å¿— ----------
         try:
-            target_mask_index = list(dets.keys())[0]
-            target_id = target_ids[0]  # assume single correct object
-        except IndexError:
-            print(f'Object not found: {user_query}')
+            # ç»Ÿä¸€æˆä¸€ç»´æ•°ç»„å½¢å¼
+            if isinstance(target_ids, (list, tuple, np.ndarray)):
+                sel_ids = np.array(target_ids, dtype=int)
+            else:
+                sel_ids = np.array([int(target_ids)], dtype=int)
+
+            log_dict = {
+                "image": image.astype(np.uint8),
+                "masks": marker_data["masks"].astype(bool),
+                "labels": np.array(marker_data["labels"]),
+                "selected_ids": sel_ids,
+                "query": user_query,
+            }
+            # å¯é€‰ï¼šæœ‰ depth / K / pose_cam å°±ä¸€èµ·å­˜
+            if depth is not None:
+                log_dict["depth"] = np.array(depth)
+            if K is not None:
+                log_dict["K"] = np.array(K)
+            if pose_cam is not None:
+                log_dict["pose_cam"] = np.array(pose_cam)
+
+            ts = time.time()
+            fname = os.path.join(GROUND_LOG_DIR, f"ground_{ts:.3f}.npz")
+            np.savez_compressed(fname, **log_dict)
+
+            if getattr(self, "verbose", False):
+                print(f"ğŸ“ Saved grounding log â†’ {fname}")
+        except Exception as e:
+            if getattr(self, "verbose", False):
+                print("âš ï¸ Failed to log grounding example:", e)
+
+        # ---------------- Planning ----------------
+        if isinstance(target_ids, (list, tuple, np.ndarray)):
+            if len(target_ids) == 0:
+                print("âŒ target_ids empty")
+                return {'action': 'fail'}
+            qid = target_ids[0]
+        else:
+            qid = target_ids
+
+        try:
+            if isinstance(qid, (np.integer,)):
+                query_id = str(int(qid))
+            else:
+                query_id = str(qid)
+        except Exception:
+            query_id = str(qid)
+
+        plan_raw = self.planner.request(text_query=query_id, image=image.copy(), data=marker_data)
+        if getattr(self, "verbose", False):
+            print("ğŸŸ¢ Planner raw response:", plan_raw)
+
+        # robust parse planner output
+        plan = None
+        if isinstance(plan_raw, list):
+            plan = plan_raw
+        elif isinstance(plan_raw, dict):
+            if 'plan' in plan_raw and isinstance(plan_raw['plan'], list):
+                plan = plan_raw['plan']
+            else:
+                plan = [plan_raw]
+        elif isinstance(plan_raw, str):
+            plan = safe_json_parse(plan_raw)
+
+        if plan is None or not isinstance(plan, list) or len(plan) == 0:
+            print("âŒ Planner returned invalid plan:", plan_raw)
             return {'action': 'fail'}
 
-        # planning
-        plan = self.planner.request(text_query=target_id,
-                                    image=image.copy(),
-                                    data=marker_data)
         action = plan[0]
-        action['target_id'] = target_id
-        action['grasps'] = list(range(len(grasps[action['input']])))
 
+        # ---------------- Grasp Assignment ----------------
         try:
-            obj_grasps = grasps[action['input']]
-            if action['action'] == 'pick':
-                obj_mask = target_mask
-            else:
-                obj_mask = all_masks[obj_ids.tolist().index(action['input'])]
-        except IndexError:
-            print(f'Object {action["input"]} not detected in image.')
-            return {'action': 'fail'}
+            if isinstance(target_ids, (list, tuple, np.ndarray)):
+                action['target_id'] = target_ids[0]
+            else:
+                action['target_id'] = target_ids
 
-        # grasp ranking
-        if self.use_grasp_ranker:
+            if grasps is not None:
+                if isinstance(grasps, dict):
+                    action['grasps'] = list(range(len(grasps.get(action['input'], []))))
+                else:
+                    try:
+                        action['grasps'] = list(range(len(grasps[action['input']])))
+                    except Exception:
+                        action['grasps'] = []
+            else:
+                action['grasps'] = []
+        except Exception as e:
+            print("âš ï¸ Grasp assignment error:", e)
+            return {'action': 'fail'}
 
-            if not self.grasp_ranker.use_3d_prompt:
-                req_data = {'grasps': obj_grasps, 'mask': obj_mask}
-                inp_prompt = image.copy()
-
-            else:
-                points = obs['points']
-                colors = image.reshape(-1, 3) / 255.
-                cloud = to_o3d(points, colors)
-                req_data = {
-                    'grasps': obj_grasps,
-                }
-                inp_prompt = cloud
-
-            if self.vis:
-                # Visualize grounding visual prompt
-                visual_prompt, _ = self.grasp_ranker.prepare_image_prompt(
-                    inp_prompt, req_data)
-                marked_image_grasping = visual_prompt[-1]
-                Image.fromarray(marked_image_grasping).show()
+        # ---------------- Grasp Ranking ----------------
+        if getattr(self, "use_grasp_ranker", False) and grasps is not None and getattr(self, "grasp_ranker", None) is not None:
+            # å–å‡ºå½“å‰ç›®æ ‡ç‰©ä½“çš„ grasps
+            if isinstance(grasps, dict):
+                obj_grasps = grasps.get(action['input'], [])
+            else:
+                if isinstance(action.get('input'), (int, np.integer)) and action['input'] <     len(grasps):
+                    obj_grasps = grasps[action['input']]
+                else:
+                    obj_grasps = []
 
-            _, _, grasp_indices = self.grasp_ranker.request(
-                inp_prompt, req_data)
+            if len(obj_grasps) == 0:
+                action['grasps'] = []
+            else:
+                # ç”¨ LGGSN å¯¹ 3D grasps æ‰“åˆ†å¹¶æ’åº
+                order, scores = self.grasp_ranker.rank(
+                    obj_grasps,
+                    query_text=user_query,
+                    obj_type=None,
+                )
+                action['grasps'] = order.tolist()
+                if getattr(self, "verbose", False) and len(order) > 0:
+                    print("ğŸŸ¢ LGGSN grasp scores (top 5):", scores[order[:5]])
 
-            action['grasps'] = grasp_indices
+            if getattr(self, "verbose", False):
+                print("ğŸŸ¢ Final action:", action)
 
         return action
+
+        
+        
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg/visual_prompt.py OWG-main/owg/visual_prompt.py
--- OWG-upstream/owg/visual_prompt.py	2025-12-25 13:40:46.542883990 +0800
+++ OWG-main/owg/visual_prompt.py	2025-11-25 11:19:38.239465087 +0800
@@ -6,9 +6,9 @@
 import json
 import numpy as np
 import open3d as o3d
+import time
 from PIL import Image
 from typing import List, Union, Dict, Any, Optional, Tuple
-
 from owg.gpt_utils import request_gpt
 from owg.utils.config import load_config
 from owg.utils.grasp import Grasp2D, grasp_to_mat
@@ -28,6 +28,8 @@
 
 #o3d.visualization.rendering.OffscreenRenderer.enable_headless(True)
 
+LOG_DIR = "logs/grounding_examples"
+os.makedirs(LOG_DIR, exist_ok=True)
 
 class VisualPrompter:
 
@@ -159,11 +161,24 @@
         )
         if self.debug:
             print("\033[92mGPT response:\033[0m")
-            print("\033[92m" + response.strip() + "\033[0m")
+            if isinstance(response, str):
+                print("\033[92m" + response.strip() + "\033[0m")
+            else:
+                print("\033[92m[LLM returned structured data] " + json.dumps(response, indent=2,  ensure_ascii=False) + "\033[0m")
+
             print()
 
         # Parse and return the response (this will be subclassed to define behavior)
-        return self.parse_response(response, image_prompt_utils)
+        try:
+            parsed = self.parse_response(response, image_prompt_utils)
+            if parsed is None:
+                print("âš ï¸ GPT å“åº”è§£æå¤±è´¥ï¼Œè¿”å›å€¼ä¸º None")
+            return parsed
+        except Exception as e:
+            print(f"âŒ è§£æ GPT å“åº”å¤±è´¥: {e}")
+            print("åŸå§‹ response:", response)
+            return None
+
 
 
 class VisualPrompterGrounding(VisualPrompter):
@@ -293,45 +308,126 @@
             "markers": markers,
             "raw_image": image.copy(),
             'labels': labels,
+            "masks": masks,
         }
 
         return img_prompt, output_data
 
-    def parse_response(self, response: str, data: Dict[str,
-                                                       Any]) -> Dict[int, Any]:
+    def parse_response(self, response: Any, data: Dict[str, Any]) -> Dict[str, Any]:
         """
-        Parses the GPT response to extract relevant mask IDs and returns corresponding markers.
+        Parse GPT response and return a dict with:
+            {
+              "outputs": {label_id: marker_or_None, ...},
+              "mask":    (H,W) bool merged mask,
+              "ids":     [label_id1, label_id2, ...]
+            }
+        If parsing fails or no valid IDs are found, returns ({}, None, []).
+        """
+        # ä» prepare_image_prompt ä¼ è¿›æ¥çš„æ•°æ®é‡Œæ‹¿ä¸œè¥¿
+        masks = data.get("masks", None)      # (N,H,W) bool
+        labels = np.array(data.get("labels", []))  # (N,)
 
-        Args:
-            response (str): The raw response from GPT, which contains the IDs of the objects identified.
-            data (Dict[int, Any]): Contains `markers`, a dictionary where keys are mask IDs and values are corresponding mask data.
-                                   (Optional) Contains `labels`, list of label IDs to name the markers.
+        # markers å…¶å®åç»­åŸºæœ¬ä¸ç”¨äº†ï¼Œåªæ˜¯ä¸ºäº†ä¸ç ´åæ¥å£ç•™ç€
+        markers = data.get("markers", None)
 
-        Returns:
-            Dict[int, Any]: A dictionary of selected markers based on GPT's response.
-        """
-        markers = data["markers"]
-        labels = list(data['labels'])
-        try:
-            # Extract the portion of the response that contains the final answer IDs
-            output_IDs_str = (response.split("final answer is:")[1].replace(
-                ".", "").strip())
-            output_IDs = eval(output_IDs_str)  # Convert string to list of IDs
+        # ---------- 1. è§£æ GPT è¿”å›çš„ ID åˆ—è¡¨ ----------
+        output_IDs: List[int] = []
 
-            # Convert to `labels` indexing
-            output_IDs_ret = [labels.index(x) for x in output_IDs]
+        try:
+            # Case 1: ç›´æ¥æ˜¯ listï¼Œä¾‹å¦‚ [9]
+            if isinstance(response, list):
+                output_IDs = [
+                    int(x) for x in response
+                    if isinstance(x, (int, float, str))
+                ]
+
+            # Case 2: dictï¼Œä¾‹å¦‚ {"ids": [9, 10]}
+            elif isinstance(response, dict):
+                cand = response.get("ids", [])
+                if isinstance(cand, (list, tuple, np.ndarray)):
+                    output_IDs = [
+                        int(x) for x in cand
+                        if isinstance(x, (int, float, str))
+                    ]
+                elif isinstance(cand, (int, float, str)):
+                    output_IDs = [int(cand)]
+
+            # Case 3: å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯ "[9]" æˆ– "final answer is: [9]."
+            elif isinstance(response, str):
+                txt = response.strip()
+                lower = txt.lower()
+
+                # å…ˆå¤„ç† "final answer is ..." è¿™ç§æ¨¡æ¿
+                if "final answer is" in lower:
+                    try:
+                        tail = txt[lower.index("final answer is") + len("final answer is"):].strip()
+                    except Exception:
+                        tail = txt
+                    tail = tail.replace(".", "").strip()
+                    try:
+                        tmp = ast.literal_eval(tail)
+                        if isinstance(tmp, (list, tuple, np.ndarray)):
+                            output_IDs = [int(x) for x in tmp]
+                        elif isinstance(tmp, (int, float, str)):
+                            output_IDs = [int(tmp)]
+                    except Exception:
+                        pass
+
+                # å¦‚æœä¸Šé¢æ²¡æˆåŠŸï¼Œå†è¯•æ•´ä¸ªå­—ç¬¦ä¸² literal_eval
+                if not output_IDs:
+                    try:
+                        tmp = ast.literal_eval(txt)
+                        if isinstance(tmp, (list, tuple, np.ndarray)):
+                            output_IDs = [int(x) for x in tmp]
+                        elif isinstance(tmp, (int, float, str)):
+                            output_IDs = [int(tmp)]
+                    except Exception:
+                        # æœ€åå…œåº•ï¼šç”¨æ­£åˆ™æŠ“æ‰€æœ‰æ•´æ•°
+                        nums = re.findall(r"-?\d+", txt)
+                        output_IDs = [int(n) for n in nums]
+
+            # å…¶å®ƒç±»å‹å°±å½“æ²¡è§£æå‡ºæ¥
+            else:
+                output_IDs = []
 
-            # Return the masks corresponding to the extracted IDs
-            outputs = {mark: markers[mark] for mark in output_IDs_ret}
-            output_mask = np.zeros_like(markers[0].mask.squeeze(0))
-            for _, mark in outputs.items():
-                output_mask[mark.mask.squeeze(0) == True] = True
+        except Exception as e:
+            print("âš ï¸ parse_response: failed to interpret ids from response:", e)
+            output_IDs = []
 
-            return outputs, output_mask, output_IDs
+        # ---------- 2. ç”¨ labels + masks åšä¸€ä¸ªçœŸæ­£çš„ mask ----------
+        if masks is None or labels.size == 0:
+            # æ­£å¸¸æƒ…å†µä¸‹ä¸ä¼šå‡ºç°ï¼Œå¦‚æœå‡ºç°å°±å½“å¤±è´¥
+            return {}, None, []
+
+        valid_ids: List[int] = []
+        merged_mask = np.zeros_like(masks[0], dtype=bool)
+
+        for lab in output_IDs:
+            try:
+                lab_int = int(lab)
+            except Exception:
+                continue
+
+            idxs = np.where(labels == lab_int)[0]
+            if idxs.size == 0:
+                continue
+
+            idx = int(idxs[0])
+            valid_ids.append(lab_int)
+            merged_mask |= masks[idx].astype(bool)
+
+        if len(valid_ids) == 0:
+            return {}, None, []
+
+        # ---------- 3. outputs å­—æ®µï¼špolicy é‡Œåªæ˜¯å½“ dets ç”¨ï¼Œä¸ä¼šçœŸæ­£è®¿é—®å†…å®¹ ----------
+        outputs: Dict[int, Any] = {lab_id: None for lab_id in valid_ids}
+
+        return {
+            "outputs": outputs,
+            "mask": merged_mask,
+            "ids": valid_ids,
+        }
 
-        except Exception as e:
-            print(f"Failed parsing response: {e}")
-            return {}
 
 
 class VisualPrompterPlanning(VisualPrompterGrounding):
@@ -378,22 +474,67 @@
     def parse_response_text(self, response: str) -> List[Dict[str, Any]]:
         raise NotImplementedError
 
-    def parse_response_json(self, response: str,
-                            data: Dict[str, Any]) -> List[Dict[str, Any]]:
-        """Parses a response string into a list of Python dictionaries.
+    def parse_response_json(self, response: str, data: Dict[str, Any]) ->   Optional[List[Dict[str, Any]]]:
+        """
+        More robust parser for planner JSON responses.
 
-        Args:
-        response: The response string to parse.
+        Handles:
+        - "Plan: ```json ... ```"
+        - "```json ... ```"
+        - "``` ... ```"
+        - plain JSON text
+        Tries json.loads, ast.literal_eval, and a fallback that replaces single   quotes with double quotes.
+        """
+        if response is None:
+            return None
+
+        # try several patterns to extract the JSON payload
+        patterns = [
+            r"Plan:\s*```json(.*?)```",   # strict with Plan: and ```json
+            r"```json(.*?)```",           # ```json ... ```
+            r"```(.*?)```",               # generic ``` ... ```
+            r"(\[[\s\S]*\]|\{[\s\S]*\})"  # plain JSON array or object anywhere
+        ]
+
+        json_text = None
+        for p in patterns:
+            m = re.search(p, response, re.DOTALL | re.IGNORECASE)
+            if m:
+                json_text = m.group(1).strip()
+                break
+
+        # if nothing matched, use whole response as last resort
+        if json_text is None:
+            json_text = response.strip()
+
+        # normalize: remove leading/trailing fences/spaces
+        json_text = re.sub(r"^```[a-zA-Z]*\s*", "", json_text).rstrip("` \n\r\t")
+
+        # If looks like Python list/dict with single quotes, try multiple parsers
+        # Try json.loads first
+        try:
+            return json.loads(json_text)
+        except Exception:
+            pass
+
+        # Try ast.literal_eval (accepts single quotes)
+        try:
+            parsed = ast.literal_eval(json_text)
+            return parsed
+        except Exception:
+            pass
+
+        # Last-ditch attempt: replace single quotes with double quotes and try json again
+        try:
+            fixed = json_text.replace("'", '"')
+            return json.loads(fixed)
+        except Exception as e:
+            # final failure â€” print debug info so we can inspect why parsing failed
+            print("âš ï¸ parse_response_json failed:", e)
+            print("ğŸ”¹ Original response:\n", response)
+            print("ğŸ”¹ Extracted text to parse:\n", json_text[:1000])  # print prefix for large responses
+            return None
 
-        Returns:
-        A list of Python dictionaries containing the parsed actions and inputs.
-        """
-        match = re.search(r"Plan:\s*```json(.*?)```", response, re.DOTALL)
-        if match:
-            # Replace single quotes with double quotes for valid JSON
-            json_str = match.group(1).strip().replace("'", '"')
-            return json.loads(json_str)
-        return None
 
 
 class VisualPrompterGraspRanking(VisualPrompter):
@@ -543,34 +684,63 @@
 
         return [marked_image], output_data
 
-    def parse_response(self, response: str, data: Dict[str,
-                                                       Any]) -> Dict[int, Any]:
+    def parse_response(self, response: str, data: Dict[str, Any]):
+        """
+        Robust parser for GPT grounding output.
+        Handles list, dict, string, or complex tuple outputs safely.
         """
-        Parses the GPT response to extract relevant grasp IDs and returns corresponding markers.
+        import numpy as np
 
-        Args:
-            response (str): The raw response from GPT, which contains the IDs of the objects identified.
-            data (Dict[int, Any]): Contains `grasp_markers`, a list of Grasp2D candidates.
+        markers = data.get("markers", {})
+        labels = list(data.get("labels", []))
 
-        Returns:
-            Dict[int, Any]: A dictionary of selected markers based on GPT's response.
-        """
-        grasps = data["grasp_markers"]
         try:
-            # Extract the portion of the response that contains the final answer IDs
-            output_IDs_str = (response.split("final answer is:")[1].replace(
-                ".", "").strip())
-            output_IDs = eval(output_IDs_str)  # Convert string to list of IDs
-
-            # Convert to 1-indexing
-            output_IDs_ret = [x - 1 for x in output_IDs]
-
-            # Return the masks corresponding to the extracted IDs
-            sorted_grasps = [grasps[i] for i in output_IDs_ret]
-            best_grasp = sorted_grasps[0]
+            # 1ï¸âƒ£ Handle direct list or dict (structured data)
+            if isinstance(response, list):
+                output_IDs = [int(x) for x in response if isinstance(x, (int, float, str))]
+
+            elif isinstance(response, dict):
+                output_IDs = [int(x) for x in response.get("ids", [])]
+
+            # 2ï¸âƒ£ Handle tuple (e.g. when GPT returned multiple parts)
+            elif isinstance(response, tuple):
+                # try to extract numeric ids from last element
+                for item in reversed(response):
+                    if isinstance(item, (list, np.ndarray)):
+                        output_IDs = [int(x) for x in item if isinstance(x, (int, float, str))]
+                        break
+                else:
+                    raise ValueError("No list of IDs found in tuple response.")
+
+            # 3ï¸âƒ£ Handle string responses (JSON-like or raw text)
+            elif isinstance(response, str):
+                lowered = response.lower()
+                if "final answer is:" in lowered:
+                    output_IDs_str = lowered.split("final answer is:")[1].replace(".", "").strip()
+                    output_IDs = eval(output_IDs_str)
+                else:
+                    try:
+                        output_IDs = eval(response)
+                        if not isinstance(output_IDs, (list, tuple)):
+                            output_IDs = [int(output_IDs)]
+                    except Exception:
+                        raise ValueError("String response not parseable to list of IDs.")
 
-            return sorted_grasps, best_grasp, output_IDs_ret
+            else:
+                raise ValueError(f"Unexpected response type: {type(response)}")
+
+            # 4ï¸âƒ£ Match IDs to internal markers
+            output_IDs_ret = [labels.index(x) for x in output_IDs if x in labels]
+            outputs = {mark: markers[mark] for mark in output_IDs_ret}
+
+            output_mask = np.zeros_like(list(markers.values())[0].mask.squeeze(0))
+            for _, mark in outputs.items():
+                output_mask[mark.mask.squeeze(0) == True] = True
+
+            return outputs, output_mask, output_IDs
 
         except Exception as e:
-            print(f"Failed parsing response: {e}")
+            print(f"âŒ Failed parsing response: {e}")
+            print("åŸå§‹ GPT response:", response)
             return {}
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg_robot/0ui.py OWG-main/owg_robot/0ui.py
--- OWG-upstream/owg_robot/0ui.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/owg_robot/0ui.py	2025-10-18 13:29:25.672887501 +0800
@@ -0,0 +1,287 @@
+import time
+from pygments import highlight
+from pygments.lexers import PythonLexer
+from pygments.formatters import TerminalFormatter, HtmlFormatter
+import tkinter as tk
+from tkinter import simpledialog, messagebox, scrolledtext
+from typing import *
+
+from owg_robot.env import *
+from owg_robot.camera import Camera
+from owg_robot.objects import YcbObjects
+from owg.policy import OwgPolicy
+from owg.utils.config import load_config
+from owg.utils.grasp import Grasp2D
+from third_party.grconvnet import load_grasp_generator
+
+
+# GUI stuff
+# Function to create a text input dialog using Tkinter
+def ask_for_user_input():
+    root = tk.Tk()
+    root.withdraw()  # Hide the main window
+    user_input = simpledialog.askstring("Input", "User Input: ")
+    root.destroy()
+    return input("è¯·è¾“å…¥å¯¹è±¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰ï¼š")
+
+
+class RobotEnvUI:
+
+    def __init__(self, config: Union[Dict[str, Any], str]):
+        self.cfg = load_config(config) if isinstance(config, str) else config
+        self.n_objects = self.cfg.n_objects
+        self.seed = self.cfg.seed
+
+        # init env
+        center_x, center_y, center_z = CAM_X, CAM_Y, CAM_Z
+        cam_center = (self.cfg.camera.center_x, self.cfg.camera.center_y,
+                      self.cfg.camera.center_z)
+        cam_target = (self.cfg.camera.target_x, self.cfg.camera.target_y,
+                      self.cfg.camera.target_z)
+        self.img_size = (self.cfg.camera.img_size, self.cfg.camera.img_size)
+        self.camera = Camera(cam_center, cam_target, self.cfg.camera.znear,
+                             self.cfg.camera.zfar, self.img_size,
+                             self.cfg.camera.fov)
+        self.n_grasp_attempts = self.cfg.n_grasp_attempts
+        self.env = Environment(self.camera,
+                               vis=True,
+                               asset_root='./owg_robot/assets',
+                               debug=False,
+                               finger_length=self.cfg.finger_length,
+                               n_grasp_attempts=self.cfg.n_grasp_attempts)
+
+        # load objects
+        self.objects = YcbObjects(
+            './owg_robot/assets/ycb_objects',
+            mod_orn=['ChipsCan', 'MustardBottle', 'TomatoSoupCan'],
+            mod_stiffness=['Strawberry'],
+            seed=self.seed)
+        self.objects.shuffle_objects()
+        self.env.dummy_simulation_steps(10)
+
+        # init OWG policy
+        self.policy = OwgPolicy(
+            self.cfg.policy.config_path,
+            verbose=self.cfg.policy.verbose,
+            vis=self.cfg.policy.vis,
+            use_grasp_ranker=self.cfg.policy.use_grasp_ranker)
+
+        self.grasp_rank_3d = False
+        if self.cfg.policy.use_grasp_ranker:
+            self.grasp_rank_3d = self.policy.grasp_ranker.use_3d_prompt
+
+        # spawn scene
+        obs = self.spawn(self.n_objects)
+
+        # GR-ConvNet grasp generator
+        self.grasp_generator = load_grasp_generator(self.env.camera)
+        # setup and visualize once
+        self.setup_grasps(obs, visualise_grasps=True)
+
+        self.n_action_attempts = self.cfg.n_action_attempts
+        self.n_grasp_attempts = self.cfg.n_grasp_attempts
+
+    def spawn(self, n_objects):
+        self.n_objects = n_objects
+        for obj_name in self.objects.obj_names[:self.n_objects]:
+            path, mod_orn, mod_stiffness = self.objects.get_obj_info(obj_name)
+            self.env.load_isolated_obj(path, obj_name, mod_orn, mod_stiffness)
+            self.env.dummy_simulation_steps(30)
+        self.init_obj_state = self.env.get_obj_states()
+        obs = self.env.get_obs()
+        return obs
+
+    def reset_same(self):
+        assert self.init_obj_state is not None, "Have to spawn once to initialize state"
+        self.env.reset_robot()
+        self.env.set_obj_state(self.init_obj_state)
+        self.env.dummy_simulation_steps(10)
+        obs = self.update()
+        self.init_obj_state = self.env.get_obj_states()
+        for _ in range(30):
+            self.env.step_simulation()
+        return obs
+
+    def reset(self, new=False):
+        if new:
+            self.env.remove_all_obj()
+            for _ in range(30):
+                self.env.step_simulation()
+            # self.objects = YcbObjects('./owg_robot/assets/ycb_objects',
+            #         mod_orn=['ChipsCan', 'MustardBottle', 'TomatoSoupCan'],
+            #         mod_stiffness=['Strawberry'],
+            #         seed=self.seed
+            # )
+            self.seed += 100
+            self.objects.set_seed(self.seed)
+            self.objects.shuffle_objects()
+            self.env.dummy_simulation_steps(10)
+            return self.spawn(self.n_objects)
+        return self.reset_same()
+
+    def update(self):
+        self.env.dummy_simulation_steps(10)
+        self.env.update_obj_states()
+        obs = self.env.get_obs()
+        self.setup_grasps(obs)
+        self.env.dummy_simulation_steps(10)
+        return obs
+
+    def setup_grasps(self,
+                     obs: Dict[str, Any],
+                     visualise_grasps: bool = False):
+        """
+        Run inference with GR-ConvNet grasp generator on current observation
+        """
+        rgb, depth, seg = obs['image'], obs['depth'], obs['seg']
+        img_size = self.grasp_generator.IMG_WIDTH
+        if img_size != self.env.camera.width:
+            rgb = cv2.resize(rgb, (img_size, img_size))
+            depth = cv2.resize(depth, (img_size, img_size))
+        for obj_id in self.env.obj_ids:
+            mask = seg == obj_id
+            if img_size != self.env.camera.width:
+                mask = np.array(
+                    Image.fromarray(mask).resize((img_size, img_size),
+                                                 Image.LANCZOS))
+            grasps, grasp_rects = self.grasp_generator.predict_grasp_from_mask(
+                rgb, depth, mask, n_grasps=self.n_grasp_attempts, show_output=False)
+            if img_size != self.env.camera.width:
+                # normalize to original size
+                for j, gr in enumerate(grasp_rects):
+                    grasp_rects[j][0] = int(gr[0] / img_size *
+                                            self.env.camera.width)
+                    grasp_rects[j][1] = int(gr[1] / img_size *
+                                            self.env.camera.width)
+                    grasp_rects[j][4] = int(gr[4] / img_size *
+                                            self.env.camera.width)
+                    grasp_rects[j][3] = int(gr[3] / img_size *
+                                            self.env.camera.width)
+            grasp_rects = [
+                Grasp2D.from_vector(
+                    x=g[1],
+                    y=g[0],
+                    w=g[4],
+                    h=g[3],
+                    theta=g[2],
+                    W=self.env.camera.width,
+                    H=self.env.camera.width,
+                    normalized=False,
+                    line_offset=5,
+                ) for g in grasp_rects
+            ]
+            self.env.set_obj_grasps(obj_id, grasps, grasp_rects)
+
+        if visualise_grasps:
+            LID = []
+            for obj_id in self.env.obj_ids:
+                grasps = self.env.get_obj_grasps(obj_id)
+                color = np.random.rand(3).tolist()
+                for g in grasps:
+                    LID = self.env.draw_predicted_grasp(g,
+                                                        color=color,
+                                                        lineIDs=LID)
+
+            time.sleep(1)
+            self.env.remove_drawing(LID)
+
+    def step(self, action):
+        '''
+        Wrapper around OWG action predictions and implemented robot primitives.
+
+        Args:
+          action: Predicted action by OWG 
+            - `action`: Either `remove` to place blocking object in free space, or `pick` to put target in tray.
+            - `input`: The object ID of object to manipulate.
+        '''
+        #for attempt in range(self.cfg.n_action_attempts):
+        if action['action'] == 'remove':
+            success_grasp, success_target = self.env.put_obj_in_free_space(
+                action['input'], grasp_indices=action['grasps'])
+        elif action['action'] == 'pick':
+            success_grasp, success_target = self.env.put_obj_in_tray(
+                action['input'], grasp_indices=action['grasps'])
+        for _ in range(30):
+            self.env.step_simulation()
+
+        if not success_grasp or not success_target:
+            print(f'Action failed...')
+            success, done = False, False
+
+        elif action['input'] != action['target_id']:
+            # successfull action, but not terminal
+            print(f'Done {action["action"]} {action["input"]}')
+            success, done = True, False
+
+        else:
+            # successfull terminal action
+            print(f'Done {action["action"]} {action["input"]}')
+            success, done = True, True
+
+        return success, done
+
+    def run(self):
+        while True:
+            # ask user for command
+            self.env.reset_robot()
+            user_input = ask_for_user_input()
+
+            # reset same scene
+            if user_input == ':reset':
+                self.reset(new=False)
+                self.env.dummy_simulation_steps(10)
+                continue
+
+            # spawn new scene
+            elif user_input == ':new':
+                self.reset(new=True)
+                self.env.dummy_simulation_steps(10)
+                continue
+
+            # close demo
+            elif user_input == ':exit':
+                print('Exitting demo')
+                self.env.close()
+                break
+
+            # actual query
+            else:
+                # call OWG Policy to predict next action
+                attempt = 0
+                while True:
+                    self.env.reset_robot()
+                    obs = self.update()
+
+                    # save a dict for all predicted grasps for policy grasp ranking
+                    if self.grasp_rank_3d:
+                        all_grasps = {
+                            k: self.env.get_obj_grasps(k)
+                            for k in self.env.obj_ids
+                        }
+                    else:
+                        all_grasps = {
+                            k: self.env.get_obj_grasp_rects(k)
+                            for k in self.env.obj_ids
+                        }
+
+                    action = self.policy.predict(obs, user_input, all_grasps)
+
+                    if action['action'] == 'fail':
+                        break
+                    # convert marker IDs to state IDs
+                    success, done = self.step(action)
+                    if success and done:
+                        # task finished
+                        break
+                    elif not success:
+                        attempt += 1
+                        if attempt == self.cfg.n_action_attempts:
+                            print(f'Action failed. No more atempts.')
+                            break
+                        print(f'Action failed. {attempt} attempt. Retrying..')
+                        continue
+                    else:
+                        # plan step finished, move to next
+                        attempt = 0
+                        self.env.dummy_simulation_steps(30)
+                        continue
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg_robot/assets/ycb_objects/YcbChipsCan/ChipsCan.ply OWG-main/owg_robot/assets/ycb_objects/YcbChipsCan/ChipsCan.ply
--- OWG-upstream/owg_robot/assets/ycb_objects/YcbChipsCan/ChipsCan.ply	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/owg_robot/assets/ycb_objects/YcbChipsCan/ChipsCan.ply	2025-11-24 23:32:29.762324554 +0800
@@ -0,0 +1,462 @@
+ply
+format ascii 1.0
+element vertex 455
+property float x
+property float y
+property float z
+end_header
+0.365929 -0.002452 0.108557
+0.365778 -0.001771 0.098030
+0.365904 -0.001037 0.103627
+0.365724 -0.001035 0.110807
+0.365299 -0.000173 0.112835
+0.365258 -0.000178 0.116796
+0.365104 0.001658 0.097276
+0.365481 0.001169 0.100673
+0.365147 0.001402 0.103628
+0.365074 0.000148 0.108220
+0.365097 0.001583 0.111616
+0.365588 0.001231 0.112906
+0.364995 0.003442 0.095878
+0.364511 0.003933 0.099520
+0.365170 0.002412 0.103716
+0.364975 0.003515 0.113413
+0.365217 0.002943 0.114728
+0.365342 0.002371 0.117454
+0.364963 0.005873 0.103379
+0.365162 0.004848 0.105819
+0.364965 0.004180 0.109949
+0.364966 0.004106 0.111108
+0.364743 0.004592 0.113598
+0.365078 0.004535 0.115705
+0.365104 0.005698 0.116938
+0.365470 0.006960 0.094410
+0.365629 0.007621 0.098269
+0.365096 0.006091 0.101162
+0.365212 0.007718 0.107630
+0.365543 0.007118 0.109546
+0.365227 0.007739 0.110203
+0.365576 0.007188 0.115623
+0.364835 0.007247 0.116555
+0.365943 0.008798 0.094925
+0.365495 0.008782 0.098202
+0.365800 0.009636 0.113790
+0.365716 0.009612 0.116892
+0.367161 -0.004156 0.097785
+0.367267 -0.003435 0.098987
+0.366495 -0.002662 0.103760
+0.366698 -0.002169 0.105219
+0.366474 -0.002070 0.108508
+0.367321 -0.003489 0.111275
+0.366128 -0.002108 0.112016
+0.367357 -0.002527 0.117992
+0.366800 -0.002071 0.118638
+0.366096 -0.001310 0.093402
+0.366275 -0.001566 0.097846
+0.366362 -0.001689 0.100958
+0.366584 -0.001682 0.111428
+0.366102 -0.001155 0.117577
+0.367324 0.007329 0.118093
+0.366076 0.009912 0.095714
+0.366375 0.009377 0.100611
+0.366072 0.008570 0.110130
+0.366533 0.009685 0.113635
+0.366005 0.008108 0.115499
+0.367245 0.008233 0.118093
+0.367346 0.011954 0.096635
+0.366035 0.010540 0.106093
+0.367669 0.011222 0.110526
+0.366641 0.010900 0.112444
+0.367992 0.011844 0.116838
+0.367883 0.012139 0.107575
+0.367749 0.012236 0.114247
+0.369793 -0.006467 0.094728
+0.368660 -0.006092 0.096389
+0.369697 -0.006568 0.103345
+0.369940 -0.006535 0.107547
+0.369989 -0.006893 0.113723
+0.369161 -0.005897 0.098212
+0.368853 -0.005315 0.102197
+0.368213 -0.004380 0.105797
+0.369774 -0.005973 0.106604
+0.368714 -0.004882 0.109020
+0.369548 -0.005715 0.111295
+0.369207 -0.005498 0.112354
+0.369399 -0.005875 0.115721
+0.368668 -0.005786 0.116056
+0.369659 -0.005506 0.118062
+0.369067 -0.002628 0.118843
+0.369395 0.007315 0.118093
+0.369863 0.009320 0.118093
+0.368062 0.011871 0.113254
+0.368152 0.011988 0.116664
+0.368654 0.011573 0.118228
+0.368606 0.012836 0.095941
+0.368514 0.012775 0.098713
+0.368205 0.012556 0.100947
+0.368376 0.012212 0.105235
+0.369159 0.012995 0.109421
+0.368047 0.012445 0.110462
+0.368442 0.012278 0.113736
+0.369095 0.013180 0.114243
+0.369779 0.013659 0.116594
+0.371751 -0.008352 0.107811
+0.371921 -0.008552 0.112579
+0.370918 -0.006893 0.095881
+0.371331 -0.007177 0.099940
+0.371440 -0.007234 0.102307
+0.370754 -0.007278 0.105966
+0.371207 -0.007126 0.109056
+0.371587 -0.007676 0.110333
+0.370980 -0.007410 0.114399
+0.370932 -0.007135 0.117003
+0.370320 -0.005899 0.118122
+0.370543 -0.002608 0.118093
+0.371865 0.000231 0.118093
+0.371254 0.013145 0.118605
+0.370563 0.015139 0.096037
+0.371183 0.015305 0.098239
+0.370641 0.015100 0.100382
+0.370635 0.014260 0.102049
+0.370489 0.014157 0.105304
+0.370930 0.015609 0.107694
+0.370669 0.015296 0.110270
+0.371558 0.015235 0.118258
+0.373840 -0.008348 0.094897
+0.373127 -0.008158 0.096048
+0.373776 -0.008332 0.098044
+0.373495 -0.008257 0.100360
+0.373325 -0.008695 0.102377
+0.373522 -0.008264 0.104258
+0.373857 -0.008354 0.106157
+0.372938 -0.008107 0.109066
+0.373048 -0.008500 0.114996
+0.373998 -0.008730 0.116819
+0.372211 -0.007710 0.095443
+0.372892 -0.007911 0.098907
+0.372403 -0.007964 0.105683
+0.372215 -0.007397 0.117740
+0.373357 -0.007737 0.118225
+0.372962 -0.002345 0.118093
+0.372301 0.002286 0.118093
+0.372133 0.011563 0.118093
+0.372455 0.015646 0.099432
+0.372053 0.015188 0.102546
+0.372096 0.015208 0.104105
+0.372878 0.015615 0.108133
+0.373198 0.015846 0.111465
+0.372564 0.015427 0.112486
+0.372446 0.015644 0.114346
+0.373999 0.016061 0.097312
+0.373883 0.016030 0.100522
+0.373956 0.016049 0.104965
+0.372788 0.016276 0.113713
+0.373053 0.016229 0.114667
+0.374318 -0.008478 0.094696
+0.374371 -0.008492 0.097838
+0.374472 -0.008538 0.101533
+0.375901 -0.008902 0.104483
+0.374545 -0.008539 0.106031
+0.374712 -0.008584 0.108109
+0.374338 -0.008483 0.111771
+0.375145 -0.008700 0.113327
+0.374955 -0.009565 0.115200
+0.374101 -0.008892 0.116229
+0.375576 -0.006083 0.118093
+0.375496 0.002722 0.118093
+0.374163 0.005309 0.118093
+0.375989 0.007423 0.118093
+0.374936 0.015787 0.118201
+0.375111 0.016379 0.095824
+0.374044 0.016073 0.096340
+0.375681 0.016571 0.098025
+0.375898 0.016590 0.101754
+0.374456 0.016464 0.102741
+0.375994 0.016599 0.104413
+0.374178 0.016109 0.106815
+0.374645 0.016480 0.108655
+0.374140 0.016099 0.110606
+0.374858 0.016291 0.112063
+0.375081 0.016519 0.114257
+0.377648 -0.009076 0.093560
+0.377687 -0.009079 0.095183
+0.376660 -0.008989 0.097714
+0.376107 -0.008949 0.099410
+0.377936 -0.009101 0.100806
+0.377709 -0.009081 0.104145
+0.376114 -0.008960 0.106776
+0.376879 -0.009251 0.108838
+0.377726 -0.009392 0.111475
+0.376178 -0.008977 0.112081
+0.376075 -0.009362 0.114525
+0.377606 -0.009072 0.116119
+0.376452 -0.007583 0.119091
+0.377545 -0.002195 0.118093
+0.376151 0.003291 0.118093
+0.376894 0.004738 0.118093
+0.377051 0.011299 0.118093
+0.376568 0.015749 0.119066
+0.377345 0.016718 0.096195
+0.376522 0.016645 0.099200
+0.377727 0.016829 0.100859
+0.377807 0.016759 0.102102
+0.376236 0.016661 0.104701
+0.377154 0.016725 0.107483
+0.376964 0.016684 0.108264
+0.376702 0.016705 0.111514
+0.377585 0.016739 0.113959
+0.377071 0.016840 0.114972
+0.376396 0.016781 0.117256
+0.378564 -0.009108 0.093374
+0.378810 -0.009178 0.095462
+0.379147 -0.009135 0.097411
+0.379218 -0.009100 0.099908
+0.378472 -0.009148 0.101372
+0.379046 -0.009684 0.103221
+0.378547 -0.009141 0.109732
+0.379906 -0.009644 0.110239
+0.378228 -0.009893 0.112088
+0.379908 -0.009275 0.114673
+0.379054 -0.007535 0.118796
+0.379651 -0.004341 0.118093
+0.379069 -0.002279 0.118093
+0.379289 0.016708 0.095718
+0.379429 0.016695 0.096594
+0.379362 0.016701 0.099532
+0.379857 0.016658 0.100685
+0.379650 0.016800 0.102620
+0.378260 0.016798 0.104741
+0.379150 0.016720 0.106216
+0.379684 0.016673 0.108435
+0.378606 0.016768 0.111700
+0.379524 0.016864 0.113061
+0.379534 0.016686 0.114692
+0.380678 -0.008917 0.093684
+0.380091 -0.008968 0.095727
+0.380446 -0.008937 0.097804
+0.380227 -0.008957 0.098265
+0.380239 -0.009304 0.100909
+0.381913 -0.008709 0.103581
+0.381454 -0.008678 0.105178
+0.381375 -0.008855 0.108542
+0.381548 -0.008840 0.111250
+0.381753 -0.009437 0.114324
+0.380921 -0.008895 0.116574
+0.381872 -0.007740 0.118489
+0.381599 -0.002004 0.118093
+0.381262 0.004550 0.118093
+0.380136 0.015840 0.118358
+0.381671 0.016289 0.095403
+0.380168 0.016651 0.096253
+0.381063 0.016552 0.100306
+0.381868 0.016236 0.102817
+0.381827 0.016247 0.105314
+0.380682 0.016576 0.106967
+0.380780 0.016906 0.109908
+0.381767 0.017078 0.111952
+0.380017 0.017261 0.113367
+0.381435 0.017272 0.114005
+0.380273 0.016632 0.117343
+0.381328 0.016874 0.118044
+0.382161 -0.008488 0.095110
+0.382921 -0.008260 0.097153
+0.382986 -0.008344 0.098089
+0.383234 -0.008201 0.101561
+0.382948 -0.008717 0.102324
+0.382015 -0.008528 0.104934
+0.383481 -0.008670 0.106081
+0.382438 -0.008432 0.109446
+0.382156 -0.008490 0.111295
+0.382833 -0.008280 0.113822
+0.383114 -0.008194 0.114108
+0.383068 -0.008592 0.116518
+0.382538 -0.008259 0.118357
+0.383739 -0.007858 0.095354
+0.383681 -0.007885 0.099741
+0.383769 -0.007844 0.114850
+0.382505 -0.007502 0.118093
+0.383284 -0.005883 0.118093
+0.382576 0.004305 0.118093
+0.382131 0.007470 0.118093
+0.382580 0.009586 0.118093
+0.382720 0.013834 0.118973
+0.383613 0.015767 0.093911
+0.383093 0.015907 0.095078
+0.382737 0.015998 0.098424
+0.383152 0.015804 0.100519
+0.382750 0.015992 0.105866
+0.383336 0.015715 0.110428
+0.383477 0.015946 0.113291
+0.383682 0.015803 0.114139
+0.383560 0.015795 0.117517
+0.382529 0.016058 0.097329
+0.383879 0.016304 0.104545
+0.382724 0.016006 0.107032
+0.382645 0.016037 0.108339
+0.382946 0.016318 0.111739
+0.383481 0.016493 0.112756
+0.382388 0.016907 0.114340
+0.382487 0.016096 0.117251
+0.384959 -0.008250 0.113894
+0.384976 -0.008049 0.115582
+0.384473 -0.007516 0.095615
+0.384161 -0.007800 0.096138
+0.384317 -0.007589 0.098092
+0.384739 -0.007392 0.100522
+0.384639 -0.007823 0.102675
+0.384037 -0.007719 0.107318
+0.385302 -0.007989 0.109224
+0.384455 -0.007873 0.111717
+0.385098 -0.007907 0.112669
+0.385035 -0.007465 0.115745
+0.385992 -0.007136 0.116214
+0.384463 -0.007318 0.118086
+0.385999 -0.004818 0.119025
+0.385562 -0.002859 0.118093
+0.384824 -0.000230 0.118093
+0.385135 0.006852 0.118093
+0.385812 0.014371 0.095417
+0.384688 0.015085 0.097752
+0.384059 0.015378 0.099003
+0.385468 0.014551 0.101787
+0.384553 0.015151 0.102082
+0.385714 0.015454 0.107838
+0.385357 0.014628 0.111657
+0.385910 0.014254 0.112179
+0.385841 0.014752 0.115266
+0.384509 0.015904 0.116896
+0.385444 0.014371 0.118768
+0.384434 0.016100 0.112197
+0.384028 0.016171 0.117893
+0.387563 -0.006409 0.095317
+0.386740 -0.006808 0.100292
+0.386218 -0.007098 0.102575
+0.386842 -0.006419 0.104533
+0.387684 -0.006020 0.106527
+0.387245 -0.006225 0.109772
+0.387072 -0.006091 0.112909
+0.386542 -0.006129 0.114241
+0.386170 -0.006386 0.117776
+0.387469 -0.005524 0.104357
+0.387497 -0.005460 0.108437
+0.387172 -0.005948 0.113679
+0.387766 -0.005224 0.115744
+0.387404 -0.005047 0.118267
+0.386852 -0.003582 0.118983
+0.387554 0.006120 0.118858
+0.387235 0.008281 0.118093
+0.386866 0.010076 0.119063
+0.387689 0.012993 0.097957
+0.387441 0.013437 0.099080
+0.387236 0.013119 0.100453
+0.386317 0.013956 0.102172
+0.386830 0.013761 0.106745
+0.387660 0.012778 0.109183
+0.387947 0.013682 0.111203
+0.387676 0.013967 0.113693
+0.386487 0.013836 0.114459
+0.387596 0.013444 0.117615
+0.386129 0.013967 0.118814
+0.387014 0.014395 0.097674
+0.386192 0.014384 0.103717
+0.386534 0.014526 0.106971
+0.386899 0.014055 0.110353
+0.386253 0.014670 0.114510
+0.389447 -0.004093 0.094561
+0.388236 -0.004622 0.099447
+0.388428 -0.004258 0.105876
+0.388587 -0.004331 0.107252
+0.388068 -0.004669 0.108213
+0.388382 -0.005016 0.110856
+0.388545 -0.004725 0.113354
+0.388245 -0.004935 0.114158
+0.388551 -0.004452 0.117905
+0.389450 -0.003235 0.094292
+0.389160 -0.003126 0.101517
+0.389149 -0.003537 0.103708
+0.389545 -0.002562 0.105249
+0.389662 -0.003024 0.106872
+0.388841 -0.003844 0.111315
+0.389190 -0.003281 0.113834
+0.388901 -0.003759 0.114012
+0.388952 -0.003659 0.117438
+0.388276 -0.003036 0.118544
+0.389630 -0.001395 0.118027
+0.388738 0.001138 0.119066
+0.389202 0.002826 0.118536
+0.388960 0.006442 0.119058
+0.389740 0.009769 0.109020
+0.389702 0.009849 0.111438
+0.389913 0.009397 0.113588
+0.389970 0.008542 0.118001
+0.388527 0.011827 0.101815
+0.388998 0.011360 0.105813
+0.389443 0.010406 0.106640
+0.388756 0.011421 0.109290
+0.389502 0.010280 0.110273
+0.388528 0.011715 0.114930
+0.389987 0.010546 0.116152
+0.388678 0.011339 0.118386
+0.388005 0.012349 0.103096
+0.388644 0.012157 0.107002
+0.388681 0.012298 0.110854
+0.388354 0.012000 0.112008
+0.388228 0.013015 0.114684
+0.388265 0.012056 0.116245
+0.390266 -0.002402 0.109625
+0.390179 -0.003328 0.114152
+0.391475 -0.000294 0.095170
+0.390011 -0.001898 0.097141
+0.391004 -0.000816 0.101165
+0.390109 -0.001757 0.103073
+0.390824 -0.000703 0.104166
+0.390664 -0.000965 0.107371
+0.390592 -0.000030 0.108510
+0.390377 -0.000746 0.110349
+0.391122 -0.000162 0.113552
+0.391407 -0.000362 0.115925
+0.391285 -0.000927 0.116732
+0.391115 0.001918 0.093174
+0.391392 0.001446 0.097553
+0.391330 0.001313 0.100057
+0.391497 0.001672 0.102722
+0.391019 0.001520 0.106523
+0.391557 0.001800 0.109097
+0.390978 0.001357 0.111099
+0.390991 0.001192 0.112776
+0.391866 0.001231 0.117567
+0.390806 0.001572 0.118485
+0.391413 0.003029 0.095976
+0.391575 0.003740 0.104151
+0.391159 0.003104 0.107661
+0.391573 0.003777 0.108089
+0.391234 0.003709 0.113406
+0.391871 0.003988 0.115355
+0.391837 0.003680 0.116103
+0.391508 0.004814 0.095166
+0.391094 0.005304 0.105005
+0.391197 0.004132 0.109498
+0.391480 0.004345 0.111451
+0.391139 0.004786 0.113345
+0.391054 0.005761 0.114061
+0.391367 0.005912 0.116191
+0.391041 0.005108 0.118351
+0.390930 0.007649 0.101020
+0.391191 0.007883 0.103852
+0.391011 0.006210 0.106092
+0.390605 0.007649 0.109268
+0.391155 0.006732 0.110791
+0.390883 0.006613 0.112730
+0.390947 0.007491 0.115257
+0.391591 0.006615 0.117318
+0.391111 0.008939 0.094418
+0.391245 0.008009 0.099608
+0.390769 0.009157 0.100570
+0.390006 0.009353 0.104675
+0.390406 0.008988 0.106967
+0.390105 0.008986 0.109426
+0.390127 0.009041 0.110037
+0.390425 0.008299 0.113044
+0.390726 0.008546 0.116444
+0.390121 0.010234 0.115613
+0.392128 0.003025 0.101011
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg_robot/env.py OWG-main/owg_robot/env.py
--- OWG-upstream/owg_robot/env.py	2025-12-25 13:40:47.056633225 +0800
+++ OWG-main/owg_robot/env.py	2025-11-22 22:01:12.612286545 +0800
@@ -11,7 +11,7 @@
 import matplotlib.pyplot as plt
 from matplotlib.ticker import FuncFormatter
 from matplotlib.patches import FancyBboxPatch
-from numpy.lib.npyio import save
+import numpy as np
 from PIL import Image
 from typing import *
 from pprint import pprint
@@ -959,7 +959,15 @@
                        grasp_indices: Optional[List[int]] = None):
         """
         Pick object by its state id, potentially with ranked grasp indices 
-        """
+        """        
+        # ---- å®‰å…¨åˆå§‹åŒ–ï¼Œé¿å… UnboundLocalError ----
+        succes_grasp = False
+        grasped_obj_id = None
+
+        grasps = self.get_obj_grasps(obj_id)
+        if grasps is None or len(grasps) == 0:
+            print(f"[WARN] No grasps available for obj {obj_id}")
+            return False, None, None                
         success_grasp, grasped_obj_id = False, None
         idx = self.obj_ids.index(obj_id)
         grasps = self.obj_grasps[idx]
@@ -1081,6 +1089,40 @@
         else:
             return False
 
+    def _log_ui_grasp_exec(self, mode, obj_id, grasp, success_grasp, success_target):
+        """
+        Append one grasp execution record used by UI.
+        mode: 'tray' or 'free_space'
+        grasp: tuple (x, y, z, yaw, opening_len, obj_height) or None.
+        """
+        import os, time, json  # ç¡®ä¿æ–‡ä»¶é¡¶éƒ¨å·²ç» import è¿‡ä¹Ÿå¯ä»¥å»æ‰è¿™è¡Œ
+
+        LOG_PATH = "logs/ui_grasp_exec.jsonl"
+        os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
+
+        if grasp is None:
+            x = y = z = yaw = opening_len = obj_height = None
+        else:
+            x, y, z, yaw, opening_len, obj_height = grasp
+
+        log_item = dict(
+            time=time.strftime("%Y-%m-%d %H:%M:%S"),
+            mode=mode,                    # "tray" or "free_space"
+            obj_id=int(obj_id),
+            x=float(x) if x is not None else None,
+            y=float(y) if y is not None else None,
+            z=float(z) if z is not None else None,
+            yaw=float(yaw) if yaw is not None else None,
+            opening_len=float(opening_len) if opening_len is not None else None,
+            obj_height=float(obj_height) if obj_height is not None else None,
+            success_grasp=bool(success_grasp),
+            success_target=bool(success_target),
+        )
+
+        with open(LOG_PATH, "a") as f:
+            f.write(json.dumps(log_item) + "\n")
+
+
     def put_obj_in_tray(self,
                         obj_id: int,
                         debug: bool = False,
@@ -1097,6 +1139,15 @@
 
         if not succes_grasp:
             print('Grasping failed. Exitting')
+
+            # è®°å½•ä¸€æ¬¡å¤±è´¥çš„ grasp æ—¥å¿—ï¼ˆæ²¡æœ‰æ‹¿èµ·æ¥ï¼Œä¹Ÿæ²¡æœ‰æ”¾æˆåŠŸï¼‰
+            self._log_ui_grasp_exec(
+                mode="tray",
+                obj_id=obj_id,
+                grasp=None,
+                success_grasp=False,
+                success_target=False,
+            )
             return False, False
 
         # Move object to target zone
@@ -1124,6 +1175,15 @@
             succes_target = True
             #self.remove_obj(grasped_obj_id)
 
+        # âœ… æ— è®ºæ”¾æ²¡æ”¾è¿›æ‰˜ç›˜ï¼Œè¿™é‡Œéƒ½è®°ä¸€æ¡æ—¥å¿—
+        self._log_ui_grasp_exec(
+            mode="tray",
+            obj_id=grasped_obj_id,
+            grasp=grasp,
+            success_grasp=succes_grasp,
+            success_target=succes_target,
+        )
+
         return succes_grasp, succes_target
 
     def put_obj_in_free_space(self,
@@ -1142,6 +1202,10 @@
             np.linalg.norm(target_pos_candidates - obj_pos, axis=1))[1]
         #chosen_idx = np.argmin(np.linalg.norm(target_pos_candidates - obj_pos, axis=1))
         target_pos = target_pos_candidates[chosen_idx]
+        
+        LOG_PATH = "logs/ui_grasp_exec.jsonl"
+        os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
+        
         return self.put_obj_in_loc(obj_id,
                                    target_pos,
                                    grasp_indices=grasp_indices,
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg_robot/grasp_ranker_lggsn.py OWG-main/owg_robot/grasp_ranker_lggsn.py
--- OWG-upstream/owg_robot/grasp_ranker_lggsn.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/owg_robot/grasp_ranker_lggsn.py	2025-11-23 17:08:16.950499507 +0800
@@ -0,0 +1,176 @@
+import numpy as np
+import torch
+
+from lggsn_model import LGGSN
+
+# ä¸ train_lggsn.py ä¸€è‡´çš„å‡ ä½•ç‰¹å¾ç»´åº¦
+FEATURE_COLS = [
+    "x", "y", "z",
+    "roll", "pitch", "yaw",
+    "width", "score",
+    "dz", "dz_lift", "need_dz", "H",
+]
+
+
+class LggsnGraspRanker:
+    """
+    ç”¨å‡ ä½• LGGSN æ¨¡å‹å¯¹ä¸€æ‰¹ 3D grasps æ‰“åˆ†å¹¶æ’åºã€‚
+
+    æ—¢å…¼å®¹:
+      - JSON/dict å½¢å¼çš„ grasp: {"position":[x,y,z], "rpy":[r,p,y], "width":w, "score":s, ...}
+      - ä¹Ÿå…¼å®¹ GR-ConvNet åœ¨çº¿ç”Ÿæˆæ—¶å¯èƒ½è¿”å›çš„ tuple / list å½¢å¼:
+          (pos, rpy, width, score, ...)
+          æˆ– ( {dict_grasp}, ...extra )
+    """
+
+    def __init__(
+        self,
+        model_path: str = "grasp_6dof/models/lggsn_geom_only.pt",
+        device: str = "cuda",
+    ):
+        # è®¾å¤‡
+        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
+
+        # æ¨¡å‹ï¼šgeom_dim = ç‰¹å¾ç»´åº¦
+        self.model = LGGSN(
+            n_queries=1,
+            geom_dim=len(FEATURE_COLS),
+            query_dim=0,
+            hidden_dim=40,
+        )
+        state = torch.load(model_path, map_location=self.device)
+        self.model.load_state_dict(state)
+        self.model.to(self.device)
+        self.model.eval()
+
+        # å‘Šè¯‰ä¸Šå±‚è¿™æ˜¯ 3D grasp rankerï¼ˆä¸ç”¨å›¾åƒï¼‰
+        self.use_3d_prompt = True
+
+    # --------- è¾“å…¥æ ¼å¼é€‚é… ---------
+
+    def _unwrap_grasp(self, g):
+        """
+        æŠŠå¤šç§å¯èƒ½çš„ grasp è¡¨ç¤ºç»Ÿä¸€æ•´ç†æˆ dict å½¢å¼:
+          {"position":[x,y,z], "rpy":[r,p,y], "width":w, "score":s, ...}
+        """
+        # 1) å·²ç»æ˜¯ dict çš„æƒ…å†µï¼ˆç¦»çº¿åº“ / validated graspsï¼‰
+        if isinstance(g, dict):
+            return g
+
+        # 2) tuple / list / np.ndarray
+        if isinstance(g, (tuple, list, np.ndarray)):
+            # 2.1) å½¢å¦‚ (dict_grasp, extra_info...)
+            if len(g) > 0 and isinstance(g[0], dict):
+                return g[0]
+
+            # 2.2) å½¢å¦‚ (pos, rpy, width, score, ...)
+            #      å…¶ä¸­ pos/rpy é€šå¸¸æ˜¯é•¿åº¦ä¸º 3 çš„ list/ndarray
+            if len(g) >= 2 and isinstance(g[0], (tuple, list, np.ndarray)):
+                pos = list(g[0])
+                rpy = list(g[1]) if isinstance(g[1], (tuple, list, np.ndarray)) else [np.pi, 0.0, 0.0]
+
+                pos = [float(p) for p in pos[:3]] + [0.0] * (3 - len(pos))
+                rpy = [float(r) for r in rpy[:3]] + [0.0] * (3 - len(rpy))
+
+                width = 0.04
+                score = 0.0
+                # ä»åé¢çš„æ ‡é‡é‡Œé¡ºåºçŒœ width / score
+                for v in g[2:]:
+                    if isinstance(v, (int, float, np.floating)):
+                        if width == 0.04:
+                            width = float(v)
+                        elif score == 0.0:
+                            score = float(v)
+
+                return {
+                    "position": pos,
+                    "rpy": rpy,
+                    "width": width,
+                    "score": score,
+                }
+
+            # 2.3) å®Œå…¨æ‰å¹³çš„ä¸€ä¸²æ•°å­—ï¼ŒæŒ‰ [x,y,z,roll,pitch,yaw,width,score,...] è§£é‡Š
+            flat = [float(v) for v in g]
+            while len(flat) < 8:
+                flat.append(0.0)
+            pos = flat[0:3]
+            rpy = flat[3:6]
+            width = flat[6]
+            score = flat[7]
+            return {
+                "position": pos,
+                "rpy": rpy,
+                "width": width,
+                "score": score,
+            }
+
+        # 3) å…¶ä»–æœªçŸ¥ç±»å‹ï¼šè¿”å›ä¸€ä¸ªé»˜è®¤å ä½ï¼Œé¿å…ç›´æ¥å´©æºƒ
+        return {
+            "position": [0.0, 0.0, 0.0],
+            "rpy": [np.pi, 0.0, 0.0],
+            "width": 0.04,
+            "score": 0.0,
+        }
+
+    # --------- ç‰¹å¾æ„é€  ---------
+
+    def _featurize_one(self, g):
+        """
+        ä»å•ä¸ª graspï¼ˆä»»æ„æ”¯æŒçš„æ ¼å¼ï¼‰æŠ½å–å‡ ä½• + è´¨é‡ç‰¹å¾ã€‚
+        """
+        g = self._unwrap_grasp(g)
+
+        pos = g.get("position") or g.get("pos") or [0.0, 0.0, 0.0]
+        rpy = g.get("rpy") or [np.pi, 0.0, 0.0]
+
+        pos = [float(p) for p in pos[:3]] + [0.0] * (3 - len(pos))
+        rpy = [float(r) for r in rpy[:3]] + [0.0] * (3 - len(rpy))
+
+        width = float(g.get("width", 0.04))
+        score = float(g.get("score", 0.0))
+        dz = float(g.get("dz", 0.0))
+
+        m = g.get("_metrics", {}) or {}
+        dz_lift = float(m.get("dz_lift", dz))
+        need_dz = float(m.get("need_dz", 0.0))
+        H = float(m.get("H", 0.08))
+
+        x, y, z = pos
+        roll, pitch, yaw = rpy
+
+        return [
+            x, y, z,
+            roll, pitch, yaw,
+            width, score,
+            dz, dz_lift, need_dz, H,
+        ]
+
+    def _featurize(self, grasps):
+        feats = [self._featurize_one(g) for g in grasps]
+        return np.asarray(feats, dtype=np.float32)
+
+    # --------- æ’åºæ¥å£ ---------
+
+    def rank(self, grasps, query_text: str | None = None, obj_type: str | None = None):
+        """
+        è¾“å…¥:
+          grasps: list[dict æˆ– tuple]
+
+        è¾“å‡º:
+          order: np.array[int]ï¼ŒæŒ‰è´¨é‡ä»é«˜åˆ°ä½çš„ç´¢å¼•é¡ºåº
+          scores: np.array[float]ï¼Œæ¯ä¸ª grasp çš„ [0,1] è´¨é‡åˆ†
+        """
+        if len(grasps) == 0:
+            return np.array([], dtype=int), np.array([], dtype=float)
+
+        X = self._featurize(grasps)
+        geom = torch.from_numpy(X).to(self.device)
+        q_id = torch.zeros(len(grasps), dtype=torch.long, device=self.device)
+
+        with torch.no_grad():
+            logit = self.model(geom, q_id)        # [N]
+            score = torch.sigmoid(logit).cpu().numpy()
+
+        order = np.argsort(-score)                # ä»å¤§åˆ°å°æ’åº
+        return order, score
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/owg_robot/ui.py OWG-main/owg_robot/ui.py
--- OWG-upstream/owg_robot/ui.py	2025-12-25 13:40:47.056633225 +0800
+++ OWG-main/owg_robot/ui.py	2025-11-24 18:26:40.718945392 +0800
@@ -5,7 +5,7 @@
 import tkinter as tk
 from tkinter import simpledialog, messagebox, scrolledtext
 from typing import *
-
+import os, json
 from owg_robot.env import *
 from owg_robot.camera import Camera
 from owg_robot.objects import YcbObjects
@@ -13,7 +13,17 @@
 from owg.utils.config import load_config
 from owg.utils.grasp import Grasp2D
 from third_party.grconvnet import load_grasp_generator
+from datetime import datetime
 
+LOG_DIR = "logs"
+os.makedirs(LOG_DIR, exist_ok=True)
+LOG_PATH = os.path.join(LOG_DIR, "ui_grasp_exec.jsonl")
+
+def log_exec(event: dict):
+    """Append one grasp execution record to JSONL log."""
+    event = dict(time=time.strftime("%Y-%m-%d %H:%M:%S"), **event)
+    with open(LOG_PATH, "a") as f:
+        f.write(json.dumps(event) + "\n")
 
 # GUI stuff
 # Function to create a text input dialog using Tkinter
@@ -22,7 +32,7 @@
     root.withdraw()  # Hide the main window
     user_input = simpledialog.askstring("Input", "User Input: ")
     root.destroy()
-    return user_input
+    return input("è¯·è¾“å…¥å¯¹è±¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰ï¼š")
 
 
 class RobotEnvUI:
@@ -50,6 +60,10 @@
                                finger_length=self.cfg.finger_length,
                                n_grasp_attempts=self.cfg.n_grasp_attempts)
 
+        # è‡ªç„¶è¯­è¨€çº§æ‰§è¡Œæ—¥å¿—ï¼ˆquery -> action -> successï¼‰
+        os.makedirs("logs", exist_ok=True)
+        self.nl_log_path = os.path.join("logs", "ui_nl_exec.jsonl")
+
         # load objects
         self.objects = YcbObjects(
             './owg_robot/assets/ycb_objects',
@@ -65,7 +79,9 @@
             verbose=self.cfg.policy.verbose,
             vis=self.cfg.policy.vis,
             use_grasp_ranker=self.cfg.policy.use_grasp_ranker)
-
+        print(f"[DEBUG] cfg.use_grasp_ranker = {self.cfg.policy.use_grasp_ranker}, "
+            f"policy.use_grasp_ranker = {getattr(self.policy, 'use_grasp_ranker', None)}")
+         
         self.grasp_rank_3d = False
         if self.cfg.policy.use_grasp_ranker:
             self.grasp_rank_3d = self.policy.grasp_ranker.use_3d_prompt
@@ -194,13 +210,21 @@
             - `action`: Either `remove` to place blocking object in free space, or `pick` to put target in tray.
             - `input`: The object ID of object to manipulate.
         '''
-        #for attempt in range(self.cfg.n_action_attempts):
+        # ---- ç»Ÿä¸€å¤„ç† graspsï¼šå¦‚æœ planner ç»™çš„æ˜¯ [] æˆ– Noneï¼Œå°±å›é€€åˆ°é»˜è®¤ grasp é›†åˆ ----
+        grasp_indices = action.get('grasps')
+        if not grasp_indices:   # None æˆ– []
+            print("[WARN] Empty grasp list from planner, fallback to default indices")
+            # é»˜è®¤ç”¨å‰ n_grasp_attempts ä¸ª grasp
+            grasp_indices = list(range(self.n_grasp_attempts))
+            action['grasps'] = grasp_indices
+
         if action['action'] == 'remove':
             success_grasp, success_target = self.env.put_obj_in_free_space(
                 action['input'], grasp_indices=action['grasps'])
         elif action['action'] == 'pick':
             success_grasp, success_target = self.env.put_obj_in_tray(
                 action['input'], grasp_indices=action['grasps'])
+
         for _ in range(30):
             self.env.step_simulation()
 
@@ -222,66 +246,107 @@
 
     def run(self):
         while True:
-            # ask user for command
+            # æ¯ä¸€è½®å…ˆæŠŠæœºæ¢°è‡‚å¤ä½
             self.env.reset_robot()
-            user_input = ask_for_user_input()
 
-            # reset same scene
+            # åªé—®ä¸€æ¬¡è‡ªç„¶è¯­è¨€æŒ‡ä»¤
+            user_input = input("è¯·è¾“å…¥å¯¹è±¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰ï¼š").strip()
+            if not user_input:
+                continue
+
+            # ç‰¹æ®Šå‘½ä»¤
             if user_input == ':reset':
+                # åŒä¸€åœºæ™¯é‡æ¥
                 self.reset(new=False)
                 self.env.dummy_simulation_steps(10)
                 continue
 
-            # spawn new scene
             elif user_input == ':new':
+                # æ¢ä¸€æ‰¹æ–°ç‰©ä½“
                 self.reset(new=True)
                 self.env.dummy_simulation_steps(10)
                 continue
 
-            # close demo
             elif user_input == ':exit':
                 print('Exitting demo')
                 self.env.close()
                 break
 
-            # actual query
-            else:
-                # call OWG Policy to predict next action
+            # çœŸæ­£çš„ä»»åŠ¡æŒ‡ä»¤
+            attempt = 0
+            while True:
+                # æ¯æ¬¡åŠ¨ä½œå‰æŠŠæœºæ¢°è‡‚å›åˆ°åˆå§‹å§¿æ€
+                self.env.reset_robot()
+                obs = self.update()
+
+                # ä¸º policy å‡†å¤‡æŠ“å–å€™é€‰ï¼ˆ2D æˆ– 3Dï¼‰
+                if self.grasp_rank_3d:
+                    all_grasps = {
+                        k: self.env.get_obj_grasps(k)
+                        for k in self.env.obj_ids
+                    }
+                else:
+                    all_grasps = {
+                        k: self.env.get_obj_grasp_rects(k)
+                        for k in self.env.obj_ids
+                    }
+
+                # ç”¨ OWG policy é¢„æµ‹åŠ¨ä½œ
+                action = self.policy.predict(obs, user_input, all_grasps)
+
+                if action['action'] == 'fail':
+                    success, done = False, False
+                else:
+                    success, done = self.step(action)
+
+                # è‡ªç„¶è¯­è¨€æ‰§è¡Œæ—¥å¿—ï¼ˆquery -> action -> successï¼‰
+                try:
+                    log_item = {
+                        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
+                        "query": user_input,    # âœ… ç”¨ user_inputï¼Œä¸è¦ç”¨ text
+                        "action": action,
+                        "success": bool(success),
+                        "use_lggsn": bool(self.policy.use_grasp_ranker),
+                    }
+                    with open(self.nl_log_path, "a") as f:
+                        f.write(json.dumps(log_item, ensure_ascii=False) + "\n")
+                except Exception as e:
+                    print(f"[WARN] failed to log NL exec: {e}")
+
+                # ä¸‹é¢æ˜¯æ§åˆ¶é‡è¯•/ç»“æŸé€»è¾‘
+                if success and done:
+                    # ä»»åŠ¡å®Œå…¨å®Œæˆï¼ˆä¾‹å¦‚ pick åˆ°æ‰˜ç›˜ï¼‰
+                    break
+
+                if not success:
+                    attempt += 1
+                    if attempt >= self.n_action_attempts:
+                        print('Action failed. No more atempts.')
+                        break
+                    print(f'Action failed. {attempt} attempt. Retrying..')
+                    continue
+
+                # æˆåŠŸä½†è¿˜æ²¡ç»“æŸæ•´æ¡è®¡åˆ’ï¼ˆä¾‹å¦‚ remove äº†ä¸Šé¢çš„ç‰©ä½“ï¼‰
                 attempt = 0
-                while True:
-                    self.env.reset_robot()
-                    obs = self.update()
-
-                    # save a dict for all predicted grasps for policy grasp ranking
-                    if self.grasp_rank_3d:
-                        all_grasps = {
-                            k: self.env.get_obj_grasps(k)
-                            for k in self.env.obj_ids
-                        }
-                    else:
-                        all_grasps = {
-                            k: self.env.get_obj_grasp_rects(k)
-                            for k in self.env.obj_ids
-                        }
+                self.env.dummy_simulation_steps(30)
+                continue
+def main():
+    import argparse
 
-                    action = self.policy.predict(obs, user_input, all_grasps)
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--config",
+        type=str,
+        default="env.yaml",
+        help="Path to environment config file (YAML).",
+    )
+    args = parser.parse_args()
+
+    # åˆ›å»º UI å¹¶è¿è¡Œä¸»å¾ªç¯
+    ui = RobotEnvUI(args.config)
+    ui.run()
+
+
+if __name__ == "__main__":
+    main()
 
-                    if action['action'] == 'fail':
-                        break
-                    # convert marker IDs to state IDs
-                    success, done = self.step(action)
-                    if success and done:
-                        # task finished
-                        break
-                    elif not success:
-                        attempt += 1
-                        if attempt == self.cfg.n_action_attempts:
-                            print(f'Action failed. No more atempts.')
-                            break
-                        print(f'Action failed. {attempt} attempt. Retrying..')
-                        continue
-                    else:
-                        # plan step finished, move to next
-                        attempt = 0
-                        self.env.dummy_simulation_steps(30)
-                        continue
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/patch.diff OWG-main/patch.diff
--- OWG-upstream/patch.diff	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/patch.diff	2025-11-14 21:52:33.562416587 +0800
@@ -0,0 +1,281 @@
+*** a/grasp_6dof/validate_grasps_panda.py
+--- b/grasp_6dof/validate_grasps_panda.py
+***************
+*** 1,6 ****
+  # -*- coding: utf-8 -*-
+  import pybullet as p
+  import pybullet_data
+  import numpy as np
+  import json
+  import time
+--- 1,7 ----
+  # -*- coding: utf-8 -*-
+  import pybullet as p
+  import pybullet_data
+  import numpy as np
+  import json
+  import time
++ import math
+  import argparse
+  import os
+  import random
+  from datetime import datetime
+***************
+*** 107,113 ****
+      LIFT_UP = 0.25
+      LIFT_SUCCESS_DZ = 0.05
+      JOINT_FORCE = float(joint_force)
+      IK_ITERS = int(ik_iters)
+      IK_ATTEMPTS = max(1, int(ik_attempts))
+  
+      # ---------- step å‡½æ•° ----------
+      if step_fn is None:
+          def step_fn(n):
+--- 108,116 ----
+      LIFT_UP = 0.25
+      LIFT_SUCCESS_DZ = 0.05
+      JOINT_FORCE = float(joint_force)
+      IK_ITERS = int(ik_iters)
+      IK_ATTEMPTS = max(1, int(ik_attempts))
++     CONTACT_NEG_MARGIN = 0.05  # æ‰æ¡Œ/æ•°å€¼çˆ†æ‰çš„ä¸‹ç•Œé˜ˆå€¼
+  
+      # ---------- step å‡½æ•° ----------
+      if step_fn is None:
+          def step_fn(n):
+***************
+*** 217,237 ****
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(0.20 * 480))
+  
+      if not contact:
+          cps = p.getContactPoints(bodyA=panda_id, bodyB=obj_id)
+          contact = any(c[3] in finger_ids for c in cps)
+  
+      # ---------- è½»æŠ¬ + äºŒæ¬¡æŒ¤å‹ ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + 0.015], orn=quat_target, steps=120)
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(SQUEEZE_EXTRA_TIME * 480))
+  
+      # ---------- æŠ¬å‡å¹¶åˆ¤å®š ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + LIFT_UP], orn=quat_target, steps=360)
+  
+      now_z = p.getBasePositionAndOrientation(obj_id)[0][2]
+      base_z = (init_base_z if init_base_z is not None else obj_pos0[2])
+!     lifted = (now_z - base_z) > LIFT_SUCCESS_DZ
+!     print(f"[DEBUG] contact={contact}, Î”z={now_z - base_z:.3f}, success={lifted}")
+  
+      # ---------- æ¾æ‰‹ ----------
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.04, force=200)
+      step_fn(int(0.20 * 480))
+  
+      return lifted
+--- 219,255 ----
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(0.20 * 480))
+  
+      if not contact:
+          cps = p.getContactPoints(bodyA=panda_id, bodyB=obj_id)
+          contact = any(c[3] in finger_ids for c in cps)
++     # æ²¡æ¥è§¦å°±ç›´æ¥å¤±è´¥å¹¶è½»æ”¾å›ï¼Œé¿å…â€œç›²æŠ¬â€æ¨ªæ‰«
++     if not contact:
++         for fid in finger_ids:
++             p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.04, force=200)
++         step_fn(int(0.15 * 480))
++         return False
+  
+      # ---------- è½»æŠ¬ + äºŒæ¬¡æŒ¤å‹ ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + 0.015], orn=quat_target, steps=120)
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.0, force=POS_CLOSE_FORCE)
+      step_fn(int(SQUEEZE_EXTRA_TIME * 480))
+  
+      # ---------- æŠ¬å‡å¹¶åˆ¤å®š ----------
+      ee = p.getLinkState(panda_id, end_effector_index, computeForwardKinematics=True)
+      ee_pos = np.array(ee[0])
+      move_to([ee_pos[0], ee_pos[1], ee_pos[2] + LIFT_UP], orn=quat_target, steps=360)
+  
+      now_z = p.getBasePositionAndOrientation(obj_id)[0][2]
+      base_z = (init_base_z if init_base_z is not None else obj_pos0[2])
+!     dz = float(now_z - base_z)
+!     fell_off = (dz < -CONTACT_NEG_MARGIN) or (abs(dz) > 0.5)
+!     lifted = (dz > LIFT_SUCCESS_DZ) and (not fell_off)
+!     print(f"[DEBUG] contact={contact}, Î”z={dz:.3f}, fell_off={fell_off}, success={lifted}")
+  
+      # ---------- æ¾æ‰‹ ----------
+      for fid in finger_ids:
+          p.setJointMotorControl2(panda_id, fid, p.POSITION_CONTROL, targetPosition=0.04, force=200)
+      step_fn(int(0.20 * 480))
+  
+      return lifted
+***************
+*** 287,300 ****
+      physicsClient = p.connect(p.GUI if (args.vis and not args.fast) else p.DIRECT)
+      p.setAdditionalSearchPath(pybullet_data.getDataPath())
+      p.setGravity(0, 0, -9.8)
+  
+      save_env_snapshot()
+  
+      # æ­¥è¿›å‡½æ•°ï¼šfast æ¨¡å¼å…³é—­ sleep ä¸”æŒ‰ scale å‡å°‘æ­¥æ•°
+      def step(n):
+          steps = int(max(1, n * (args.fast_scale if args.fast else 1.0)))
+          if args.fast or not args.vis:
+              for _ in range(steps):
+                  p.stepSimulation()
+          else:
+              for _ in range(steps):
+                  p.stepSimulation()
+                  time.sleep(1/480.0)
+--- 305,330 ----
+      physicsClient = p.connect(p.GUI if (args.vis and not args.fast) else p.DIRECT)
+      p.setAdditionalSearchPath(pybullet_data.getDataPath())
+      p.setGravity(0, 0, -9.8)
++     # â€”â€” ç‰©ç†ç¨³å®šæ€§/ç¡®å®šæ€§å‚æ•° â€”â€” 
++     p.setPhysicsEngineParameter(
++         deterministicOverlappingPairs=1,
++         collisionFilterMode=1,
++         contactSlop=1e-3,
++         enableConeFriction=1,
++         solverResidualThreshold=1e-7,
++         numSolverIterations=200,
++         erp=0.2, contactERP=0.2, frictionERP=0.2,
++         useSplitImpulse=1, splitImpulsePenetrationThreshold=-0.01
++     )
++     p.setTimeStep(1.0/480.0)
+  
+      save_env_snapshot()
+  
+      # æ­¥è¿›å‡½æ•°ï¼šfast æ¨¡å¼å…³é—­ sleep ä¸”æŒ‰ scale å‡å°‘æ­¥æ•°
+      def step(n):
+          steps = int(max(1, n * (args.fast_scale if args.fast else 1.0)))
+          if args.fast or not args.vis:
+              for _ in range(steps):
+                  p.stepSimulation()
+          else:
+              for _ in range(steps):
+                  p.stepSimulation()
+                  time.sleep(1/480.0)
+***************
+*** 306,311 ****
+--- 336,345 ----
+      table_id = p.loadURDF("table/table.urdf", basePosition=[0.5, 0, -0.63])
+      TABLE_TOP_Z = get_table_top_z(table_id)
+      print(f"[INFO] Detected table top z = {TABLE_TOP_Z:.3f}")
++     # æ¡Œé¢ä¹Ÿç»™é«˜æ‘©æ“¦å’Œä½å›å¼¹ï¼Œå‡å°‘è¢«â€œåˆ®è½æ¡Œâ€çš„æ¦‚ç‡
++     p.changeDynamics(table_id, -1,
++                      lateralFriction=1.6, rollingFriction=0.05, spinningFriction=0.02,
++                      restitution=0.0)
+  
+      # ç‰©ä½“ï¼šé™ç½®åœ¨æ¡Œé¢
+      CUBE_SCALE = float(args.cube_scale)
+      CUBE_HALF_Z = 0.5 * CUBE_SCALE
+***************
+*** 343,354 ****
+      except Exception:
+          grasps = []
+      if args.topk is not None and len(grasps) > 0:
+-         if "score" in grasps[0]:
+-             grasps = sorted(grasps, key=lambda g: g.get("score", 0.0), reverse=True)
+-         grasps = grasps[:args.topk]
++         # å¼ºåˆ¶å°è¯•æŒ‰ score æ’åºï¼Œå¥å£®å…œåº•
++         try:
++             grasps = sorted(grasps, key=lambda g: float(g.get("score", 0.0)), reverse=True)
++         except Exception:
++             pass
++         grasps = grasps[:args.topk]
+  
+      if len(grasps) == 0:
+          cx, cy, cz = p.getBasePositionAndOrientation(obj_id)[0]
+          z_above = cz + 0.12
+***************
+*** 384,392 ****
+          ok = grasp_with_panda(
+              obj_id, grasp, panda_id,
+              end_effector_index=END_EFFECTOR_INDEX,
+              finger_ids=finger_ids,
+              table_top_z=TABLE_TOP_Z,
+              init_base_z=init_base_z,
+              open_width_m=target_open,           # â† ä¼ è¿›å»
+              descent_step=args.descent_step,
+--- 418,427 ----
+          ok = grasp_with_panda(
+              obj_id, grasp, panda_id,
+              end_effector_index=END_EFFECTOR_INDEX,
+              finger_ids=finger_ids,
+              table_top_z=TABLE_TOP_Z,
+              init_base_z=init_base_z,
+              open_width_m=target_open,           # â† ä¼ è¿›å»
+              descent_step=args.descent_step,
+***************
+*** 396,402 ****
+              squeeze=args.squeeze,
+          )
+          out_g = dict(grasp); out_g["success"] = bool(ok)
+          validated.append(out_g)
+          if ok: success_count += 1
+          print(f"[{i+1}/{len(grasps)}] Grasp success = {ok}")
+--- 431,444 ----
+              squeeze=args.squeeze,
+          )
+          out_g = dict(grasp); out_g["success"] = bool(ok)
++         # ä»è°ƒè¯•æ‰“å°ä¸­å›å¡« contact / dz / fell_offï¼ˆå¦‚æœæœ‰å…¨å±€å˜é‡å°±ç•¥è¿‡ï¼Œä¿å®ˆè®¡ç®—ä¸€æ¬¡ï¼‰
++         cps_now = p.getContactPoints(bodyA=panda_id, bodyB=obj_id)
++         out_g["contact_seen"] = any(c[3] in finger_ids for c in cps_now)
++         now_z = p.getBasePositionAndOrientation(obj_id)[0][2]
++         out_g["dz"] = round(float(now_z - init_base_z), 4)
++         out_g["fell_off"] = bool((out_g["dz"] < -0.05) or (abs(out_g["dz"]) > 0.5))
++ 
+          validated.append(out_g)
+          if ok: success_count += 1
+          print(f"[{i+1}/{len(grasps)}] Grasp success = {ok}")
+***************
+*** 416,421 ****
+--- 458,488 ----
+      print(f"[INFO] Saved validated grasps â†’ {args.out}")
+  
+      fields = [
+          "time","obj","cube_scale","topk","seed",
+          "ee_index","ik_iters","ik_attempts","joint_force",
+          "descent_step","descend_clear","vel_close","pos_close","squeeze",
+          "fast","fast_scale",
+          "n_trials","success_count","success_rate"
+      ]
+      values = [
+          datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
+          args.obj, args.cube_scale, args.topk, args.seed,
+          args.ee_index, args.ik_iters, args.ik_attempts, args.joint_force,
+          args.descent_step, args.descend_clear, args.vel_close, args.pos_close, args.squeeze,
+          int(args.fast), args.fast_scale,
+          len(grasps), success_count, round(success_count/max(1,len(grasps)), 4)
+      ]
++     # ç‰©ä½“å½¢çŠ¶æ ‡ç­¾ï¼ˆè®°å½•åˆ° CSVï¼Œä¾¿äºåç»­åˆ†ç»„åˆ†æï¼‰
++     obj_aabb = p.getAABB(obj_id, -1)
++     dx = obj_aabb[1][0] - obj_aabb[0][0]
++     dy = obj_aabb[1][1] - obj_aabb[0][1]
++     dz = obj_aabb[1][2] - obj_aabb[0][2]
++     rx = abs(dx - dy) / max(1e-6, max(dx, dy))
++     ry = abs(dx - dz) / max(1e-6, max(dx, dz))
++     rz = abs(dy - dz) / max(1e-6, max(dy, dz))
++     obj_shape = "sphere_like" if (rx < 0.15 and ry < 0.15 and rz < 0.15) else "other"
++     fields += ["obj_shape"]
++     values += [obj_shape]
++ 
++     # è®°å½•æŠ“å–æ–‡ä»¶å’Œå£å¾„ tagï¼Œä¾¿äºè¿‡æ»¤å¯¹æ¯”
++     fields += ["grasps_path","tag"]
++     base_tag = f"{os.path.basename(args.obj)}_pc{int(args.pos_close)}_sq{args.squeeze}_topk{args.topk}"
++     gfile_tag = os.path.splitext(os.path.basename(args.grasps))[0] if args.grasps else "fallback"
++     values += [args.grasps, f"{base_tag}_{gfile_tag}"]
+  
+      append_summary_row(args.summary_c
+sv, fields, values)
+      print(f"[INFO] Appended summary â†’ {args.summary_csv}")
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/report.md OWG-main/report.md
--- OWG-upstream/report.md	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/report.md	2025-11-02 18:29:02.876162113 +0800
@@ -0,0 +1,61 @@
+# å®éªŒæŠ¥å‘Šï¼šé—¯å…³ç±»æ‰‹æ¸¸ç”¨æˆ·æµå¤±é¢„æµ‹
+
+## ä¸€ã€ä»»åŠ¡
+æ ¹æ® 2.1â€“2.4 çš„äº¤äº’æ—¥å¿—ï¼Œé¢„æµ‹æ¬¡å‘¨ï¼ˆ2.7â€“2.13ï¼‰æ˜¯å¦ç™»å½•ï¼ˆ1=æµå¤±ï¼Œ0=ç•™å­˜ï¼‰ã€‚è¯„ä»·æŒ‡æ ‡ï¼šAUCã€‚
+
+## äºŒã€æ•°æ®
+- `train.csv` / `dev.csv` / `test.csv`ï¼ˆå‡ä¸º **åˆ¶è¡¨ç¬¦åˆ†éš”**ï¼‰
+- `level_seq.csv`ï¼šé€å…³å¡åºåˆ—æ—¥å¿—ï¼Œå« `user_id, level_id, f_success, f_duration, f_reststep, f_help, time`
+- `level_meta.csv`ï¼šå…³å¡å±‚é¢ç»Ÿè®¡ï¼Œå« `level_id, f_avg_passrate, f_avg_duration, f_avg_win_duration, f_avg_retrytimes`
+- ï¼ˆå¯é€‰ï¼‰Groundtruth ç”¨äº Test å¤æ ¸
+
+## ä¸‰ã€æ–¹æ³•æ¦‚è¿°
+**æ€è·¯**ï¼šå°†åŸå§‹åºåˆ—æŒ‰ `user_id` èšåˆä¸ºç”¨æˆ·çº§ç‰¹å¾è¡¨ â†’ ä¼ ç»Ÿæ¨¡å‹å»ºæ¨¡ï¼ˆå¯è§£é‡Šã€é²æ£’ã€æ•ˆç‡é«˜ï¼‰â†’ å¤šæ¨¡å‹é›†æˆæå‡ä¸Šé™ã€‚
+
+### 3.1 ç‰¹å¾å·¥ç¨‹ï¼ˆç”¨æˆ·çº§èšåˆï¼‰
+- **ä½“é‡/å¤šæ ·æ€§**ï¼šæ€»æ¸¸ç©æ¬¡æ•° `plays_total`ã€ä¸åŒå…³å¡æ•° `levels_unique`ã€`level_max`
+- **ç»“æœè¡¨ç°**ï¼šæˆåŠŸæ¬¡æ•°/æˆåŠŸç‡ `succ_cnt/succ_rate`
+- **è¿‡ç¨‹å¼ºåº¦**ï¼šæ—¶é•¿ `dur_sum/dur_mean`ã€å‰©ä½™æ­¥æ•°å‡å€¼ `reststep_mean`ã€æ±‚åŠ©æ¯”ä¾‹ `help_rate`
+- **å…³å¡å…ƒä¿¡æ¯**ï¼ˆä¸ `level_meta` äº¤å‰ï¼‰ï¼šç”¨æˆ·ç»å†å…³å¡çš„å¹³å‡é€šå…³ç‡/å¹³å‡æ—¶é•¿/å¹³å‡èƒœåˆ©æ—¶é•¿/å¹³å‡é‡è¯•æ¬¡æ•°
+- **è¡ç”Ÿæ¯”ç‡**ï¼š`succ_per_play`ã€`dur_per_day` ç­‰
+> æ•°æ®æ¸…æ´—ï¼š`time` è§£æä¸º datetimeï¼Œä»…åœ¨èšåˆæ—¶ä½¿ç”¨ï¼Œæœ€ç»ˆè¾“å…¥æ¨¡å‹çš„å‡ä¸ºæ•°å€¼åˆ—ï¼Œé¿å… datetime ä¸ float æ··å‹å¯¼è‡´çš„ dtype é”™è¯¯ã€‚
+
+### 3.2 æ¨¡å‹ä¸è®­ç»ƒ
+- **åŸºå­¦ä¹ å™¨**ï¼š
+  - HistGradientBoostingï¼ˆæ ‘æ¨¡å‹ï¼Œèƒ½æ•æ‰éçº¿æ€§ã€äº¤äº’ï¼‰
+  - Logistic Regressionï¼ˆé…åˆç¼ºå¤±å€¼å¡«å……ä¸æ ‡å‡†åŒ–ï¼‰
+- **å †å ç­–ç•¥**ï¼š5 æŠ˜ Stratified KFold ç”Ÿæˆ OOFï¼ˆ`hgb_oof`ã€`lr_oof`ï¼‰â†’ ä½œä¸ºå…ƒç‰¹å¾å–‚ç»™ **å…ƒå­¦ä¹ å™¨ LR**
+- **è¯„ä¼°**ï¼šAUCï¼›ç»˜åˆ¶ ROC/PR ä»¥è§‚å¯Ÿé˜ˆå€¼å˜åŒ–ä¸‹çš„æ€§èƒ½
+
+## å››ã€å®éªŒç»“æœ
+- è®­ç»ƒæ ·æœ¬æ•°ï¼š**8158**
+- å¼€å‘é›†æ ·æœ¬æ•°ï¼š**2658**
+- æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š**2773**
+- æœ€ç»ˆç‰¹å¾ç»´åº¦ï¼š**55**
+- **Dev AUCï¼š0.799207**
+- ç”Ÿæˆæäº¤æ–‡ä»¶ï¼š`submission.csv`ï¼ˆç”± Notebook è‡ªåŠ¨å¯¼å‡ºï¼‰
+
+> è¯´æ˜ï¼šAUC åœ¨ 0.79â€“0.80 åŒºé—´çš„è½»å¾®æ³¢åŠ¨å±äºæ­£å¸¸ï¼ˆéšæœºç§å­ã€ç¯å¢ƒã€å¹¶è¡Œçº¿ç¨‹ç­‰ä¼šæœ‰ç»†å¾®å½±å“ï¼‰ã€‚
+
+## äº”ã€è¯¯å·®åˆ†æï¼ˆè¦ç‚¹ï¼‰
+- æ—©æœŸçŸ­æ—¶æ®µæ•°æ®å¯¹â€œé•¿æœŸç•™å­˜â€å­˜åœ¨å¤©ç„¶ä¸ç¡®å®šæ€§ï¼Œæ¨¡å‹å¯¹çŸ­é¢‘å¿«ç”¨æˆ·æ›´æ•æ„Ÿï¼›
+- å…³å¡ç‰¹å¾ä¸ç”¨æˆ·è¡Œä¸ºçš„äº¤äº’å°šæœªå……åˆ†å±•å¼€ï¼ˆä¾‹å¦‚åˆ†æ®µâ€œå¡å…³â€è¡Œä¸ºã€æœ€è¿‘è¡Œä¸ºè¡°å‡ï¼‰ï¼›
+- éƒ¨åˆ†ç”¨æˆ·æ•°æ®è¾ƒç¨€ç–ï¼Œèšåˆç‰¹å¾ä¸ç¨³å®šï¼Œå¯é€šè¿‡ç›®æ ‡ç¼–ç /è´å¶æ–¯å¹³æ»‘ç¼“è§£ã€‚
+
+## å…­ã€ä¸šåŠ¡è½åœ°å»ºè®®
+- ä»¥æ¦‚ç‡åˆ†å±‚ï¼šTop 10/20/30% é«˜é£é™©ç”¨æˆ·è¿›è¡Œä¸åŒåŠ›åº¦å¹²é¢„ï¼ˆå¥–åŠ±ã€çŸ­ä¿¡ã€Pushã€å®¢æœå›è®¿ï¼‰ï¼›
+- A/B éªŒè¯ï¼šä¸åŒå¹²é¢„ç­–ç•¥çš„ **å¢é‡ç•™å­˜ç‡** ä¸ **å•ä½æˆæœ¬**ï¼›
+- åœ¨çº¿ç›‘æ§ï¼šAUC ä¸ PSIï¼ˆç‰¹å¾åˆ†å¸ƒæ¼‚ç§»ï¼‰é¢„è­¦ï¼Œç‰ˆæœ¬æ›´æ–°æˆ–èŠ‚å‡æ—¥å‰åé‡ç‚¹å…³æ³¨ã€‚
+
+## ä¸ƒã€æ”¹è¿›æ–¹å‘ï¼ˆå¯å†²å‡» +0.5~1 ä¸ªç™¾åˆ†ç‚¹ AUCï¼‰
+1. **åºåˆ—ç‰¹å¾**ï¼šæœ€è¿‘ N å±€åŠ æƒã€æœ€é•¿å¤±è´¥è¿ä¸²ã€å¡å…³æ—¶é•¿/è§£é”é€Ÿåº¦ã€æ—¥/æ—¶æ®µèŠ‚å¾‹ï¼›
+2. **æ›´å¤šåŸºæ¨¡å‹**ï¼šLightGBM / CatBoost çº³å…¥å †å ï¼›
+3. **é˜ˆå€¼ä¸æ ¡å‡†**ï¼šPlatt / Isotonic æ ¡å‡†åæŒ‰ä¸šåŠ¡ç›®æ ‡ä¼˜åŒ–é˜ˆå€¼ï¼›
+4. **æ›´ç»†è‡´çš„å†·å¯åŠ¨å¤„ç†**ï¼šå®‰è£…æ¥æºã€è®¾å¤‡ã€æ¸ é“ç‰¹å¾ï¼ˆè‹¥å¯ç”¨ï¼‰ï¼›
+5. **K æŠ˜æ•°ä¸Šè°ƒ + å¤šæ¬¡é‡å¯**ï¼šé™ä½æ–¹å·®ï¼Œæäº¤é›†æˆå¹³å‡ã€‚
+
+## å…«ã€å¯å¤ç°å®éªŒæ–‡ä»¶
+- Notebookï¼š`final_project_notebook.ipynb`ï¼ˆåŒ…å«å®Œæ•´è®­ç»ƒ/è¯„ä¼°/æäº¤å¯¼å‡ºï¼‰
+- æäº¤ï¼š`submission.csv`
+- æŠ¥å‘Šï¼š`report.md`
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/results.md OWG-main/results.md
--- OWG-upstream/results.md	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/results.md	2025-11-25 14:43:12.023536217 +0800
@@ -0,0 +1,81 @@
+Preliminary Results.
+We evaluated our geometry-based LGGSN grasp ranker in the OWG language-conditioned grasping pipeline by comparing it against the default OWG grasp selection strategy (baseline). On a total of 485 natural-language-guided manipulation trials, the baseline achieved a grasp success rate of 58% (133/231), whereas enabling LGGSN ranking improved the overall success rate to 65% (166/254).
+
+We further analyzed performance per object category. For the Campbellâ€™s soup can, a relatively challenging cylindrical object, the success rate increased from 40% (36/90) to 52% (50/96) with LGGSN. For scissors, which are thin and orientation-sensitive, LGGSN improved the success rate from 64% (39/61) to 77% (43/56). On the simpler hammer object, performance remained stable at 72% (58/80 vs. 73/102), suggesting that LGGSN does not significantly harm already confident grasps while providing tangible benefits for more geometrically challenging objects.
+
+These preliminary results indicate that a 6-DoF simulation-trained geometry scorer can be effectively integrated into an open-vocabulary grasping system, yielding consistent gains in overall success rate and notably improving performance on hard cases such as cluttered cans and thin tools.
+
+[INFO] Found 95 semantic_pc samples in semantic_pc
+
+[STATS] Points per semantic point cloud:
+  N samples   : 95
+  min points  : 492
+  mean points : 1457.0315789473684
+  max points  : 3513
+
+[STATS] Per-object sample counts:
+  tennis ball    : 13
+  pringles can   : 6
+  mustard bottle : 5
+  rectangular tin: 5
+  hammer         : 6
+  blue container : 3
+  green clamp    : 3
+  eraser         : 8
+  scissors       : 4
+  soup can       : 8
+  cheez-it box   : 1
+  other          : 33
+
+[EXAMPLES]
+  1. ground_1764042064.319_semantic_pc.npz
+     query = tennis
+     pc.shape = (583, 3)
+     meta keys = ['sid', 'label_index']
+  2. ground_1764043633.842_semantic_pc.npz
+     query = can of pringles
+     pc.shape = (3030, 3)
+     meta keys = ['sid', 'label_index']
+  3. ground_1764043668.195_semantic_pc.npz
+     query = mustard bottle
+     pc.shape = (2575, 3)
+     meta keys = ['sid', 'label_index']
+  4. ground_1764043683.760_semantic_pc.npz
+     query = mustard bottle
+     pc.shape = (2438, 3)
+     meta keys = ['sid', 'label_index']
+  5. ground_1764043860.873_semantic_pc.npz
+     query = rectangular tin
+     pc.shape = (1139, 3)
+     meta keys = ['sid', 'label_index']
+
+
+=== Experiment: GeomOnly (use_sem=False, use_text=False) ===
+Epoch 00 | train loss=0.3762 acc=0.91 | val loss=0.2446 acc=0.93
+Epoch 01 | train loss=0.2019 acc=0.93 | val loss=0.1954 acc=0.93
+Epoch 02 | train loss=0.1600 acc=0.94 | val loss=0.1670 acc=0.94
+Epoch 03 | train loss=0.1462 acc=0.95 | val loss=0.1524 acc=0.94
+Epoch 04 | train loss=0.1336 acc=0.95 | val loss=0.1550 acc=0.95
+[GeomOnly] best val acc = 0.949
+
+=== Experiment: Geom+Semantic (use_sem=True, use_text=False) ===
+Epoch 00 | train loss=0.3625 acc=0.91 | val loss=0.2529 acc=0.92
+Epoch 01 | train loss=0.2318 acc=0.93 | val loss=0.2190 acc=0.93
+Epoch 02 | train loss=0.1828 acc=0.94 | val loss=0.1926 acc=0.94
+Epoch 03 | train loss=0.1533 acc=0.95 | val loss=0.1602 acc=0.94
+Epoch 04 | train loss=0.1481 acc=0.95 | val loss=0.1493 acc=0.95
+[Geom+Semantic] best val acc = 0.951
+
+=== Experiment: Geom+Semantic+Text (use_sem=True, use_text=True) ===
+Epoch 00 | train loss=0.4639 acc=0.81 | val loss=0.2878 acc=0.93
+Epoch 01 | train loss=0.2701 acc=0.93 | val loss=0.2714 acc=0.92
+Epoch 02 | train loss=0.2542 acc=0.93 | val loss=0.2599 acc=0.93
+Epoch 03 | train loss=0.2439 acc=0.93 | val loss=0.2572 acc=0.92
+Epoch 04 | train loss=0.2341 acc=0.93 | val loss=0.2375 acc=0.92
+[Geom+Semantic+Text] best val acc = 0.930
+
+=== Summary (best val acc) ===
+GeomOnly          : 0.949
+Geom+Semantic     : 0.951
+Geom+Sem+Text     : 0.930
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/cleanup.sh OWG-main/robotic-grasping-master/cleanup.sh
--- OWG-upstream/robotic-grasping-master/cleanup.sh	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/cleanup.sh	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,3 @@
+#!/usr/bin/env bash
+
+find logs/ -maxdepth 1 -type d | grep -v ^\\.$ | xargs -n 1 du -s | while read size name ; do if [ $size -le 10485 ] ; then echo rm -rf $name ; fi done
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/_config.yml OWG-main/robotic-grasping-master/_config.yml
--- OWG-upstream/robotic-grasping-master/_config.yml	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/_config.yml	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1 @@
+theme: jekyll-theme-cayman
\ No newline at end of file
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/evaluate.py OWG-main/robotic-grasping-master/evaluate.py
--- OWG-upstream/robotic-grasping-master/evaluate.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/evaluate.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,172 @@
+import argparse
+import logging
+import time
+
+import numpy as np
+import torch.utils.data
+
+from hardware.device import get_device
+from inference.post_process import post_process_output
+from utils.data import get_dataset
+from utils.dataset_processing import evaluation, grasp
+from utils.visualisation.plot import save_results
+
+logging.basicConfig(level=logging.INFO)
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Evaluate networks')
+
+    # Network
+    parser.add_argument('--network', metavar='N', type=str, nargs='+',
+                        help='Path to saved networks to evaluate')
+    parser.add_argument('--input-size', type=int, default=224,
+                        help='Input image size for the network')
+
+    # Dataset
+    parser.add_argument('--dataset', type=str,
+                        help='Dataset Name ("cornell" or "jaquard")')
+    parser.add_argument('--dataset-path', type=str,
+                        help='Path to dataset')
+    parser.add_argument('--use-depth', type=int, default=1,
+                        help='Use Depth image for evaluation (1/0)')
+    parser.add_argument('--use-rgb', type=int, default=1,
+                        help='Use RGB image for evaluation (1/0)')
+    parser.add_argument('--augment', action='store_true',
+                        help='Whether data augmentation should be applied')
+    parser.add_argument('--split', type=float, default=0.9,
+                        help='Fraction of data for training (remainder is validation)')
+    parser.add_argument('--ds-shuffle', action='store_true', default=False,
+                        help='Shuffle the dataset')
+    parser.add_argument('--ds-rotate', type=float, default=0.0,
+                        help='Shift the start point of the dataset to use a different test/train split')
+    parser.add_argument('--num-workers', type=int, default=8,
+                        help='Dataset workers')
+
+    # Evaluation
+    parser.add_argument('--n-grasps', type=int, default=1,
+                        help='Number of grasps to consider per image')
+    parser.add_argument('--iou-threshold', type=float, default=0.25,
+                        help='Threshold for IOU matching')
+    parser.add_argument('--iou-eval', action='store_true',
+                        help='Compute success based on IoU metric.')
+    parser.add_argument('--jacquard-output', action='store_true',
+                        help='Jacquard-dataset style output')
+
+    # Misc.
+    parser.add_argument('--vis', action='store_true',
+                        help='Visualise the network output')
+    parser.add_argument('--cpu', dest='force_cpu', action='store_true', default=False,
+                        help='Force code to run in CPU mode')
+    parser.add_argument('--random-seed', type=int, default=123,
+                        help='Random seed for numpy')
+
+    args = parser.parse_args()
+
+    if args.jacquard_output and args.dataset != 'jacquard':
+        raise ValueError('--jacquard-output can only be used with the --dataset jacquard option.')
+    if args.jacquard_output and args.augment:
+        raise ValueError('--jacquard-output can not be used with data augmentation.')
+
+    return args
+
+
+if __name__ == '__main__':
+    args = parse_args()
+
+    # Get the compute device
+    device = get_device(args.force_cpu)
+
+    # Load Dataset
+    logging.info('Loading {} Dataset...'.format(args.dataset.title()))
+    Dataset = get_dataset(args.dataset)
+    test_dataset = Dataset(args.dataset_path,
+                           output_size=args.input_size,
+                           ds_rotate=args.ds_rotate,
+                           random_rotate=args.augment,
+                           random_zoom=args.augment,
+                           include_depth=args.use_depth,
+                           include_rgb=args.use_rgb)
+
+    indices = list(range(test_dataset.length))
+    split = int(np.floor(args.split * test_dataset.length))
+    if args.ds_shuffle:
+        np.random.seed(args.random_seed)
+        np.random.shuffle(indices)
+    val_indices = indices[split:]
+    val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)
+    logging.info('Validation size: {}'.format(len(val_indices)))
+
+    test_data = torch.utils.data.DataLoader(
+        test_dataset,
+        batch_size=1,
+        num_workers=args.num_workers,
+        sampler=val_sampler
+    )
+    logging.info('Done')
+
+    for network in args.network:
+        logging.info('\nEvaluating model {}'.format(network))
+
+        # Load Network
+        net = torch.load(network)
+
+        results = {'correct': 0, 'failed': 0}
+
+        if args.jacquard_output:
+            jo_fn = network + '_jacquard_output.txt'
+            with open(jo_fn, 'w') as f:
+                pass
+
+        start_time = time.time()
+
+        with torch.no_grad():
+            for idx, (x, y, didx, rot, zoom) in enumerate(test_data):
+                xc = x.to(device)
+                yc = [yi.to(device) for yi in y]
+                lossd = net.compute_loss(xc, yc)
+
+                q_img, ang_img, width_img = post_process_output(lossd['pred']['pos'], lossd['pred']['cos'],
+                                                                lossd['pred']['sin'], lossd['pred']['width'])
+
+                if args.iou_eval:
+                    s = evaluation.calculate_iou_match(q_img, ang_img, test_data.dataset.get_gtbb(didx, rot, zoom),
+                                                       no_grasps=args.n_grasps,
+                                                       grasp_width=width_img,
+                                                       threshold=args.iou_threshold
+                                                       )
+                    if s:
+                        results['correct'] += 1
+                    else:
+                        results['failed'] += 1
+
+                if args.jacquard_output:
+                    grasps = grasp.detect_grasps(q_img, ang_img, width_img=width_img, no_grasps=1)
+                    with open(jo_fn, 'a') as f:
+                        for g in grasps:
+                            f.write(test_data.dataset.get_jname(didx) + '\n')
+                            f.write(g.to_jacquard(scale=1024 / 300) + '\n')
+
+                if args.vis:
+                    save_results(
+                        rgb_img=test_data.dataset.get_rgb(didx, rot, zoom, normalise=False),
+                        depth_img=test_data.dataset.get_depth(didx, rot, zoom),
+                        grasp_q_img=q_img,
+                        grasp_angle_img=ang_img,
+                        no_grasps=args.n_grasps,
+                        grasp_width_img=width_img
+                    )
+
+        avg_time = (time.time() - start_time) / len(test_data)
+        logging.info('Average evaluation time per image: {}ms'.format(avg_time * 1000))
+
+        if args.iou_eval:
+            logging.info('IOU Results: %d/%d = %f' % (results['correct'],
+                                                      results['correct'] + results['failed'],
+                                                      results['correct'] / (results['correct'] + results['failed'])))
+
+        if args.jacquard_output:
+            logging.info('Jacquard output saved to {}'.format(jo_fn))
+
+        del net
+        torch.cuda.empty_cache()
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/.gitattributes OWG-main/robotic-grasping-master/.gitattributes
--- OWG-upstream/robotic-grasping-master/.gitattributes	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/.gitattributes	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,6 @@
+.92 filter=lfs diff=lfs merge=lfs -text
+.93 filter=lfs diff=lfs merge=lfs -text
+.94 filter=lfs diff=lfs merge=lfs -text
+.96 filter=lfs diff=lfs merge=lfs -text
+.97 filter=lfs diff=lfs merge=lfs -text
+.98 filter=lfs diff=lfs merge=lfs -text
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/.gitignore OWG-main/robotic-grasping-master/.gitignore
--- OWG-upstream/robotic-grasping-master/.gitignore	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/.gitignore	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,111 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# pyenv
+.python-version
+
+# celery beat schedule file
+celerybeat-schedule
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+env/
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+/docker/
+
+results/
+
+saved_data/
+
+*.pyc
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/hardware/calibrate_camera.py OWG-main/robotic-grasping-master/hardware/calibrate_camera.py
--- OWG-upstream/robotic-grasping-master/hardware/calibrate_camera.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/hardware/calibrate_camera.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,185 @@
+import logging
+import os
+import time
+
+import cv2
+import matplotlib.pyplot as plt
+import numpy as np
+from scipy import optimize
+# This import registers the 3D projection, but is otherwise unused.
+from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
+
+from hardware.camera import RealSenseCamera
+
+
+class Calibration:
+    def __init__(self,
+                 cam_id,
+                 calib_grid_step,
+                 checkerboard_offset_from_tool,
+                 workspace_limits
+                 ):
+        self.calib_grid_step = calib_grid_step
+        self.checkerboard_offset_from_tool = checkerboard_offset_from_tool
+
+        # Cols: min max, Rows: x y z (define workspace limits in robot coordinates)
+        self.workspace_limits = workspace_limits
+
+        self.camera = RealSenseCamera(device_id=cam_id)
+        
+        self.measured_pts = []
+        self.observed_pts = []
+        self.observed_pix = []
+        self.camera2world = np.eye(4)
+
+        homedir = os.path.join(os.path.expanduser('~'), "grasp-comms")
+        self.move_completed = os.path.join(homedir, "move_completed.npy")
+        self.tool_position = os.path.join(homedir, "tool_position.npy")
+
+    @staticmethod
+    def _get_rigid_transform(A, B):
+        """
+        Estimate rigid transform with SVD (from Nghia Ho)
+        """
+        assert len(A) == len(B)
+
+        N = A.shape[0]  # Total points
+        centroid_A = np.mean(A, axis=0)
+        centroid_B = np.mean(B, axis=0)
+        AA = A - np.tile(centroid_A, (N, 1))  # Centre the points
+        BB = B - np.tile(centroid_B, (N, 1))
+        H = np.dot(np.transpose(AA), BB)  # Dot is matrix multiplication for array
+        U, S, Vt = np.linalg.svd(H)
+        R = np.dot(Vt.T, U.T)
+        if np.linalg.det(R) < 0:  # Special reflection case
+            Vt[2, :] *= -1
+            R = np.dot(Vt.T, U.T)
+        t = np.dot(-R, centroid_A.T) + centroid_B.T
+        return R, t
+
+    def _get_rigid_transform_error(self, z_scale):
+        """
+        Calculate the rigid transform RMS error
+
+        :return RMS error
+        """
+        # Apply z offset and compute new observed points using camera intrinsics
+        observed_z = np.squeeze(self.observed_pts[:, 2:] * z_scale)
+        observed_x = np.multiply(np.squeeze(self.observed_pix[:, [0]]) - self.camera.intrinsics.ppx,
+                                 observed_z / self.camera.intrinsics.fx)
+        observed_y = np.multiply(np.squeeze(self.observed_pix[:, [1]]) - self.camera.intrinsics.ppy,
+                                 observed_z / self.camera.intrinsics.fy)
+
+        new_observed_pts = np.asarray([observed_x, observed_y, observed_z]).T
+
+        # Estimate rigid transform between new observed points and measured points
+        R, t = self._get_rigid_transform(np.asarray(new_observed_pts), np.asarray(self.measured_pts))
+        t.shape = (3, 1)
+        self.camera2world = np.concatenate((np.concatenate((R, t), axis=1), np.array([[0, 0, 0, 1]])), axis=0)
+
+        # Compute rigid transform error
+        registered_pts = np.dot(R, np.transpose(new_observed_pts)) + np.tile(t, (1, new_observed_pts.shape[0]))
+        error = np.transpose(registered_pts) - self.measured_pts
+        error = np.sum(np.multiply(error, error))
+        rmse = np.sqrt(error / new_observed_pts.shape[0])
+        return rmse
+
+    def _generate_grid(self):
+        """
+        Construct 3D calibration grid across workspace
+
+        :return calibration grid points
+        """
+        gridspace_x = np.linspace(self.workspace_limits[0][0], self.workspace_limits[0][1],
+                                  1 + (self.workspace_limits[0][1] - self.workspace_limits[0][
+                                      0]) / self.calib_grid_step)
+        gridspace_y = np.linspace(self.workspace_limits[1][0], self.workspace_limits[1][1],
+                                  1 + (self.workspace_limits[1][1] - self.workspace_limits[1][
+                                      0]) / self.calib_grid_step)
+        gridspace_z = np.linspace(self.workspace_limits[2][0], self.workspace_limits[2][1],
+                                  1 + (self.workspace_limits[2][1] - self.workspace_limits[2][
+                                      0]) / self.calib_grid_step)
+        calib_grid_x, calib_grid_y, calib_grid_z = np.meshgrid(gridspace_x, gridspace_y, gridspace_z)
+        num_calib_grid_pts = calib_grid_x.shape[0] * calib_grid_x.shape[1] * calib_grid_x.shape[2]
+        calib_grid_x.shape = (num_calib_grid_pts, 1)
+        calib_grid_y.shape = (num_calib_grid_pts, 1)
+        calib_grid_z.shape = (num_calib_grid_pts, 1)
+        calib_grid_pts = np.concatenate((calib_grid_x, calib_grid_y, calib_grid_z), axis=1)
+        return calib_grid_pts
+        
+    def run(self):
+        # Connect to camera
+        self.camera.connect()
+        logging.debug(self.camera.intrinsics)
+
+        logging.info('Collecting data...')
+
+        calib_grid_pts = self._generate_grid()
+
+        logging.info('Total grid points: ', calib_grid_pts.shape[0])
+
+        for tool_position in calib_grid_pts:
+            logging.info('Requesting move to tool position: ', tool_position)
+            np.save(self.tool_position, tool_position)
+            np.save(self.move_completed, 0)
+            while not np.load(self.move_completed):
+                time.sleep(0.1)
+            # Wait for robot to be stable
+            time.sleep(2)
+
+            # Find checkerboard center
+            checkerboard_size = (3, 3)
+            refine_criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
+            image_bundle = self.camera.get_image_bundle()
+            camera_color_img = image_bundle['rgb']
+            camera_depth_img = image_bundle['aligned_depth']
+            bgr_color_data = cv2.cvtColor(camera_color_img, cv2.COLOR_RGB2BGR)
+            gray_data = cv2.cvtColor(bgr_color_data, cv2.COLOR_RGB2GRAY)
+            checkerboard_found, corners = cv2.findChessboardCorners(gray_data, checkerboard_size, None,
+                                                                    cv2.CALIB_CB_ADAPTIVE_THRESH)
+            if checkerboard_found:
+                corners_refined = cv2.cornerSubPix(gray_data, corners, (3, 3), (-1, -1), refine_criteria)
+
+                # Get observed checkerboard center 3D point in camera space
+                checkerboard_pix = np.round(corners_refined[4, 0, :]).astype(int)
+                checkerboard_z = camera_depth_img[checkerboard_pix[1]][checkerboard_pix[0]]
+                checkerboard_x = np.multiply(checkerboard_pix[0] - self.camera.intrinsics.ppx,
+                                             checkerboard_z / self.camera.intrinsics.fx)
+                checkerboard_y = np.multiply(checkerboard_pix[1] - self.camera.intrinsics.ppy,
+                                             checkerboard_z / self.camera.intrinsics.fy)
+                if checkerboard_z == 0:
+                    continue
+
+                # Save calibration point and observed checkerboard center
+                self.observed_pts.append([checkerboard_x, checkerboard_y, checkerboard_z])
+                # tool_position[2] += self.checkerboard_offset_from_tool
+                tool_position = tool_position + self.checkerboard_offset_from_tool
+
+                self.measured_pts.append(tool_position)
+                self.observed_pix.append(checkerboard_pix)
+
+                # Draw and display the corners
+                vis = cv2.drawChessboardCorners(bgr_color_data, (1, 1), corners_refined[4, :, :], checkerboard_found)
+                # cv2.imwrite('%06d.png' % len(self.measured_pts), vis)
+                cv2.imshow('Calibration', vis)
+                cv2.waitKey(10)
+            else:
+                logging.info('Checker board not found')
+
+        self.measured_pts = np.asarray(self.measured_pts)
+        self.observed_pts = np.asarray(self.observed_pts)
+        self.observed_pix = np.asarray(self.observed_pix)
+
+        # Optimize z scale w.r.t. rigid transform error
+        logging.info('Calibrating...')
+        z_scale_init = 1
+        optim_result = optimize.minimize(self._get_rigid_transform_error, np.asarray(z_scale_init), method='Nelder-Mead')
+        camera_depth_offset = optim_result.x
+
+        # Save camera optimized offset and camera pose
+        logging.info('Saving...')
+        np.savetxt('saved_data/camera_depth_scale.txt', camera_depth_offset, delimiter=' ')
+        rmse = self._get_rigid_transform_error(camera_depth_offset)
+        logging.info('RMSE: ', rmse)
+        np.savetxt('saved_data/camera_pose.txt', self.camera2world, delimiter=' ')
+        logging.info('Done.')
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/hardware/camera.py OWG-main/robotic-grasping-master/hardware/camera.py
--- OWG-upstream/robotic-grasping-master/hardware/camera.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/hardware/camera.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,80 @@
+import logging
+
+import matplotlib.pyplot as plt
+import numpy as np
+import pyrealsense2 as rs
+
+logger = logging.getLogger(__name__)
+
+
+class RealSenseCamera:
+    def __init__(self,
+                 device_id,
+                 width=640,
+                 height=480,
+                 fps=6):
+        self.device_id = device_id
+        self.width = width
+        self.height = height
+        self.fps = fps
+
+        self.pipeline = None
+        self.scale = None
+        self.intrinsics = None
+
+    def connect(self):
+        # Start and configure
+        self.pipeline = rs.pipeline()
+        config = rs.config()
+        config.enable_device(str(self.device_id))
+        config.enable_stream(rs.stream.depth, self.width, self.height, rs.format.z16, self.fps)
+        config.enable_stream(rs.stream.color, self.width, self.height, rs.format.rgb8, self.fps)
+        cfg = self.pipeline.start(config)
+
+        # Determine intrinsics
+        rgb_profile = cfg.get_stream(rs.stream.color)
+        self.intrinsics = rgb_profile.as_video_stream_profile().get_intrinsics()
+
+        # Determine depth scale
+        self.scale = cfg.get_device().first_depth_sensor().get_depth_scale()
+
+    def get_image_bundle(self):
+        frames = self.pipeline.wait_for_frames()
+
+        align = rs.align(rs.stream.color)
+        aligned_frames = align.process(frames)
+        color_frame = aligned_frames.first(rs.stream.color)
+        aligned_depth_frame = aligned_frames.get_depth_frame()
+
+        depth_image = np.asarray(aligned_depth_frame.get_data(), dtype=np.float32)
+        depth_image *= self.scale
+        color_image = np.asanyarray(color_frame.get_data())
+
+        depth_image = np.expand_dims(depth_image, axis=2)
+
+        return {
+            'rgb': color_image,
+            'aligned_depth': depth_image,
+        }
+
+    def plot_image_bundle(self):
+        images = self.get_image_bundle()
+
+        rgb = images['rgb']
+        depth = images['aligned_depth']
+
+        fig, ax = plt.subplots(1, 2, squeeze=False)
+        ax[0, 0].imshow(rgb)
+        m, s = np.nanmean(depth), np.nanstd(depth)
+        ax[0, 1].imshow(depth.squeeze(axis=2), vmin=m - s, vmax=m + s, cmap=plt.cm.gray)
+        ax[0, 0].set_title('rgb')
+        ax[0, 1].set_title('aligned_depth')
+
+        plt.show()
+
+
+if __name__ == '__main__':
+    cam = RealSenseCamera(device_id=830112070066)
+    cam.connect()
+    while True:
+        cam.plot_image_bundle()
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/hardware/device.py OWG-main/robotic-grasping-master/hardware/device.py
--- OWG-upstream/robotic-grasping-master/hardware/device.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/hardware/device.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,19 @@
+import logging
+
+import torch
+
+logging.basicConfig(level=logging.INFO)
+
+
+def get_device(force_cpu):
+    # Check if CUDA can be used
+    if torch.cuda.is_available() and not force_cpu:
+        logging.info("CUDA detected. Running with GPU acceleration.")
+        device = torch.device("cuda")
+    elif force_cpu:
+        logging.info("CUDA detected, but overriding with option '--cpu'. Running with only CPU.")
+        device = torch.device("cpu")
+    else:
+        logging.info("CUDA is *NOT* detected. Running with only CPU.")
+        device = torch.device("cpu")
+    return device
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/grasp_generator.py OWG-main/robotic-grasping-master/inference/grasp_generator.py
--- OWG-upstream/robotic-grasping-master/inference/grasp_generator.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/grasp_generator.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,106 @@
+import os
+import time
+
+import matplotlib.pyplot as plt
+import numpy as np
+import torch
+
+from hardware.camera import RealSenseCamera
+from hardware.device import get_device
+from inference.post_process import post_process_output
+from utils.data.camera_data import CameraData
+from utils.dataset_processing.grasp import detect_grasps
+from utils.visualisation.plot import plot_grasp
+
+
+class GraspGenerator:
+    def __init__(self, saved_model_path, cam_id, visualize=False):
+        self.saved_model_path = saved_model_path
+        self.camera = RealSenseCamera(device_id=cam_id)
+
+        self.saved_model_path = saved_model_path
+        self.model = None
+        self.device = None
+
+        self.cam_data = CameraData(include_depth=True, include_rgb=True)
+
+        # Connect to camera
+        self.camera.connect()
+
+        # Load camera pose and depth scale (from running calibration)
+        self.cam_pose = np.loadtxt('saved_data/camera_pose.txt', delimiter=' ')
+        self.cam_depth_scale = np.loadtxt('saved_data/camera_depth_scale.txt', delimiter=' ')
+
+        homedir = os.path.join(os.path.expanduser('~'), "grasp-comms")
+        self.grasp_request = os.path.join(homedir, "grasp_request.npy")
+        self.grasp_available = os.path.join(homedir, "grasp_available.npy")
+        self.grasp_pose = os.path.join(homedir, "grasp_pose.npy")
+
+        if visualize:
+            self.fig = plt.figure(figsize=(10, 10))
+        else:
+            self.fig = None
+
+    def load_model(self):
+        print('Loading model... ')
+        self.model = torch.load(self.saved_model_path)
+        # Get the compute device
+        self.device = get_device(force_cpu=False)
+
+    def generate(self):
+        # Get RGB-D image from camera
+        image_bundle = self.camera.get_image_bundle()
+        rgb = image_bundle['rgb']
+        depth = image_bundle['aligned_depth']
+        x, depth_img, rgb_img = self.cam_data.get_data(rgb=rgb, depth=depth)
+
+        # Predict the grasp pose using the saved model
+        with torch.no_grad():
+            xc = x.to(self.device)
+            pred = self.model.predict(xc)
+
+        q_img, ang_img, width_img = post_process_output(pred['pos'], pred['cos'], pred['sin'], pred['width'])
+        grasps = detect_grasps(q_img, ang_img, width_img)
+
+        # Get grasp position from model output
+        pos_z = depth[grasps[0].center[0] + self.cam_data.top_left[0], grasps[0].center[1] + self.cam_data.top_left[1]] * self.cam_depth_scale - 0.04
+        pos_x = np.multiply(grasps[0].center[1] + self.cam_data.top_left[1] - self.camera.intrinsics.ppx,
+                            pos_z / self.camera.intrinsics.fx)
+        pos_y = np.multiply(grasps[0].center[0] + self.cam_data.top_left[0] - self.camera.intrinsics.ppy,
+                            pos_z / self.camera.intrinsics.fy)
+
+        if pos_z == 0:
+            return
+
+        target = np.asarray([pos_x, pos_y, pos_z])
+        target.shape = (3, 1)
+        print('target: ', target)
+
+        # Convert camera to robot coordinates
+        camera2robot = self.cam_pose
+        target_position = np.dot(camera2robot[0:3, 0:3], target) + camera2robot[0:3, 3:]
+        target_position = target_position[0:3, 0]
+
+        # Convert camera to robot angle
+        angle = np.asarray([0, 0, grasps[0].angle])
+        angle.shape = (3, 1)
+        target_angle = np.dot(camera2robot[0:3, 0:3], angle)
+
+        # Concatenate grasp pose with grasp angle
+        grasp_pose = np.append(target_position, target_angle[2])
+
+        print('grasp_pose: ', grasp_pose)
+
+        np.save(self.grasp_pose, grasp_pose)
+
+        if self.fig:
+            plot_grasp(fig=self.fig, rgb_img=self.cam_data.get_rgb(rgb, False), grasps=grasps, save=True)
+
+    def run(self):
+        while True:
+            if np.load(self.grasp_request):
+                self.generate()
+                np.save(self.grasp_request, 0)
+                np.save(self.grasp_available, 1)
+            else:
+                time.sleep(0.1)
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/models/grasp_model.py OWG-main/robotic-grasping-master/inference/models/grasp_model.py
--- OWG-upstream/robotic-grasping-master/inference/models/grasp_model.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/models/grasp_model.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,67 @@
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class GraspModel(nn.Module):
+    """
+    An abstract model for grasp network in a common format.
+    """
+
+    def __init__(self):
+        super(GraspModel, self).__init__()
+
+    def forward(self, x_in):
+        raise NotImplementedError()
+
+    def compute_loss(self, xc, yc):
+        y_pos, y_cos, y_sin, y_width = yc
+        pos_pred, cos_pred, sin_pred, width_pred = self(xc)
+
+        p_loss = F.smooth_l1_loss(pos_pred, y_pos)
+        cos_loss = F.smooth_l1_loss(cos_pred, y_cos)
+        sin_loss = F.smooth_l1_loss(sin_pred, y_sin)
+        width_loss = F.smooth_l1_loss(width_pred, y_width)
+
+        return {
+            'loss': p_loss + cos_loss + sin_loss + width_loss,
+            'losses': {
+                'p_loss': p_loss,
+                'cos_loss': cos_loss,
+                'sin_loss': sin_loss,
+                'width_loss': width_loss
+            },
+            'pred': {
+                'pos': pos_pred,
+                'cos': cos_pred,
+                'sin': sin_pred,
+                'width': width_pred
+            }
+        }
+
+    def predict(self, xc):
+        pos_pred, cos_pred, sin_pred, width_pred = self(xc)
+        return {
+            'pos': pos_pred,
+            'cos': cos_pred,
+            'sin': sin_pred,
+            'width': width_pred
+        }
+
+
+class ResidualBlock(nn.Module):
+    """
+    A residual block with dropout option
+    """
+
+    def __init__(self, in_channels, out_channels, kernel_size=3):
+        super(ResidualBlock, self).__init__()
+        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)
+        self.bn1 = nn.BatchNorm2d(in_channels)
+        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)
+        self.bn2 = nn.BatchNorm2d(in_channels)
+
+    def forward(self, x_in):
+        x = self.bn1(self.conv1(x_in))
+        x = F.relu(x)
+        x = self.bn2(self.conv2(x))
+        return x + x_in
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/models/grconvnet2.py OWG-main/robotic-grasping-master/inference/models/grconvnet2.py
--- OWG-upstream/robotic-grasping-master/inference/models/grconvnet2.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/models/grconvnet2.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,75 @@
+import torch.nn as nn
+import torch.nn.functional as F
+
+from inference.models.grasp_model import GraspModel, ResidualBlock
+
+
+class GenerativeResnet(GraspModel):
+
+    def __init__(self, input_channels=4, output_channels=1, channel_size=32, dropout=False, prob=0.0):
+        super(GenerativeResnet, self).__init__()
+        self.conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)
+        self.bn1 = nn.BatchNorm2d(channel_size)
+
+        self.conv2 = nn.Conv2d(channel_size, channel_size * 2, kernel_size=4, stride=2, padding=1)
+        self.bn2 = nn.BatchNorm2d(channel_size * 2)
+
+        self.conv3 = nn.Conv2d(channel_size * 2, channel_size * 4, kernel_size=4, stride=2, padding=1)
+        self.bn3 = nn.BatchNorm2d(channel_size * 4)
+
+        self.res1 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res2 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res3 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res4 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res5 = ResidualBlock(channel_size * 4, channel_size * 4)
+
+        self.conv4 = nn.ConvTranspose2d(channel_size * 4, channel_size * 2, kernel_size=4, stride=2, padding=1,
+                                        output_padding=1)
+        self.bn4 = nn.BatchNorm2d(channel_size * 2)
+
+        self.conv5 = nn.ConvTranspose2d(channel_size * 2, channel_size, kernel_size=4, stride=2, padding=2,
+                                        output_padding=1)
+        self.bn5 = nn.BatchNorm2d(channel_size)
+
+        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)
+
+        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.cos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.sin_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+
+        self.dropout = dropout
+        self.dropout_pos = nn.Dropout(p=prob)
+        self.dropout_cos = nn.Dropout(p=prob)
+        self.dropout_sin = nn.Dropout(p=prob)
+        self.dropout_wid = nn.Dropout(p=prob)
+
+        for m in self.modules():
+            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
+                nn.init.xavier_uniform_(m.weight, gain=1)
+
+    def forward(self, x_in):
+        x = F.relu(self.bn1(self.conv1(x_in)))
+        x = F.relu(self.bn2(self.conv2(x)))
+        x = F.relu(self.bn3(self.conv3(x)))
+        x = self.res1(x)
+        x = self.res2(x)
+        x = self.res3(x)
+        x = self.res4(x)
+        x = self.res5(x)
+        x = F.relu(self.bn4(self.conv4(x)))
+        x = F.relu(self.bn5(self.conv5(x)))
+        x = self.conv6(x)
+
+        if self.dropout:
+            pos_output = self.pos_output(self.dropout_pos(x))
+            cos_output = self.cos_output(self.dropout_cos(x))
+            sin_output = self.sin_output(self.dropout_sin(x))
+            width_output = self.width_output(self.dropout_wid(x))
+        else:
+            pos_output = self.pos_output(x)
+            cos_output = self.cos_output(x)
+            sin_output = self.sin_output(x)
+            width_output = self.width_output(x)
+
+        return pos_output, cos_output, sin_output, width_output
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/models/grconvnet3.py OWG-main/robotic-grasping-master/inference/models/grconvnet3.py
--- OWG-upstream/robotic-grasping-master/inference/models/grconvnet3.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/models/grconvnet3.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,75 @@
+import torch.nn as nn
+import torch.nn.functional as F
+
+from inference.models.grasp_model import GraspModel, ResidualBlock
+
+
+class GenerativeResnet(GraspModel):
+
+    def __init__(self, input_channels=4, output_channels=1, channel_size=32, dropout=False, prob=0.0):
+        super(GenerativeResnet, self).__init__()
+        self.conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)
+        self.bn1 = nn.BatchNorm2d(channel_size)
+
+        self.conv2 = nn.Conv2d(channel_size, channel_size * 2, kernel_size=4, stride=2, padding=1)
+        self.bn2 = nn.BatchNorm2d(channel_size * 2)
+
+        self.conv3 = nn.Conv2d(channel_size * 2, channel_size * 4, kernel_size=4, stride=2, padding=1)
+        self.bn3 = nn.BatchNorm2d(channel_size * 4)
+
+        self.res1 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res2 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res3 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res4 = ResidualBlock(channel_size * 4, channel_size * 4)
+        self.res5 = ResidualBlock(channel_size * 4, channel_size * 4)
+
+        self.conv4 = nn.ConvTranspose2d(channel_size * 4, channel_size * 2, kernel_size=4, stride=2, padding=1,
+                                        output_padding=1)
+        self.bn4 = nn.BatchNorm2d(channel_size * 2)
+
+        self.conv5 = nn.ConvTranspose2d(channel_size * 2, channel_size, kernel_size=4, stride=2, padding=2,
+                                        output_padding=1)
+        self.bn5 = nn.BatchNorm2d(channel_size)
+
+        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)
+
+        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.cos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.sin_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+
+        self.dropout = dropout
+        self.dropout_pos = nn.Dropout(p=prob)
+        self.dropout_cos = nn.Dropout(p=prob)
+        self.dropout_sin = nn.Dropout(p=prob)
+        self.dropout_wid = nn.Dropout(p=prob)
+
+        for m in self.modules():
+            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
+                nn.init.xavier_uniform_(m.weight, gain=1)
+
+    def forward(self, x_in):
+        x = F.relu(self.bn1(self.conv1(x_in)))
+        x = F.relu(self.bn2(self.conv2(x)))
+        x = F.relu(self.bn3(self.conv3(x)))
+        x = self.res1(x)
+        x = self.res2(x)
+        x = self.res3(x)
+        x = self.res4(x)
+        x = self.res5(x)
+        x = F.relu(self.bn4(self.conv4(x)))
+        x = F.relu(self.bn5(self.conv5(x)))
+        x = self.conv6(x)
+
+        if self.dropout:
+            pos_output = self.pos_output(self.dropout_pos(x))
+            cos_output = self.cos_output(self.dropout_cos(x))
+            sin_output = self.sin_output(self.dropout_sin(x))
+            width_output = self.width_output(self.dropout_wid(x))
+        else:
+            pos_output = self.pos_output(x)
+            cos_output = self.cos_output(x)
+            sin_output = self.sin_output(x)
+            width_output = self.width_output(x)
+
+        return pos_output, cos_output, sin_output, width_output
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/models/grconvnet4.py OWG-main/robotic-grasping-master/inference/models/grconvnet4.py
--- OWG-upstream/robotic-grasping-master/inference/models/grconvnet4.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/models/grconvnet4.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,75 @@
+import torch.nn as nn
+import torch.nn.functional as F
+
+from inference.models.grasp_model import GraspModel, ResidualBlock
+
+
+class GenerativeResnet(GraspModel):
+
+    def __init__(self, input_channels=4, output_channels=1, channel_size=32, dropout=False, prob=0.0):
+        super(GenerativeResnet, self).__init__()
+        self.conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)
+        self.bn1 = nn.BatchNorm2d(channel_size)
+
+        self.conv2 = nn.Conv2d(channel_size, channel_size // 2, kernel_size=4, stride=2, padding=1)
+        self.bn2 = nn.BatchNorm2d(channel_size // 2)
+
+        self.conv3 = nn.Conv2d(channel_size // 2, channel_size // 4, kernel_size=4, stride=2, padding=1)
+        self.bn3 = nn.BatchNorm2d(channel_size // 4)
+
+        self.res1 = ResidualBlock(channel_size // 4, channel_size // 4)
+        self.res2 = ResidualBlock(channel_size // 4, channel_size // 4)
+        self.res3 = ResidualBlock(channel_size // 4, channel_size // 4)
+        self.res4 = ResidualBlock(channel_size // 4, channel_size // 4)
+        self.res5 = ResidualBlock(channel_size // 4, channel_size // 4)
+
+        self.conv4 = nn.ConvTranspose2d(channel_size // 4, channel_size // 2, kernel_size=4, stride=2, padding=1,
+                                        output_padding=1)
+        self.bn4 = nn.BatchNorm2d(channel_size // 2)
+
+        self.conv5 = nn.ConvTranspose2d(channel_size // 2, channel_size, kernel_size=4, stride=2, padding=2,
+                                        output_padding=1)
+        self.bn5 = nn.BatchNorm2d(channel_size)
+
+        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)
+
+        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.cos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.sin_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=2)
+
+        self.dropout = dropout
+        self.dropout_pos = nn.Dropout(p=prob)
+        self.dropout_cos = nn.Dropout(p=prob)
+        self.dropout_sin = nn.Dropout(p=prob)
+        self.dropout_wid = nn.Dropout(p=prob)
+
+        for m in self.modules():
+            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
+                nn.init.xavier_uniform_(m.weight, gain=1)
+
+    def forward(self, x_in):
+        x = F.relu(self.bn1(self.conv1(x_in)))
+        x = F.relu(self.bn2(self.conv2(x)))
+        x = F.relu(self.bn3(self.conv3(x)))
+        x = self.res1(x)
+        x = self.res2(x)
+        x = self.res3(x)
+        x = self.res4(x)
+        x = self.res5(x)
+        x = F.relu(self.bn4(self.conv4(x)))
+        x = F.relu(self.bn5(self.conv5(x)))
+        x = self.conv6(x)
+
+        if self.dropout:
+            pos_output = self.pos_output(self.dropout_pos(x))
+            cos_output = self.cos_output(self.dropout_cos(x))
+            sin_output = self.sin_output(self.dropout_sin(x))
+            width_output = self.width_output(self.dropout_wid(x))
+        else:
+            pos_output = self.pos_output(x)
+            cos_output = self.cos_output(x)
+            sin_output = self.sin_output(x)
+            width_output = self.width_output(x)
+
+        return pos_output, cos_output, sin_output, width_output
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/models/grconvnet.py OWG-main/robotic-grasping-master/inference/models/grconvnet.py
--- OWG-upstream/robotic-grasping-master/inference/models/grconvnet.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/models/grconvnet.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,63 @@
+import torch.nn as nn
+import torch.nn.functional as F
+
+from inference.models.grasp_model import GraspModel, ResidualBlock
+
+
+class GenerativeResnet(GraspModel):
+
+    def __init__(self, input_channels=1, dropout=False, prob=0.0, channel_size=32):
+        super(GenerativeResnet, self).__init__()
+        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=9, stride=1, padding=4)
+        self.bn1 = nn.BatchNorm2d(32)
+
+        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)
+        self.bn2 = nn.BatchNorm2d(64)
+
+        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
+        self.bn3 = nn.BatchNorm2d(128)
+
+        self.res1 = ResidualBlock(128, 128)
+        self.res2 = ResidualBlock(128, 128)
+        self.res3 = ResidualBlock(128, 128)
+        self.res4 = ResidualBlock(128, 128)
+        self.res5 = ResidualBlock(128, 128)
+
+        self.conv4 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, output_padding=1)
+        self.bn4 = nn.BatchNorm2d(64)
+
+        self.conv5 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=2, output_padding=1)
+        self.bn5 = nn.BatchNorm2d(32)
+
+        self.conv6 = nn.ConvTranspose2d(32, 32, kernel_size=9, stride=1, padding=4)
+
+        self.pos_output = nn.Conv2d(32, 1, kernel_size=2)
+        self.cos_output = nn.Conv2d(32, 1, kernel_size=2)
+        self.sin_output = nn.Conv2d(32, 1, kernel_size=2)
+        self.width_output = nn.Conv2d(32, 1, kernel_size=2)
+
+        self.dropout1 = nn.Dropout(p=prob)
+
+        for m in self.modules():
+            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
+                nn.init.xavier_uniform_(m.weight, gain=1)
+
+    def forward(self, x_in):
+        x = F.relu(self.bn1(self.conv1(x_in)))
+        x = F.relu(self.bn2(self.conv2(x)))
+        x = F.relu(self.bn3(self.conv3(x)))
+        x = self.res1(x)
+        x = self.res2(x)
+        x = self.res3(x)
+        x = self.res4(x)
+        x = self.res5(x)
+        x = F.relu(self.bn4(self.conv4(x)))
+        x = F.relu(self.bn5(self.conv5(x)))
+        x = self.conv6(x)
+
+        pos_output = self.pos_output(self.dropout1(x))
+        cos_output = self.cos_output(self.dropout1(x))
+        sin_output = self.sin_output(self.dropout1(x))
+        width_output = self.width_output(self.dropout1(x))
+
+        return pos_output, cos_output, sin_output, width_output
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/models/__init__.py OWG-main/robotic-grasping-master/inference/models/__init__.py
--- OWG-upstream/robotic-grasping-master/inference/models/__init__.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/models/__init__.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,20 @@
+def get_network(network_name):
+    network_name = network_name.lower()
+    # Original GR-ConvNet
+    if network_name == 'grconvnet':
+        from .grconvnet import GenerativeResnet
+        return GenerativeResnet
+    # Configurable GR-ConvNet with multiple dropouts
+    elif network_name == 'grconvnet2':
+        from .grconvnet2 import GenerativeResnet
+        return GenerativeResnet
+    # Configurable GR-ConvNet with dropout at the end
+    elif network_name == 'grconvnet3':
+        from .grconvnet3 import GenerativeResnet
+        return GenerativeResnet
+    # Inverted GR-ConvNet
+    elif network_name == 'grconvnet4':
+        from .grconvnet4 import GenerativeResnet
+        return GenerativeResnet
+    else:
+        raise NotImplementedError('Network {} is not implemented'.format(network_name))
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/inference/post_process.py OWG-main/robotic-grasping-master/inference/post_process.py
--- OWG-upstream/robotic-grasping-master/inference/post_process.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/inference/post_process.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,22 @@
+import torch
+from skimage.filters import gaussian
+
+
+def post_process_output(q_img, cos_img, sin_img, width_img):
+    """
+    Post-process the raw output of the network, convert to numpy arrays, apply filtering.
+    :param q_img: Q output of network (as torch Tensors)
+    :param cos_img: cos output of network
+    :param sin_img: sin output of network
+    :param width_img: Width output of network
+    :return: Filtered Q output, Filtered Angle output, Filtered Width output
+    """
+    q_img = q_img.cpu().numpy().squeeze()
+    ang_img = (torch.atan2(sin_img, cos_img) / 2.0).cpu().numpy().squeeze()
+    width_img = width_img.cpu().numpy().squeeze() * 150.0
+
+    q_img = gaussian(q_img, 2.0, preserve_range=True)
+    ang_img = gaussian(ang_img, 2.0, preserve_range=True)
+    width_img = gaussian(width_img, 1.0, preserve_range=True)
+
+    return q_img, ang_img, width_img
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/LICENSE OWG-main/robotic-grasping-master/LICENSE
--- OWG-upstream/robotic-grasping-master/LICENSE	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/LICENSE	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,44 @@
+BSD 3-Clause License
+
+Copyright (c) 2019, Sulabh Kumra, Multi-Agent Bio-Robotics Laboratory (MABL), Rochester Institute of Technology
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice, this
+   list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice,
+   this list of conditions and the following disclaimer in the documentation
+   and/or other materials provided with the distribution.
+
+3. Neither the name of the copyright holder nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+
+Copyright (c) 2018, Douglas Morrison, ARC Centre of Excellence for Robotic Vision (ACRV), Queensland University of Technology
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
+
+3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/README.md OWG-main/robotic-grasping-master/README.md
--- OWG-upstream/robotic-grasping-master/README.md	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/README.md	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,117 @@
+# Antipodal Robotic Grasping
+We present a novel generative residual convolutional neural network based model architecture which detects objects in the cameraâ€™s field of view and predicts a suitable antipodal grasp configuration for the objects in the image.
+
+This repository contains the implementation of the Generative Residual Convolutional Neural Network (GR-ConvNet) from the paper:
+
+#### Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network
+
+Sulabh Kumra, Shirin Joshi, Ferat Sahin
+
+[arxiv](https://arxiv.org/abs/1909.04810) | [video](https://youtu.be/cwlEhdoxY4U)
+
+[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/antipodal-robotic-grasping-using-generative/robotic-grasping-on-cornell-grasp-dataset)](https://paperswithcode.com/sota/robotic-grasping-on-cornell-grasp-dataset?p=antipodal-robotic-grasping-using-generative)
+
+If you use this project in your research or wish to refer to the baseline results published in the paper, please use the following BibTeX entry:
+
+```
+@inproceedings{kumra2020antipodal,
+  author={Kumra, Sulabh and Joshi, Shirin and Sahin, Ferat},
+  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
+  title={Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network}, 
+  year={2020},
+  pages={9626-9633},
+  doi={10.1109/IROS45743.2020.9340777}}
+}
+```
+
+## Requirements
+
+- numpy
+- opencv-python
+- matplotlib
+- scikit-image
+- imageio
+- torch
+- torchvision
+- torchsummary
+- tensorboardX
+- pyrealsense2
+- Pillow
+
+## Installation
+- Checkout the robotic grasping package
+```bash
+$ git clone https://github.com/skumra/robotic-grasping.git
+```
+
+- Create a virtual environment
+```bash
+$ python3.6 -m venv --system-site-packages venv
+```
+
+- Source the virtual environment
+```bash
+$ source venv/bin/activate
+```
+
+- Install the requirements
+```bash
+$ cd robotic-grasping
+$ pip install -r requirements.txt
+```
+
+## Datasets
+
+This repository supports both the [Cornell Grasping Dataset](https://www.kaggle.com/oneoneliu/cornell-grasp) and
+[Jacquard Dataset](https://jacquard.liris.cnrs.fr/).
+
+#### Cornell Grasping Dataset
+
+1. Download the and extract [Cornell Grasping Dataset](https://www.kaggle.com/oneoneliu/cornell-grasp). 
+2. Convert the PCD files to depth images by running `python -m utils.dataset_processing.generate_cornell_depth <Path To Dataset>`
+
+#### Jacquard Dataset
+
+1. Download and extract the [Jacquard Dataset](https://jacquard.liris.cnrs.fr/).
+
+
+## Model Training
+
+A model can be trained using the `train_network.py` script.  Run `train_network.py --help` to see a full list of options.
+
+Example for Cornell dataset:
+
+```bash
+python train_network.py --dataset cornell --dataset-path <Path To Dataset> --description training_cornell
+```
+
+Example for Jacquard dataset:
+
+```bash
+python train_network.py --dataset jacquard --dataset-path <Path To Dataset> --description training_jacquard --use-dropout 0 --input-size 300
+```
+
+## Model Evaluation
+
+The trained network can be evaluated using the `evaluate.py` script.  Run `evaluate.py --help` for a full set of options.
+
+Example for Cornell dataset:
+
+```bash
+python evaluate.py --network <Path to Trained Network> --dataset cornell --dataset-path <Path to Dataset> --iou-eval
+```
+
+Example for Jacquard dataset:
+
+```bash
+python evaluate.py --network <Path to Trained Network> --dataset jacquard --dataset-path <Path to Dataset> --iou-eval --use-dropout 0 --input-size 300
+```
+
+## Run Tasks
+A task can be executed using the relevant run script. All task scripts are named as `run_<task name>.py`. For example, to run the grasp generator run:
+```bash
+python run_grasp_generator.py
+```
+
+## Run on a Robot
+To run the grasp generator with a robot, please use our ROS implementation for Baxter robot. It is available at: https://github.com/skumra/baxter-pnp
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/requirements.txt OWG-main/robotic-grasping-master/requirements.txt
--- OWG-upstream/robotic-grasping-master/requirements.txt	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/requirements.txt	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,11 @@
+numpy
+opencv-python
+matplotlib
+scikit-image
+imageio
+torch
+torchvision
+torchsummary
+tensorboardX
+pyrealsense2
+Pillow
\ No newline at end of file
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/run_calibration.py OWG-main/robotic-grasping-master/run_calibration.py
--- OWG-upstream/robotic-grasping-master/run_calibration.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/run_calibration.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,13 @@
+#!/usr/bin/env python
+import numpy as np
+
+from hardware.calibrate_camera import Calibration
+
+if __name__ == '__main__':
+    calibration = Calibration(
+        cam_id=830112070066,
+        calib_grid_step=0.05,
+        checkerboard_offset_from_tool=[0.0, 0.0215, 0.0115],
+        workspace_limits=np.asarray([[0.55, 0.65], [-0.2, -0.1], [0.0, 0.2]])
+    )
+    calibration.run()
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/run_grasp_generator.py OWG-main/robotic-grasping-master/run_grasp_generator.py
--- OWG-upstream/robotic-grasping-master/run_grasp_generator.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/run_grasp_generator.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,10 @@
+from inference.grasp_generator import GraspGenerator
+
+if __name__ == '__main__':
+    generator = GraspGenerator(
+        cam_id=830112070066,
+        saved_model_path='saved_data/cornell_rgbd_iou_0.96',
+        visualize=True
+    )
+    generator.load_model()
+    generator.run()
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/run_offline.py OWG-main/robotic-grasping-master/run_offline.py
--- OWG-upstream/robotic-grasping-master/run_offline.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/run_offline.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,85 @@
+import argparse
+import logging
+
+import matplotlib.pyplot as plt
+import numpy as np
+import torch.utils.data
+from PIL import Image
+
+from hardware.device import get_device
+from inference.post_process import post_process_output
+from utils.data.camera_data import CameraData
+from utils.visualisation.plot import plot_results, save_results
+
+logging.basicConfig(level=logging.INFO)
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Evaluate network')
+    parser.add_argument('--network', type=str,
+                        help='Path to saved network to evaluate')
+    parser.add_argument('--rgb_path', type=str, default='cornell/08/pcd0845r.png',
+                        help='RGB Image path')
+    parser.add_argument('--depth_path', type=str, default='cornell/08/pcd0845d.tiff',
+                        help='Depth Image path')
+    parser.add_argument('--use-depth', type=int, default=1,
+                        help='Use Depth image for evaluation (1/0)')
+    parser.add_argument('--use-rgb', type=int, default=1,
+                        help='Use RGB image for evaluation (1/0)')
+    parser.add_argument('--n-grasps', type=int, default=1,
+                        help='Number of grasps to consider per image')
+    parser.add_argument('--save', type=int, default=0,
+                        help='Save the results')
+    parser.add_argument('--cpu', dest='force_cpu', action='store_true', default=False,
+                        help='Force code to run in CPU mode')
+
+    args = parser.parse_args()
+    return args
+
+
+if __name__ == '__main__':
+    args = parse_args()
+
+    # Load image
+    logging.info('Loading image...')
+    pic = Image.open(args.rgb_path, 'r')
+    rgb = np.array(pic)
+    pic = Image.open(args.depth_path, 'r')
+    depth = np.expand_dims(np.array(pic), axis=2)
+
+    # Load Network
+    logging.info('Loading model...')
+    net = torch.load(args.network)
+    logging.info('Done')
+
+    # Get the compute device
+    device = get_device(args.force_cpu)
+
+    img_data = CameraData(include_depth=args.use_depth, include_rgb=args.use_rgb)
+
+    x, depth_img, rgb_img = img_data.get_data(rgb=rgb, depth=depth)
+
+    with torch.no_grad():
+        xc = x.to(device)
+        pred = net.predict(xc)
+
+        q_img, ang_img, width_img = post_process_output(pred['pos'], pred['cos'], pred['sin'], pred['width'])
+
+        if args.save:
+            save_results(
+                rgb_img=img_data.get_rgb(rgb, False),
+                depth_img=np.squeeze(img_data.get_depth(depth)),
+                grasp_q_img=q_img,
+                grasp_angle_img=ang_img,
+                no_grasps=args.n_grasps,
+                grasp_width_img=width_img
+            )
+        else:
+            fig = plt.figure(figsize=(10, 10))
+            plot_results(fig=fig,
+                         rgb_img=img_data.get_rgb(rgb, False),
+                         grasp_q_img=q_img,
+                         grasp_angle_img=ang_img,
+                         no_grasps=args.n_grasps,
+                         grasp_width_img=width_img)
+            fig.savefig('img_result.pdf')
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/run_realtime.py OWG-main/robotic-grasping-master/run_realtime.py
--- OWG-upstream/robotic-grasping-master/run_realtime.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/run_realtime.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,79 @@
+import argparse
+import logging
+
+import matplotlib.pyplot as plt
+import numpy as np
+import torch.utils.data
+
+from hardware.camera import RealSenseCamera
+from hardware.device import get_device
+from inference.post_process import post_process_output
+from utils.data.camera_data import CameraData
+from utils.visualisation.plot import save_results, plot_results
+
+logging.basicConfig(level=logging.INFO)
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Evaluate network')
+    parser.add_argument('--network', type=str, default='saved_data/cornell_rgbd_iou_0.96',
+                        help='Path to saved network to evaluate')
+    parser.add_argument('--use-depth', type=int, default=1,
+                        help='Use Depth image for evaluation (1/0)')
+    parser.add_argument('--use-rgb', type=int, default=1,
+                        help='Use RGB image for evaluation (1/0)')
+    parser.add_argument('--n-grasps', type=int, default=1,
+                        help='Number of grasps to consider per image')
+    parser.add_argument('--cpu', dest='force_cpu', action='store_true', default=False,
+                        help='Force code to run in CPU mode')
+
+    args = parser.parse_args()
+    return args
+
+
+if __name__ == '__main__':
+    args = parse_args()
+
+    # Connect to Camera
+    logging.info('Connecting to camera...')
+    cam = RealSenseCamera(device_id=830112070066)
+    cam.connect()
+    cam_data = CameraData(include_depth=args.use_depth, include_rgb=args.use_rgb)
+
+    # Load Network
+    logging.info('Loading model...')
+    net = torch.load(args.network)
+    logging.info('Done')
+
+    # Get the compute device
+    device = get_device(args.force_cpu)
+
+    try:
+        fig = plt.figure(figsize=(10, 10))
+        while True:
+            image_bundle = cam.get_image_bundle()
+            rgb = image_bundle['rgb']
+            depth = image_bundle['aligned_depth']
+            x, depth_img, rgb_img = cam_data.get_data(rgb=rgb, depth=depth)
+            with torch.no_grad():
+                xc = x.to(device)
+                pred = net.predict(xc)
+
+                q_img, ang_img, width_img = post_process_output(pred['pos'], pred['cos'], pred['sin'], pred['width'])
+
+                plot_results(fig=fig,
+                             rgb_img=cam_data.get_rgb(rgb, False),
+                             depth_img=np.squeeze(cam_data.get_depth(depth)),
+                             grasp_q_img=q_img,
+                             grasp_angle_img=ang_img,
+                             no_grasps=args.n_grasps,
+                             grasp_width_img=width_img)
+    finally:
+        save_results(
+            rgb_img=cam_data.get_rgb(rgb, False),
+            depth_img=np.squeeze(cam_data.get_depth(depth)),
+            grasp_q_img=q_img,
+            grasp_angle_img=ang_img,
+            no_grasps=args.n_grasps,
+            grasp_width_img=width_img
+        )
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/arch.txt OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/arch.txt
--- OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/arch.txt	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/arch.txt	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,57 @@
+----------------------------------------------------------------
+        Layer (type)               Output Shape         Param #
+================================================================
+            Conv2d-1         [-1, 16, 224, 224]           5,200
+       BatchNorm2d-2         [-1, 16, 224, 224]              32
+            Conv2d-3         [-1, 32, 112, 112]           8,224
+       BatchNorm2d-4         [-1, 32, 112, 112]              64
+            Conv2d-5           [-1, 64, 56, 56]          32,832
+       BatchNorm2d-6           [-1, 64, 56, 56]             128
+            Conv2d-7           [-1, 64, 56, 56]          36,928
+       BatchNorm2d-8           [-1, 64, 56, 56]             128
+            Conv2d-9           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-10           [-1, 64, 56, 56]             128
+    ResidualBlock-11           [-1, 64, 56, 56]               0
+           Conv2d-12           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-13           [-1, 64, 56, 56]             128
+           Conv2d-14           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-15           [-1, 64, 56, 56]             128
+    ResidualBlock-16           [-1, 64, 56, 56]               0
+           Conv2d-17           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-18           [-1, 64, 56, 56]             128
+           Conv2d-19           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-20           [-1, 64, 56, 56]             128
+    ResidualBlock-21           [-1, 64, 56, 56]               0
+           Conv2d-22           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-23           [-1, 64, 56, 56]             128
+           Conv2d-24           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-25           [-1, 64, 56, 56]             128
+    ResidualBlock-26           [-1, 64, 56, 56]               0
+           Conv2d-27           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-28           [-1, 64, 56, 56]             128
+           Conv2d-29           [-1, 64, 56, 56]          36,928
+      BatchNorm2d-30           [-1, 64, 56, 56]             128
+    ResidualBlock-31           [-1, 64, 56, 56]               0
+  ConvTranspose2d-32         [-1, 32, 113, 113]          32,800
+      BatchNorm2d-33         [-1, 32, 113, 113]              64
+  ConvTranspose2d-34         [-1, 16, 225, 225]           8,208
+      BatchNorm2d-35         [-1, 16, 225, 225]              32
+  ConvTranspose2d-36         [-1, 16, 225, 225]          20,752
+          Dropout-37         [-1, 16, 225, 225]               0
+           Conv2d-38          [-1, 1, 224, 224]              65
+          Dropout-39         [-1, 16, 225, 225]               0
+           Conv2d-40          [-1, 1, 224, 224]              65
+          Dropout-41         [-1, 16, 225, 225]               0
+           Conv2d-42          [-1, 1, 224, 224]              65
+          Dropout-43         [-1, 16, 225, 225]               0
+           Conv2d-44          [-1, 1, 224, 224]              65
+================================================================
+Total params: 479,156
+Trainable params: 479,156
+Non-trainable params: 0
+----------------------------------------------------------------
+Input size (MB): 0.77
+Forward/backward pass size (MB): 110.74
+Params size (MB): 1.83
+Estimated Total Size (MB): 113.34
+----------------------------------------------------------------
Binary files OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/epoch_17_iou_0.96 and OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/epoch_17_iou_0.96 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/epoch_20_iou_0.97 and OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/epoch_20_iou_0.97 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/epoch_30_iou_0.97 and OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch16/epoch_30_iou_0.97 differ
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/arch.txt OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/arch.txt
--- OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/arch.txt	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/arch.txt	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,57 @@
+----------------------------------------------------------------
+        Layer (type)               Output Shape         Param #
+================================================================
+            Conv2d-1         [-1, 32, 224, 224]          10,400
+       BatchNorm2d-2         [-1, 32, 224, 224]              64
+            Conv2d-3         [-1, 64, 112, 112]          32,832
+       BatchNorm2d-4         [-1, 64, 112, 112]             128
+            Conv2d-5          [-1, 128, 56, 56]         131,200
+       BatchNorm2d-6          [-1, 128, 56, 56]             256
+            Conv2d-7          [-1, 128, 56, 56]         147,584
+       BatchNorm2d-8          [-1, 128, 56, 56]             256
+            Conv2d-9          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-10          [-1, 128, 56, 56]             256
+    ResidualBlock-11          [-1, 128, 56, 56]               0
+           Conv2d-12          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-13          [-1, 128, 56, 56]             256
+           Conv2d-14          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-15          [-1, 128, 56, 56]             256
+    ResidualBlock-16          [-1, 128, 56, 56]               0
+           Conv2d-17          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-18          [-1, 128, 56, 56]             256
+           Conv2d-19          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-20          [-1, 128, 56, 56]             256
+    ResidualBlock-21          [-1, 128, 56, 56]               0
+           Conv2d-22          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-23          [-1, 128, 56, 56]             256
+           Conv2d-24          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-25          [-1, 128, 56, 56]             256
+    ResidualBlock-26          [-1, 128, 56, 56]               0
+           Conv2d-27          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-28          [-1, 128, 56, 56]             256
+           Conv2d-29          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-30          [-1, 128, 56, 56]             256
+    ResidualBlock-31          [-1, 128, 56, 56]               0
+  ConvTranspose2d-32         [-1, 64, 113, 113]         131,136
+      BatchNorm2d-33         [-1, 64, 113, 113]             128
+  ConvTranspose2d-34         [-1, 32, 225, 225]          32,800
+      BatchNorm2d-35         [-1, 32, 225, 225]              64
+  ConvTranspose2d-36         [-1, 32, 225, 225]          82,976
+          Dropout-37         [-1, 32, 225, 225]               0
+           Conv2d-38          [-1, 1, 224, 224]             129
+          Dropout-39         [-1, 32, 225, 225]               0
+           Conv2d-40          [-1, 1, 224, 224]             129
+          Dropout-41         [-1, 32, 225, 225]               0
+           Conv2d-42          [-1, 1, 224, 224]             129
+          Dropout-43         [-1, 32, 225, 225]               0
+           Conv2d-44          [-1, 1, 224, 224]             129
+================================================================
+Total params: 1,900,900
+Trainable params: 1,900,900
+Non-trainable params: 0
+----------------------------------------------------------------
+Input size (MB): 0.77
+Forward/backward pass size (MB): 219.96
+Params size (MB): 7.25
+Estimated Total Size (MB): 227.97
+----------------------------------------------------------------
Binary files OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_13_iou_0.96 and OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_13_iou_0.96 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_15_iou_0.97 and OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_15_iou_0.97 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_19_iou_0.98 and OWG-main/robotic-grasping-master/trained-models/cornell-randsplit-rgbd-grconvnet3-drop1-ch32/epoch_19_iou_0.98 differ
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/arch.txt OWG-main/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/arch.txt
--- OWG-upstream/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/arch.txt	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/arch.txt	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,57 @@
+----------------------------------------------------------------
+        Layer (type)               Output Shape         Param #
+================================================================
+            Conv2d-1         [-1, 32, 224, 224]           2,624
+       BatchNorm2d-2         [-1, 32, 224, 224]              64
+            Conv2d-3         [-1, 64, 112, 112]          32,832
+       BatchNorm2d-4         [-1, 64, 112, 112]             128
+            Conv2d-5          [-1, 128, 56, 56]         131,200
+       BatchNorm2d-6          [-1, 128, 56, 56]             256
+            Conv2d-7          [-1, 128, 56, 56]         147,584
+       BatchNorm2d-8          [-1, 128, 56, 56]             256
+            Conv2d-9          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-10          [-1, 128, 56, 56]             256
+    ResidualBlock-11          [-1, 128, 56, 56]               0
+           Conv2d-12          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-13          [-1, 128, 56, 56]             256
+           Conv2d-14          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-15          [-1, 128, 56, 56]             256
+    ResidualBlock-16          [-1, 128, 56, 56]               0
+           Conv2d-17          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-18          [-1, 128, 56, 56]             256
+           Conv2d-19          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-20          [-1, 128, 56, 56]             256
+    ResidualBlock-21          [-1, 128, 56, 56]               0
+           Conv2d-22          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-23          [-1, 128, 56, 56]             256
+           Conv2d-24          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-25          [-1, 128, 56, 56]             256
+    ResidualBlock-26          [-1, 128, 56, 56]               0
+           Conv2d-27          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-28          [-1, 128, 56, 56]             256
+           Conv2d-29          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-30          [-1, 128, 56, 56]             256
+    ResidualBlock-31          [-1, 128, 56, 56]               0
+  ConvTranspose2d-32         [-1, 64, 113, 113]         131,136
+      BatchNorm2d-33         [-1, 64, 113, 113]             128
+  ConvTranspose2d-34         [-1, 32, 225, 225]          32,800
+      BatchNorm2d-35         [-1, 32, 225, 225]              64
+  ConvTranspose2d-36         [-1, 32, 225, 225]          82,976
+          Dropout-37         [-1, 32, 225, 225]               0
+           Conv2d-38          [-1, 1, 224, 224]             129
+          Dropout-39         [-1, 32, 225, 225]               0
+           Conv2d-40          [-1, 1, 224, 224]             129
+          Dropout-41         [-1, 32, 225, 225]               0
+           Conv2d-42          [-1, 1, 224, 224]             129
+          Dropout-43         [-1, 32, 225, 225]               0
+           Conv2d-44          [-1, 1, 224, 224]             129
+================================================================
+Total params: 1,893,124
+Trainable params: 1,893,124
+Non-trainable params: 0
+----------------------------------------------------------------
+Input size (MB): 0.19
+Forward/backward pass size (MB): 219.96
+Params size (MB): 7.22
+Estimated Total Size (MB): 227.37
+----------------------------------------------------------------
Binary files OWG-upstream/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/epoch_44_iou_0.93 and OWG-main/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/epoch_44_iou_0.93 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/epoch_48_iou_0.93 and OWG-main/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/epoch_48_iou_0.93 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/epoch_50_iou_0.94 and OWG-main/robotic-grasping-master/trained-models/jacquard-d-grconvnet3-drop0-ch32/epoch_50_iou_0.94 differ
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/arch.txt OWG-main/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/arch.txt
--- OWG-upstream/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/arch.txt	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/arch.txt	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,53 @@
+----------------------------------------------------------------
+        Layer (type)               Output Shape         Param #
+================================================================
+            Conv2d-1         [-1, 32, 224, 224]          10,400
+       BatchNorm2d-2         [-1, 32, 224, 224]              64
+            Conv2d-3         [-1, 64, 112, 112]          32,832
+       BatchNorm2d-4         [-1, 64, 112, 112]             128
+            Conv2d-5          [-1, 128, 56, 56]         131,200
+       BatchNorm2d-6          [-1, 128, 56, 56]             256
+            Conv2d-7          [-1, 128, 56, 56]         147,584
+       BatchNorm2d-8          [-1, 128, 56, 56]             256
+            Conv2d-9          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-10          [-1, 128, 56, 56]             256
+    ResidualBlock-11          [-1, 128, 56, 56]               0
+           Conv2d-12          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-13          [-1, 128, 56, 56]             256
+           Conv2d-14          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-15          [-1, 128, 56, 56]             256
+    ResidualBlock-16          [-1, 128, 56, 56]               0
+           Conv2d-17          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-18          [-1, 128, 56, 56]             256
+           Conv2d-19          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-20          [-1, 128, 56, 56]             256
+    ResidualBlock-21          [-1, 128, 56, 56]               0
+           Conv2d-22          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-23          [-1, 128, 56, 56]             256
+           Conv2d-24          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-25          [-1, 128, 56, 56]             256
+    ResidualBlock-26          [-1, 128, 56, 56]               0
+           Conv2d-27          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-28          [-1, 128, 56, 56]             256
+           Conv2d-29          [-1, 128, 56, 56]         147,584
+      BatchNorm2d-30          [-1, 128, 56, 56]             256
+    ResidualBlock-31          [-1, 128, 56, 56]               0
+  ConvTranspose2d-32         [-1, 64, 113, 113]         131,136
+      BatchNorm2d-33         [-1, 64, 113, 113]             128
+  ConvTranspose2d-34         [-1, 32, 225, 225]          32,800
+      BatchNorm2d-35         [-1, 32, 225, 225]              64
+  ConvTranspose2d-36         [-1, 32, 225, 225]          82,976
+           Conv2d-37          [-1, 1, 224, 224]             129
+           Conv2d-38          [-1, 1, 224, 224]             129
+           Conv2d-39          [-1, 1, 224, 224]             129
+           Conv2d-40          [-1, 1, 224, 224]             129
+================================================================
+Total params: 1,900,900
+Trainable params: 1,900,900
+Non-trainable params: 0
+----------------------------------------------------------------
+Input size (MB): 0.77
+Forward/backward pass size (MB): 170.52
+Params size (MB): 7.25
+Estimated Total Size (MB): 178.53
+----------------------------------------------------------------
Binary files OWG-upstream/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/epoch_35_iou_0.92 and OWG-main/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/epoch_35_iou_0.92 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/epoch_42_iou_0.93 and OWG-main/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/epoch_42_iou_0.93 differ
Binary files OWG-upstream/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/epoch_48_iou_0.93 and OWG-main/robotic-grasping-master/trained-models/jacquard-rgbd-grconvnet3-drop0-ch32/epoch_48_iou_0.93 differ
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/train_network.py OWG-main/robotic-grasping-master/train_network.py
--- OWG-upstream/robotic-grasping-master/train_network.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/train_network.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,345 @@
+import argparse
+import datetime
+import json
+import logging
+import os
+import sys
+
+import cv2
+import numpy as np
+import tensorboardX
+import torch
+import torch.optim as optim
+import torch.utils.data
+from torchsummary import summary
+
+from hardware.device import get_device
+from inference.models import get_network
+from inference.post_process import post_process_output
+from utils.data import get_dataset
+from utils.dataset_processing import evaluation
+from utils.visualisation.gridshow import gridshow
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Train network')
+
+    # Network
+    parser.add_argument('--network', type=str, default='grconvnet3',
+                        help='Network name in inference/models')
+    parser.add_argument('--input-size', type=int, default=224,
+                        help='Input image size for the network')
+    parser.add_argument('--use-depth', type=int, default=1,
+                        help='Use Depth image for training (1/0)')
+    parser.add_argument('--use-rgb', type=int, default=1,
+                        help='Use RGB image for training (1/0)')
+    parser.add_argument('--use-dropout', type=int, default=1,
+                        help='Use dropout for training (1/0)')
+    parser.add_argument('--dropout-prob', type=float, default=0.1,
+                        help='Dropout prob for training (0-1)')
+    parser.add_argument('--channel-size', type=int, default=32,
+                        help='Internal channel size for the network')
+    parser.add_argument('--iou-threshold', type=float, default=0.25,
+                        help='Threshold for IOU matching')
+
+    # Datasets
+    parser.add_argument('--dataset', type=str,
+                        help='Dataset Name ("cornell" or "jaquard")')
+    parser.add_argument('--dataset-path', type=str,
+                        help='Path to dataset')
+    parser.add_argument('--split', type=float, default=0.9,
+                        help='Fraction of data for training (remainder is validation)')
+    parser.add_argument('--ds-shuffle', action='store_true', default=False,
+                        help='Shuffle the dataset')
+    parser.add_argument('--ds-rotate', type=float, default=0.0,
+                        help='Shift the start point of the dataset to use a different test/train split')
+    parser.add_argument('--num-workers', type=int, default=8,
+                        help='Dataset workers')
+
+    # Training
+    parser.add_argument('--batch-size', type=int, default=8,
+                        help='Batch size')
+    parser.add_argument('--epochs', type=int, default=50,
+                        help='Training epochs')
+    parser.add_argument('--batches-per-epoch', type=int, default=1000,
+                        help='Batches per Epoch')
+    parser.add_argument('--optim', type=str, default='adam',
+                        help='Optmizer for the training. (adam or SGD)')
+
+    # Logging etc.
+    parser.add_argument('--description', type=str, default='',
+                        help='Training description')
+    parser.add_argument('--logdir', type=str, default='logs/',
+                        help='Log directory')
+    parser.add_argument('--vis', action='store_true',
+                        help='Visualise the training process')
+    parser.add_argument('--cpu', dest='force_cpu', action='store_true', default=False,
+                        help='Force code to run in CPU mode')
+    parser.add_argument('--random-seed', type=int, default=123,
+                        help='Random seed for numpy')
+
+    args = parser.parse_args()
+    return args
+
+
+def validate(net, device, val_data, iou_threshold):
+    """
+    Run validation.
+    :param net: Network
+    :param device: Torch device
+    :param val_data: Validation Dataset
+    :param iou_threshold: IoU threshold
+    :return: Successes, Failures and Losses
+    """
+    net.eval()
+
+    results = {
+        'correct': 0,
+        'failed': 0,
+        'loss': 0,
+        'losses': {
+
+        }
+    }
+
+    ld = len(val_data)
+
+    with torch.no_grad():
+        for x, y, didx, rot, zoom_factor in val_data:
+            xc = x.to(device)
+            yc = [yy.to(device) for yy in y]
+            lossd = net.compute_loss(xc, yc)
+
+            loss = lossd['loss']
+
+            results['loss'] += loss.item() / ld
+            for ln, l in lossd['losses'].items():
+                if ln not in results['losses']:
+                    results['losses'][ln] = 0
+                results['losses'][ln] += l.item() / ld
+
+            q_out, ang_out, w_out = post_process_output(lossd['pred']['pos'], lossd['pred']['cos'],
+                                                        lossd['pred']['sin'], lossd['pred']['width'])
+
+            s = evaluation.calculate_iou_match(q_out,
+                                               ang_out,
+                                               val_data.dataset.get_gtbb(didx, rot, zoom_factor),
+                                               no_grasps=1,
+                                               grasp_width=w_out,
+                                               threshold=iou_threshold
+                                               )
+
+            if s:
+                results['correct'] += 1
+            else:
+                results['failed'] += 1
+
+    return results
+
+
+def train(epoch, net, device, train_data, optimizer, batches_per_epoch, vis=False):
+    """
+    Run one training epoch
+    :param epoch: Current epoch
+    :param net: Network
+    :param device: Torch device
+    :param train_data: Training Dataset
+    :param optimizer: Optimizer
+    :param batches_per_epoch:  Data batches to train on
+    :param vis:  Visualise training progress
+    :return:  Average Losses for Epoch
+    """
+    results = {
+        'loss': 0,
+        'losses': {
+        }
+    }
+
+    net.train()
+
+    batch_idx = 0
+    # Use batches per epoch to make training on different sized datasets (cornell/jacquard) more equivalent.
+    while batch_idx <= batches_per_epoch:
+        for x, y, _, _, _ in train_data:
+            batch_idx += 1
+            if batch_idx >= batches_per_epoch:
+                break
+
+            xc = x.to(device)
+            yc = [yy.to(device) for yy in y]
+            lossd = net.compute_loss(xc, yc)
+
+            loss = lossd['loss']
+
+            if batch_idx % 100 == 0:
+                logging.info('Epoch: {}, Batch: {}, Loss: {:0.4f}'.format(epoch, batch_idx, loss.item()))
+
+            results['loss'] += loss.item()
+            for ln, l in lossd['losses'].items():
+                if ln not in results['losses']:
+                    results['losses'][ln] = 0
+                results['losses'][ln] += l.item()
+
+            optimizer.zero_grad()
+            loss.backward()
+            optimizer.step()
+
+            # Display the images
+            if vis:
+                imgs = []
+                n_img = min(4, x.shape[0])
+                for idx in range(n_img):
+                    imgs.extend([x[idx,].numpy().squeeze()] + [yi[idx,].numpy().squeeze() for yi in y] + [
+                        x[idx,].numpy().squeeze()] + [pc[idx,].detach().cpu().numpy().squeeze() for pc in
+                                                      lossd['pred'].values()])
+                gridshow('Display', imgs,
+                         [(xc.min().item(), xc.max().item()), (0.0, 1.0), (0.0, 1.0), (-1.0, 1.0),
+                          (0.0, 1.0)] * 2 * n_img,
+                         [cv2.COLORMAP_BONE] * 10 * n_img, 10)
+                cv2.waitKey(2)
+
+    results['loss'] /= batch_idx
+    for l in results['losses']:
+        results['losses'][l] /= batch_idx
+
+    return results
+
+
+def run():
+    args = parse_args()
+
+    # Set-up output directories
+    dt = datetime.datetime.now().strftime('%y%m%d_%H%M')
+    net_desc = '{}_{}'.format(dt, '_'.join(args.description.split()))
+
+    save_folder = os.path.join(args.logdir, net_desc)
+    if not os.path.exists(save_folder):
+        os.makedirs(save_folder)
+    tb = tensorboardX.SummaryWriter(save_folder)
+
+    # Save commandline args
+    if args is not None:
+        params_path = os.path.join(save_folder, 'commandline_args.json')
+        with open(params_path, 'w') as f:
+            json.dump(vars(args), f)
+
+    # Initialize logging
+    logging.root.handlers = []
+    logging.basicConfig(
+        level=logging.INFO,
+        filename="{0}/{1}.log".format(save_folder, 'log'),
+        format='[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s',
+        datefmt='%H:%M:%S'
+    )
+    # set up logging to console
+    console = logging.StreamHandler()
+    console.setLevel(logging.DEBUG)
+    # set a format which is simpler for console use
+    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
+    console.setFormatter(formatter)
+    # add the handler to the root logger
+    logging.getLogger('').addHandler(console)
+
+    # Get the compute device
+    device = get_device(args.force_cpu)
+
+    # Load Dataset
+    logging.info('Loading {} Dataset...'.format(args.dataset.title()))
+    Dataset = get_dataset(args.dataset)
+    dataset = Dataset(args.dataset_path,
+                      output_size=args.input_size,
+                      ds_rotate=args.ds_rotate,
+                      random_rotate=True,
+                      random_zoom=True,
+                      include_depth=args.use_depth,
+                      include_rgb=args.use_rgb)
+    logging.info('Dataset size is {}'.format(dataset.length))
+
+    # Creating data indices for training and validation splits
+    indices = list(range(dataset.length))
+    split = int(np.floor(args.split * dataset.length))
+    if args.ds_shuffle:
+        np.random.seed(args.random_seed)
+        np.random.shuffle(indices)
+    train_indices, val_indices = indices[:split], indices[split:]
+    logging.info('Training size: {}'.format(len(train_indices)))
+    logging.info('Validation size: {}'.format(len(val_indices)))
+
+    # Creating data samplers and loaders
+    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)
+    val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)
+
+    train_data = torch.utils.data.DataLoader(
+        dataset,
+        batch_size=args.batch_size,
+        num_workers=args.num_workers,
+        sampler=train_sampler
+    )
+    val_data = torch.utils.data.DataLoader(
+        dataset,
+        batch_size=1,
+        num_workers=args.num_workers,
+        sampler=val_sampler
+    )
+    logging.info('Done')
+
+    # Load the network
+    logging.info('Loading Network...')
+    input_channels = 1 * args.use_depth + 3 * args.use_rgb
+    network = get_network(args.network)
+    net = network(
+        input_channels=input_channels,
+        dropout=args.use_dropout,
+        prob=args.dropout_prob,
+        channel_size=args.channel_size
+    )
+
+    net = net.to(device)
+    logging.info('Done')
+
+    if args.optim.lower() == 'adam':
+        optimizer = optim.Adam(net.parameters())
+    elif args.optim.lower() == 'sgd':
+        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)
+    else:
+        raise NotImplementedError('Optimizer {} is not implemented'.format(args.optim))
+
+    # Print model architecture.
+    summary(net, (input_channels, args.input_size, args.input_size))
+    f = open(os.path.join(save_folder, 'arch.txt'), 'w')
+    sys.stdout = f
+    summary(net, (input_channels, args.input_size, args.input_size))
+    sys.stdout = sys.__stdout__
+    f.close()
+
+    best_iou = 0.0
+    for epoch in range(args.epochs):
+        logging.info('Beginning Epoch {:02d}'.format(epoch))
+        train_results = train(epoch, net, device, train_data, optimizer, args.batches_per_epoch, vis=args.vis)
+
+        # Log training losses to tensorboard
+        tb.add_scalar('loss/train_loss', train_results['loss'], epoch)
+        for n, l in train_results['losses'].items():
+            tb.add_scalar('train_loss/' + n, l, epoch)
+
+        # Run Validation
+        logging.info('Validating...')
+        test_results = validate(net, device, val_data, args.iou_threshold)
+        logging.info('%d/%d = %f' % (test_results['correct'], test_results['correct'] + test_results['failed'],
+                                     test_results['correct'] / (test_results['correct'] + test_results['failed'])))
+
+        # Log validation results to tensorbaord
+        tb.add_scalar('loss/IOU', test_results['correct'] / (test_results['correct'] + test_results['failed']), epoch)
+        tb.add_scalar('loss/val_loss', test_results['loss'], epoch)
+        for n, l in test_results['losses'].items():
+            tb.add_scalar('val_loss/' + n, l, epoch)
+
+        # Save best performing network
+        iou = test_results['correct'] / (test_results['correct'] + test_results['failed'])
+        if iou > best_iou or epoch == 0 or (epoch % 10) == 0:
+            torch.save(net, os.path.join(save_folder, 'epoch_%02d_iou_%0.2f' % (epoch, iou)))
+            best_iou = iou
+
+
+if __name__ == '__main__':
+    run()
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/dataset_processing/evaluation.py OWG-main/robotic-grasping-master/utils/dataset_processing/evaluation.py
--- OWG-upstream/robotic-grasping-master/utils/dataset_processing/evaluation.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/dataset_processing/evaluation.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,79 @@
+import warnings
+
+import matplotlib.pyplot as plt
+import numpy as np
+
+warnings.filterwarnings("ignore")
+
+from .grasp import GraspRectangles, detect_grasps
+
+
+def plot_output(fig, rgb_img, grasp_q_img, grasp_angle_img, depth_img=None, no_grasps=1, grasp_width_img=None):
+    """
+    Plot the output of a network
+    :param fig: Figure to plot the output
+    :param rgb_img: RGB Image
+    :param depth_img: Depth Image
+    :param grasp_q_img: Q output of network
+    :param grasp_angle_img: Angle output of network
+    :param no_grasps: Maximum number of grasps to plot
+    :param grasp_width_img: (optional) Width output of network
+    :return:
+    """
+    gs = detect_grasps(grasp_q_img, grasp_angle_img, width_img=grasp_width_img, no_grasps=no_grasps)
+
+    plt.ion()
+    plt.clf()
+    ax = fig.add_subplot(2, 2, 1)
+    ax.imshow(rgb_img)
+    for g in gs:
+        g.plot(ax)
+    ax.set_title('RGB')
+    ax.axis('off')
+
+    if depth_img:
+        ax = fig.add_subplot(2, 2, 2)
+        ax.imshow(depth_img, cmap='gray')
+        for g in gs:
+            g.plot(ax)
+        ax.set_title('Depth')
+        ax.axis('off')
+
+    ax = fig.add_subplot(2, 2, 3)
+    plot = ax.imshow(grasp_q_img, cmap='jet', vmin=0, vmax=1)
+    ax.set_title('Q')
+    ax.axis('off')
+    plt.colorbar(plot)
+
+    ax = fig.add_subplot(2, 2, 4)
+    plot = ax.imshow(grasp_angle_img, cmap='hsv', vmin=-np.pi / 2, vmax=np.pi / 2)
+    ax.set_title('Angle')
+    ax.axis('off')
+    plt.colorbar(plot)
+    plt.pause(0.1)
+    fig.canvas.draw()
+
+
+def calculate_iou_match(grasp_q, grasp_angle, ground_truth_bbs, no_grasps=1, grasp_width=None, threshold=0.25):
+    """
+    Calculate grasp success using the IoU (Jacquard) metric (e.g. in https://arxiv.org/abs/1301.3592)
+    A success is counted if grasp rectangle has a 25% IoU with a ground truth, and is withing 30 degrees.
+    :param grasp_q: Q outputs of network (Nx300x300x3)
+    :param grasp_angle: Angle outputs of network
+    :param ground_truth_bbs: Corresponding ground-truth BoundingBoxes
+    :param no_grasps: Maximum number of grasps to consider per image.
+    :param grasp_width: (optional) Width output from network
+    :param threshold: Threshold for IOU matching. Detect with IOU â‰¥ threshold
+    :return: success
+    """
+
+    if not isinstance(ground_truth_bbs, GraspRectangles):
+        gt_bbs = GraspRectangles.load_from_array(ground_truth_bbs)
+    else:
+        gt_bbs = ground_truth_bbs
+    gs = detect_grasps(grasp_q, grasp_angle, width_img=grasp_width, no_grasps=no_grasps)
+    for g in gs:
+        if g.max_iou(gt_bbs) > threshold:
+            return True
+    else:
+        return False
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/dataset_processing/generate_cornell_depth.py OWG-main/robotic-grasping-master/utils/dataset_processing/generate_cornell_depth.py
--- OWG-upstream/robotic-grasping-master/utils/dataset_processing/generate_cornell_depth.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/dataset_processing/generate_cornell_depth.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,24 @@
+import argparse
+import glob
+import os
+
+import numpy as np
+from imageio import imsave
+
+from utils.dataset_processing.image import DepthImage
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser(description='Generate depth images from Cornell PCD files.')
+    parser.add_argument('path', type=str, help='Path to Cornell Grasping Dataset')
+    args = parser.parse_args()
+
+    pcds = glob.glob(os.path.join(args.path, '*', 'pcd*[0-9].txt'))
+    pcds.sort()
+
+    for pcd in pcds:
+        di = DepthImage.from_pcd(pcd, (480, 640))
+        di.inpaint()
+
+        of_name = pcd.replace('.txt', 'd.tiff')
+        print(of_name)
+        imsave(of_name, di.img.astype(np.float32))
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/dataset_processing/grasp.py OWG-main/robotic-grasping-master/utils/dataset_processing/grasp.py
--- OWG-upstream/robotic-grasping-master/utils/dataset_processing/grasp.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/dataset_processing/grasp.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,442 @@
+import matplotlib.pyplot as plt
+import numpy as np
+from skimage.draw import polygon
+from skimage.feature import peak_local_max
+
+
+def _gr_text_to_no(l, offset=(0, 0)):
+    """
+    Transform a single point from a Cornell file line to a pair of ints.
+    :param l: Line from Cornell grasp file (str)
+    :param offset: Offset to apply to point positions
+    :return: Point [y, x]
+    """
+    x, y = l.split()
+    return [int(round(float(y))) - offset[0], int(round(float(x))) - offset[1]]
+
+
+class GraspRectangles:
+    """
+    Convenience class for loading and operating on sets of Grasp Rectangles.
+    """
+
+    def __init__(self, grs=None):
+        if grs:
+            self.grs = grs
+        else:
+            self.grs = []
+
+    def __getitem__(self, item):
+        return self.grs[item]
+
+    def __iter__(self):
+        return self.grs.__iter__()
+
+    def __getattr__(self, attr):
+        """
+        Test if GraspRectangle has the desired attr as a function and call it.
+        """
+        # Fuck yeah python.
+        if hasattr(GraspRectangle, attr) and callable(getattr(GraspRectangle, attr)):
+            return lambda *args, **kwargs: list(map(lambda gr: getattr(gr, attr)(*args, **kwargs), self.grs))
+        else:
+            raise AttributeError("Couldn't find function %s in BoundingBoxes or BoundingBox" % attr)
+
+    @classmethod
+    def load_from_array(cls, arr):
+        """
+        Load grasp rectangles from numpy array.
+        :param arr: Nx4x2 array, where each 4x2 array is the 4 corner pixels of a grasp rectangle.
+        :return: GraspRectangles()
+        """
+        grs = []
+        for i in range(arr.shape[0]):
+            grp = arr[i, :, :].squeeze()
+            if grp.max() == 0:
+                break
+            else:
+                grs.append(GraspRectangle(grp))
+        return cls(grs)
+
+    @classmethod
+    def load_from_cornell_file(cls, fname):
+        """
+        Load grasp rectangles from a Cornell dataset grasp file.
+        :param fname: Path to text file.
+        :return: GraspRectangles()
+        """
+        grs = []
+        with open(fname) as f:
+            while True:
+                # Load 4 lines at a time, corners of bounding box.
+                p0 = f.readline()
+                if not p0:
+                    break  # EOF
+                p1, p2, p3 = f.readline(), f.readline(), f.readline()
+                try:
+                    gr = np.array([
+                        _gr_text_to_no(p0),
+                        _gr_text_to_no(p1),
+                        _gr_text_to_no(p2),
+                        _gr_text_to_no(p3)
+                    ])
+
+                    grs.append(GraspRectangle(gr))
+
+                except ValueError:
+                    # Some files contain weird values.
+                    continue
+        return cls(grs)
+
+    @classmethod
+    def load_from_jacquard_file(cls, fname, scale=1.0):
+        """
+        Load grasp rectangles from a Jacquard dataset file.
+        :param fname: Path to file.
+        :param scale: Scale to apply (e.g. if resizing images)
+        :return: GraspRectangles()
+        """
+        grs = []
+        with open(fname) as f:
+            for l in f:
+                x, y, theta, w, h = [float(v) for v in l[:-1].split(';')]
+                # index based on row, column (y,x), and the Jacquard dataset's angles are flipped around an axis.
+                grs.append(Grasp(np.array([y, x]), -theta / 180.0 * np.pi, w, h).as_gr)
+        grs = cls(grs)
+        grs.scale(scale)
+        return grs
+
+    def append(self, gr):
+        """
+        Add a grasp rectangle to this GraspRectangles object
+        :param gr: GraspRectangle
+        """
+        self.grs.append(gr)
+
+    def copy(self):
+        """
+        :return: A deep copy of this object and all of its GraspRectangles.
+        """
+        new_grs = GraspRectangles()
+        for gr in self.grs:
+            new_grs.append(gr.copy())
+        return new_grs
+
+    def show(self, ax=None, shape=None):
+        """
+        Draw all GraspRectangles on a matplotlib plot.
+        :param ax: (optional) existing axis
+        :param shape: (optional) Plot shape if no existing axis
+        """
+        if ax is None:
+            f = plt.figure()
+            ax = f.add_subplot(1, 1, 1)
+            ax.imshow(np.zeros(shape))
+            ax.axis([0, shape[1], shape[0], 0])
+            self.plot(ax)
+            plt.show()
+        else:
+            self.plot(ax)
+
+    def draw(self, shape, position=True, angle=True, width=True):
+        """
+        Plot all GraspRectangles as solid rectangles in a numpy array, e.g. as network training data.
+        :param shape: output shape
+        :param position: If True, Q output will be produced
+        :param angle: If True, Angle output will be produced
+        :param width: If True, Width output will be produced
+        :return: Q, Angle, Width outputs (or None)
+        """
+        if position:
+            pos_out = np.zeros(shape)
+        else:
+            pos_out = None
+        if angle:
+            ang_out = np.zeros(shape)
+        else:
+            ang_out = None
+        if width:
+            width_out = np.zeros(shape)
+        else:
+            width_out = None
+
+        for gr in self.grs:
+            rr, cc = gr.compact_polygon_coords(shape)
+            if position:
+                pos_out[rr, cc] = 1.0
+            if angle:
+                ang_out[rr, cc] = gr.angle
+            if width:
+                width_out[rr, cc] = gr.length
+
+        return pos_out, ang_out, width_out
+
+    def to_array(self, pad_to=0):
+        """
+        Convert all GraspRectangles to a single array.
+        :param pad_to: Length to 0-pad the array along the first dimension
+        :return: Nx4x2 numpy array
+        """
+        a = np.stack([gr.points for gr in self.grs])
+        if pad_to:
+            if pad_to > len(self.grs):
+                a = np.concatenate((a, np.zeros((pad_to - len(self.grs), 4, 2))))
+        return a.astype(np.int)
+
+    @property
+    def center(self):
+        """
+        Compute mean center of all GraspRectangles
+        :return: float, mean centre of all GraspRectangles
+        """
+        points = [gr.points for gr in self.grs]
+        return np.mean(np.vstack(points), axis=0).astype(np.int)
+
+
+class GraspRectangle:
+    """
+    Representation of a grasp in the common "Grasp Rectangle" format.
+    """
+
+    def __init__(self, points):
+        self.points = points
+
+    def __str__(self):
+        return str(self.points)
+
+    @property
+    def angle(self):
+        """
+        :return: Angle of the grasp to the horizontal.
+        """
+        dx = self.points[1, 1] - self.points[0, 1]
+        dy = self.points[1, 0] - self.points[0, 0]
+        return (np.arctan2(-dy, dx) + np.pi / 2) % np.pi - np.pi / 2
+
+    @property
+    def as_grasp(self):
+        """
+        :return: GraspRectangle converted to a Grasp
+        """
+        return Grasp(self.center, self.angle, self.length, self.width)
+
+    @property
+    def center(self):
+        """
+        :return: Rectangle center point
+        """
+        return self.points.mean(axis=0).astype(np.int)
+
+    @property
+    def length(self):
+        """
+        :return: Rectangle length (i.e. along the axis of the grasp)
+        """
+        dx = self.points[1, 1] - self.points[0, 1]
+        dy = self.points[1, 0] - self.points[0, 0]
+        return np.sqrt(dx ** 2 + dy ** 2)
+
+    @property
+    def width(self):
+        """
+        :return: Rectangle width (i.e. perpendicular to the axis of the grasp)
+        """
+        dy = self.points[2, 1] - self.points[1, 1]
+        dx = self.points[2, 0] - self.points[1, 0]
+        return np.sqrt(dx ** 2 + dy ** 2)
+
+    def polygon_coords(self, shape=None):
+        """
+        :param shape: Output Shape
+        :return: Indices of pixels within the grasp rectangle polygon.
+        """
+        return polygon(self.points[:, 0], self.points[:, 1], shape)
+
+    def compact_polygon_coords(self, shape=None):
+        """
+        :param shape: Output shape
+        :return: Indices of pixels within the centre thrid of the grasp rectangle.
+        """
+        return Grasp(self.center, self.angle, self.length / 3, self.width).as_gr.polygon_coords(shape)
+
+    def iou(self, gr, angle_threshold=np.pi / 6):
+        """
+        Compute IoU with another grasping rectangle
+        :param gr: GraspingRectangle to compare
+        :param angle_threshold: Maximum angle difference between GraspRectangles
+        :return: IoU between Grasp Rectangles
+        """
+        if abs((self.angle - gr.angle + np.pi / 2) % np.pi - np.pi / 2) > angle_threshold:
+            return 0
+
+        rr1, cc1 = self.polygon_coords()
+        rr2, cc2 = polygon(gr.points[:, 0], gr.points[:, 1])
+
+        try:
+            r_max = max(rr1.max(), rr2.max()) + 1
+            c_max = max(cc1.max(), cc2.max()) + 1
+        except:
+            return 0
+
+        canvas = np.zeros((r_max, c_max))
+        canvas[rr1, cc1] += 1
+        canvas[rr2, cc2] += 1
+        union = np.sum(canvas > 0)
+        if union == 0:
+            return 0
+        intersection = np.sum(canvas == 2)
+        return intersection / union
+
+    def copy(self):
+        """
+        :return: Copy of self.
+        """
+        return GraspRectangle(self.points.copy())
+
+    def offset(self, offset):
+        """
+        Offset grasp rectangle
+        :param offset: array [y, x] distance to offset
+        """
+        self.points += np.array(offset).reshape((1, 2))
+
+    def rotate(self, angle, center):
+        """
+        Rotate grasp rectangle
+        :param angle: Angle to rotate (in radians)
+        :param center: Point to rotate around (e.g. image center)
+        """
+        R = np.array(
+            [
+                [np.cos(-angle), np.sin(-angle)],
+                [-1 * np.sin(-angle), np.cos(-angle)],
+            ]
+        )
+        c = np.array(center).reshape((1, 2))
+        self.points = ((np.dot(R, (self.points - c).T)).T + c).astype(np.int)
+
+    def scale(self, factor):
+        """
+        :param factor: Scale grasp rectangle by factor
+        """
+        if factor == 1.0:
+            return
+        self.points *= factor
+
+    def plot(self, ax, color=None):
+        """
+        Plot grasping rectangle.
+        :param ax: Existing matplotlib axis
+        :param color: matplotlib color code (optional)
+        """
+        points = np.vstack((self.points, self.points[0]))
+        ax.plot(points[:, 1], points[:, 0], color=color)
+
+    def zoom(self, factor, center):
+        """
+        Zoom grasp rectangle by given factor.
+        :param factor: Zoom factor
+        :param center: Zoom zenter (focus point, e.g. image center)
+        """
+        T = np.array(
+            [
+                [1 / factor, 0],
+                [0, 1 / factor]
+            ]
+        )
+        c = np.array(center).reshape((1, 2))
+        self.points = ((np.dot(T, (self.points - c).T)).T + c).astype(np.int)
+
+
+class Grasp:
+    """
+    A Grasp represented by a center pixel, rotation angle and gripper width (length)
+    """
+
+    def __init__(self, center, angle, length=60, width=30):
+        self.center = center
+        self.angle = angle  # Positive angle means rotate anti-clockwise from horizontal.
+        self.length = length
+        self.width = width
+
+    @property
+    def as_gr(self):
+        """
+        Convert to GraspRectangle
+        :return: GraspRectangle representation of grasp.
+        """
+        xo = np.cos(self.angle)
+        yo = np.sin(self.angle)
+
+        y1 = self.center[0] + self.length / 2 * yo
+        x1 = self.center[1] - self.length / 2 * xo
+        y2 = self.center[0] - self.length / 2 * yo
+        x2 = self.center[1] + self.length / 2 * xo
+
+        return GraspRectangle(np.array(
+            [
+                [y1 - self.width / 2 * xo, x1 - self.width / 2 * yo],
+                [y2 - self.width / 2 * xo, x2 - self.width / 2 * yo],
+                [y2 + self.width / 2 * xo, x2 + self.width / 2 * yo],
+                [y1 + self.width / 2 * xo, x1 + self.width / 2 * yo],
+            ]
+        ).astype(np.float))
+
+    def max_iou(self, grs):
+        """
+        Return maximum IoU between self and a list of GraspRectangles
+        :param grs: List of GraspRectangles
+        :return: Maximum IoU with any of the GraspRectangles
+        """
+        self_gr = self.as_gr
+        max_iou = 0
+        for gr in grs:
+            iou = self_gr.iou(gr)
+            max_iou = max(max_iou, iou)
+        return max_iou
+
+    def plot(self, ax, color=None):
+        """
+        Plot Grasp
+        :param ax: Existing matplotlib axis
+        :param color: (optional) color
+        """
+        self.as_gr.plot(ax, color)
+
+    def to_jacquard(self, scale=1):
+        """
+        Output grasp in "Jacquard Dataset Format" (https://jacquard.liris.cnrs.fr/database.php)
+        :param scale: (optional) scale to apply to grasp
+        :return: string in Jacquard format
+        """
+        # Output in jacquard format.
+        return '%0.2f;%0.2f;%0.2f;%0.2f;%0.2f' % (
+            self.center[1] * scale, self.center[0] * scale, -1 * self.angle * 180 / np.pi, self.length * scale,
+            self.width * scale)
+
+
+def detect_grasps(q_img, ang_img, width_img=None, no_grasps=1):
+    """
+    Detect grasps in a network output.
+    :param q_img: Q image network output
+    :param ang_img: Angle image network output
+    :param width_img: (optional) Width image network output
+    :param no_grasps: Max number of grasps to return
+    :return: list of Grasps
+    """
+    local_max = peak_local_max(q_img, min_distance=20, threshold_abs=0.2, num_peaks=no_grasps)
+
+    grasps = []
+    for grasp_point_array in local_max:
+        grasp_point = tuple(grasp_point_array)
+
+        grasp_angle = ang_img[grasp_point]
+
+        g = Grasp(grasp_point, grasp_angle)
+        if width_img is not None:
+            g.length = width_img[grasp_point]
+            g.width = g.length / 2
+
+        grasps.append(g)
+
+    return grasps
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/dataset_processing/image.py OWG-main/robotic-grasping-master/utils/dataset_processing/image.py
--- OWG-upstream/robotic-grasping-master/utils/dataset_processing/image.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/dataset_processing/image.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,229 @@
+import warnings
+
+import cv2
+import matplotlib.pyplot as plt
+import numpy as np
+from imageio import imread
+from skimage.transform import rotate, resize
+
+warnings.filterwarnings("ignore", category=UserWarning)
+
+
+class Image:
+    """
+    Wrapper around an image with some convenient functions.
+    """
+
+    def __init__(self, img):
+        self.img = img
+
+    def __getattr__(self, attr):
+        # Pass along any other methods to the underlying ndarray
+        return getattr(self.img, attr)
+
+    @classmethod
+    def from_file(cls, fname):
+        return cls(imread(fname))
+
+    def copy(self):
+        """
+        :return: Copy of self.
+        """
+        return self.__class__(self.img.copy())
+
+    def crop(self, top_left, bottom_right, resize=None):
+        """
+        Crop the image to a bounding box given by top left and bottom right pixels.
+        :param top_left: tuple, top left pixel.
+        :param bottom_right: tuple, bottom right pixel
+        :param resize: If specified, resize the cropped image to this size
+        """
+        self.img = self.img[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]
+        if resize is not None:
+            self.resize(resize)
+
+    def cropped(self, *args, **kwargs):
+        """
+        :return: Cropped copy of the image.
+        """
+        i = self.copy()
+        i.crop(*args, **kwargs)
+        return i
+
+    def normalise(self):
+        """
+        Normalise the image by converting to float [0,1] and zero-centering
+        """
+        self.img = self.img.astype(np.float32) / 255.0
+        self.img -= self.img.mean()
+
+    def resize(self, shape):
+        """
+        Resize image to shape.
+        :param shape: New shape.
+        """
+        if self.img.shape == shape:
+            return
+        self.img = resize(self.img, shape, preserve_range=True).astype(self.img.dtype)
+
+    def resized(self, *args, **kwargs):
+        """
+        :return: Resized copy of the image.
+        """
+        i = self.copy()
+        i.resize(*args, **kwargs)
+        return i
+
+    def rotate(self, angle, center=None):
+        """
+        Rotate the image.
+        :param angle: Angle (in radians) to rotate by.
+        :param center: Center pixel to rotate if specified, otherwise image center is used.
+        """
+        if center is not None:
+            center = (center[1], center[0])
+        self.img = rotate(self.img, angle / np.pi * 180, center=center, mode='symmetric', preserve_range=True).astype(
+            self.img.dtype)
+
+    def rotated(self, *args, **kwargs):
+        """
+        :return: Rotated copy of image.
+        """
+        i = self.copy()
+        i.rotate(*args, **kwargs)
+        return i
+
+    def show(self, ax=None, **kwargs):
+        """
+        Plot the image
+        :param ax: Existing matplotlib axis (optional)
+        :param kwargs: kwargs to imshow
+        """
+        if ax:
+            ax.imshow(self.img, **kwargs)
+        else:
+            plt.imshow(self.img, **kwargs)
+            plt.show()
+
+    def zoom(self, factor):
+        """
+        "Zoom" the image by cropping and resizing.
+        :param factor: Factor to zoom by. e.g. 0.5 will keep the center 50% of the image.
+        """
+        sr = int(self.img.shape[0] * (1 - factor)) // 2
+        sc = int(self.img.shape[1] * (1 - factor)) // 2
+        orig_shape = self.img.shape
+        self.img = self.img[sr:self.img.shape[0] - sr, sc: self.img.shape[1] - sc].copy()
+        self.img = resize(self.img, orig_shape, mode='symmetric', preserve_range=True).astype(self.img.dtype)
+
+    def zoomed(self, *args, **kwargs):
+        """
+        :return: Zoomed copy of the image.
+        """
+        i = self.copy()
+        i.zoom(*args, **kwargs)
+        return i
+
+
+class DepthImage(Image):
+    def __init__(self, img):
+        super().__init__(img)
+
+    @classmethod
+    def from_pcd(cls, pcd_filename, shape, default_filler=0, index=None):
+        """
+            Create a depth image from an unstructured PCD file.
+            If index isn't specified, use euclidean distance, otherwise choose x/y/z=0/1/2
+        """
+        img = np.zeros(shape)
+        if default_filler != 0:
+            img += default_filler
+
+        with open(pcd_filename) as f:
+            for l in f.readlines():
+                ls = l.split()
+
+                if len(ls) != 5:
+                    # Not a point line in the file.
+                    continue
+                try:
+                    # Not a number, carry on.
+                    float(ls[0])
+                except ValueError:
+                    continue
+
+                i = int(ls[4])
+                r = i // shape[1]
+                c = i % shape[1]
+
+                if index is None:
+                    x = float(ls[0])
+                    y = float(ls[1])
+                    z = float(ls[2])
+
+                    img[r, c] = np.sqrt(x ** 2 + y ** 2 + z ** 2)
+
+                else:
+                    img[r, c] = float(ls[index])
+
+        return cls(img / 1000.0)
+
+    @classmethod
+    def from_tiff(cls, fname):
+        return cls(imread(fname))
+
+    def inpaint(self, missing_value=0):
+        """
+        Inpaint missing values in depth image.
+        :param missing_value: Value to fill in teh depth image.
+        """
+        # cv2 inpainting doesn't handle the border properly
+        # https://stackoverflow.com/questions/25974033/inpainting-depth-map-still-a-black-image-border
+        self.img = cv2.copyMakeBorder(self.img, 1, 1, 1, 1, cv2.BORDER_DEFAULT)
+        mask = (self.img == missing_value).astype(np.uint8)
+
+        # Scale to keep as float, but has to be in bounds -1:1 to keep opencv happy.
+        scale = np.abs(self.img).max()
+        self.img = self.img.astype(np.float32) / scale  # Has to be float32, 64 not supported.
+        self.img = cv2.inpaint(self.img, mask, 1, cv2.INPAINT_NS)
+
+        # Back to original size and value range.
+        self.img = self.img[1:-1, 1:-1]
+        self.img = self.img * scale
+
+    def gradients(self):
+        """
+        Compute gradients of the depth image using Sobel filtesr.
+        :return: Gradients in X direction, Gradients in Y diretion, Magnitude of XY gradients.
+        """
+        grad_x = cv2.Sobel(self.img, cv2.CV_64F, 1, 0, borderType=cv2.BORDER_DEFAULT)
+        grad_y = cv2.Sobel(self.img, cv2.CV_64F, 0, 1, borderType=cv2.BORDER_DEFAULT)
+        grad = np.sqrt(grad_x ** 2 + grad_y ** 2)
+
+        return DepthImage(grad_x), DepthImage(grad_y), DepthImage(grad)
+
+    def normalise(self):
+        """
+        Normalise by subtracting the mean and clippint [-1, 1]
+        """
+        self.img = np.clip((self.img - self.img.mean()), -1, 1)
+
+
+class WidthImage(Image):
+    """
+    A width image is one that describes the desired gripper width at each pixel.
+    """
+
+    def zoom(self, factor):
+        """
+        "Zoom" the image by cropping and resizing.  Also scales the width accordingly.
+        :param factor: Factor to zoom by. e.g. 0.5 will keep the center 50% of the image.
+        """
+        super().zoom(factor)
+        self.img = self.img / factor
+
+    def normalise(self):
+        """
+        Normalise by mapping [0, 150] -> [0, 1]
+        """
+        self.img = np.clip(self.img, 0, 150.0) / 150.0
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/get_cornell.sh OWG-main/robotic-grasping-master/utils/get_cornell.sh
--- OWG-upstream/robotic-grasping-master/utils/get_cornell.sh	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/get_cornell.sh	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,12 @@
+#!/usr/bin/env bash
+
+END=9
+for ((i=1;i<=END;i++)); do
+  wget --retry 5 http://pr.cs.cornell.edu/grasping/rect_data/temp/data0$i.tar.gz
+  tar xvzf data0$i.tar.gz
+  rm data0$i.tar.gz
+done
+
+wget --retry 5 http://pr.cs.cornell.edu/grasping/rect_data/temp/data10.tar.gz
+tar xvzf data10.tar.gz
+rm data10.tar.gz
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/get_jacquard.sh OWG-main/robotic-grasping-master/utils/get_jacquard.sh
--- OWG-upstream/robotic-grasping-master/utils/get_jacquard.sh	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/get_jacquard.sh	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+END=11
+for ((i=0;i<=END;i++)); do
+  wget --retry 5 https://jacquard.liris.cnrs.fr/data/Download/Jacquard_Dataset_$i.zip
+  unzip Jacquard_Dataset_$i.zip
+  rm Jacquard_Dataset_$i.zip
+done
\ No newline at end of file
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/timeit.py OWG-main/robotic-grasping-master/utils/timeit.py
--- OWG-upstream/robotic-grasping-master/utils/timeit.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/timeit.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,40 @@
+import time
+
+
+class TimeIt:
+    """
+    Print nested timing information.
+    """
+    print_output = True
+    last_parent = None
+    level = -1
+
+    def __init__(self, s):
+        self.s = s
+        self.t0 = None
+        self.t1 = None
+        self.outputs = []
+        self.parent = None
+
+    def __enter__(self):
+        self.t0 = time.time()
+        self.parent = TimeIt.last_parent
+        TimeIt.last_parent = self
+        TimeIt.level += 1
+
+    def __exit__(self, t, value, traceback):
+        self.t1 = time.time()
+        st = '%s%s: %0.1fms' % ('  ' * TimeIt.level, self.s, (self.t1 - self.t0) * 1000)
+        TimeIt.level -= 1
+
+        if self.parent:
+            self.parent.outputs.append(st)
+            self.parent.outputs += self.outputs
+        else:
+            if TimeIt.print_output:
+                print(st)
+                for o in self.outputs:
+                    print(o)
+            self.outputs = []
+
+        TimeIt.last_parent = self.parent
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/visualisation/gridshow.py OWG-main/robotic-grasping-master/utils/visualisation/gridshow.py
--- OWG-upstream/robotic-grasping-master/utils/visualisation/gridshow.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/visualisation/gridshow.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,60 @@
+import cv2
+import numpy as np
+
+
+def gridshow(name, imgs, scales, cmaps, width, border=10):
+    """
+    Display images in a grid.
+    :param name: cv2 Window Name to update
+    :param imgs: List of Images (np.ndarrays)
+    :param scales: The min/max scale of images to properly scale the colormaps
+    :param cmaps: List of cv2 Colormaps to apply
+    :param width: Number of images in a row
+    :param border: Border (pixels) between images.
+    """
+    imgrows = []
+    imgcols = []
+
+    maxh = 0
+    for i, (img, cmap, scale) in enumerate(zip(imgs, cmaps, scales)):
+
+        # Scale images into range 0-1
+        if scale is not None:
+            img = (np.clip(img, scale[0], scale[1]) - scale[0]) / (scale[1] - scale[0])
+        elif img.dtype == np.float:
+            img = (img - img.min()) / (img.max() - img.min() + 1e-6)
+
+        # Apply colormap (if applicable) and convert to uint8
+        if cmap is not None:
+            try:
+                imgc = cv2.applyColorMap((img * 255).astype(np.uint8), cmap)
+            except:
+                imgc = (img * 255.0).astype(np.uint8)
+        else:
+            imgc = img
+
+        if imgc.shape[0] == 3:
+            imgc = imgc.transpose((1, 2, 0))
+        elif imgc.shape[0] == 4:
+            imgc = imgc[1:, :, :].transpose((1, 2, 0))
+
+        # Arrange row of images.
+        maxh = max(maxh, imgc.shape[0])
+        imgcols.append(imgc)
+        if i > 0 and i % width == (width - 1):
+            imgrows.append(np.hstack(
+                [np.pad(c, ((0, maxh - c.shape[0]), (border // 2, border // 2), (0, 0)), mode='constant') for c in
+                 imgcols]))
+            imgcols = []
+            maxh = 0
+
+    # Unfinished row
+    if imgcols:
+        imgrows.append(np.hstack(
+            [np.pad(c, ((0, maxh - c.shape[0]), (border // 2, border // 2), (0, 0)), mode='constant') for c in
+             imgcols]))
+
+    maxw = max([c.shape[1] for c in imgrows])
+
+    cv2.imshow(name, np.vstack(
+        [np.pad(r, ((border // 2, border // 2), (0, maxw - r.shape[1]), (0, 0)), mode='constant') for r in imgrows]))
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/robotic-grasping-master/utils/visualisation/plot.py OWG-main/robotic-grasping-master/utils/visualisation/plot.py
--- OWG-upstream/robotic-grasping-master/utils/visualisation/plot.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/robotic-grasping-master/utils/visualisation/plot.py	2021-11-10 04:05:50.000000000 +0800
@@ -0,0 +1,195 @@
+import warnings
+from datetime import datetime
+
+import matplotlib.pyplot as plt
+import numpy as np
+
+from utils.dataset_processing.grasp import detect_grasps
+
+warnings.filterwarnings("ignore")
+
+
+def plot_results(
+        fig,
+        rgb_img,
+        grasp_q_img,
+        grasp_angle_img,
+        depth_img=None,
+        no_grasps=1,
+        grasp_width_img=None
+):
+    """
+    Plot the output of a network
+    :param fig: Figure to plot the output
+    :param rgb_img: RGB Image
+    :param depth_img: Depth Image
+    :param grasp_q_img: Q output of network
+    :param grasp_angle_img: Angle output of network
+    :param no_grasps: Maximum number of grasps to plot
+    :param grasp_width_img: (optional) Width output of network
+    :return:
+    """
+    gs = detect_grasps(grasp_q_img, grasp_angle_img, width_img=grasp_width_img, no_grasps=no_grasps)
+
+    plt.ion()
+    plt.clf()
+    ax = fig.add_subplot(2, 3, 1)
+    ax.imshow(rgb_img)
+    ax.set_title('RGB')
+    ax.axis('off')
+
+    if depth_img is not None:
+        ax = fig.add_subplot(2, 3, 2)
+        ax.imshow(depth_img, cmap='gray')
+        ax.set_title('Depth')
+        ax.axis('off')
+
+    ax = fig.add_subplot(2, 3, 3)
+    ax.imshow(rgb_img)
+    for g in gs:
+        g.plot(ax)
+    ax.set_title('Grasp')
+    ax.axis('off')
+
+    ax = fig.add_subplot(2, 3, 4)
+    plot = ax.imshow(grasp_q_img, cmap='jet', vmin=0, vmax=1)
+    ax.set_title('Q')
+    ax.axis('off')
+    plt.colorbar(plot)
+
+    ax = fig.add_subplot(2, 3, 5)
+    plot = ax.imshow(grasp_angle_img, cmap='hsv', vmin=-np.pi / 2, vmax=np.pi / 2)
+    ax.set_title('Angle')
+    ax.axis('off')
+    plt.colorbar(plot)
+
+    ax = fig.add_subplot(2, 3, 6)
+    plot = ax.imshow(grasp_width_img, cmap='jet', vmin=0, vmax=100)
+    ax.set_title('Width')
+    ax.axis('off')
+    plt.colorbar(plot)
+
+    plt.pause(0.1)
+    fig.canvas.draw()
+
+
+def plot_grasp(
+        fig,
+        grasps=None,
+        save=False,
+        rgb_img=None,
+        grasp_q_img=None,
+        grasp_angle_img=None,
+        no_grasps=1,
+        grasp_width_img=None
+):
+    """
+    Plot the output grasp of a network
+    :param fig: Figure to plot the output
+    :param grasps: grasp pose(s)
+    :param save: Bool for saving the plot
+    :param rgb_img: RGB Image
+    :param grasp_q_img: Q output of network
+    :param grasp_angle_img: Angle output of network
+    :param no_grasps: Maximum number of grasps to plot
+    :param grasp_width_img: (optional) Width output of network
+    :return:
+    """
+    if grasps is None:
+        grasps = detect_grasps(grasp_q_img, grasp_angle_img, width_img=grasp_width_img, no_grasps=no_grasps)
+
+    plt.ion()
+    plt.clf()
+
+    ax = plt.subplot(111)
+    ax.imshow(rgb_img)
+    for g in grasps:
+        g.plot(ax)
+    ax.set_title('Grasp')
+    ax.axis('off')
+
+    plt.pause(0.1)
+    fig.canvas.draw()
+
+    if save:
+        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
+        fig.savefig('results/{}.png'.format(time))
+
+
+def save_results(rgb_img, grasp_q_img, grasp_angle_img, depth_img=None, no_grasps=1, grasp_width_img=None):
+    """
+    Plot the output of a network
+    :param rgb_img: RGB Image
+    :param depth_img: Depth Image
+    :param grasp_q_img: Q output of network
+    :param grasp_angle_img: Angle output of network
+    :param no_grasps: Maximum number of grasps to plot
+    :param grasp_width_img: (optional) Width output of network
+    :return:
+    """
+    gs = detect_grasps(grasp_q_img, grasp_angle_img, width_img=grasp_width_img, no_grasps=no_grasps)
+
+    fig = plt.figure(figsize=(10, 10))
+    plt.ion()
+    plt.clf()
+    ax = plt.subplot(111)
+    ax.imshow(rgb_img)
+    ax.set_title('RGB')
+    ax.axis('off')
+    fig.savefig('results/rgb.png')
+
+    if depth_img.any():
+        fig = plt.figure(figsize=(10, 10))
+        plt.ion()
+        plt.clf()
+        ax = plt.subplot(111)
+        ax.imshow(depth_img, cmap='gray')
+        for g in gs:
+            g.plot(ax)
+        ax.set_title('Depth')
+        ax.axis('off')
+        fig.savefig('results/depth.png')
+
+    fig = plt.figure(figsize=(10, 10))
+    plt.ion()
+    plt.clf()
+    ax = plt.subplot(111)
+    ax.imshow(rgb_img)
+    for g in gs:
+        g.plot(ax)
+    ax.set_title('Grasp')
+    ax.axis('off')
+    fig.savefig('results/grasp.png')
+
+    fig = plt.figure(figsize=(10, 10))
+    plt.ion()
+    plt.clf()
+    ax = plt.subplot(111)
+    plot = ax.imshow(grasp_q_img, cmap='jet', vmin=0, vmax=1)
+    ax.set_title('Q')
+    ax.axis('off')
+    plt.colorbar(plot)
+    fig.savefig('results/quality.png')
+
+    fig = plt.figure(figsize=(10, 10))
+    plt.ion()
+    plt.clf()
+    ax = plt.subplot(111)
+    plot = ax.imshow(grasp_angle_img, cmap='hsv', vmin=-np.pi / 2, vmax=np.pi / 2)
+    ax.set_title('Angle')
+    ax.axis('off')
+    plt.colorbar(plot)
+    fig.savefig('results/angle.png')
+
+    fig = plt.figure(figsize=(10, 10))
+    plt.ion()
+    plt.clf()
+    ax = plt.subplot(111)
+    plot = ax.imshow(grasp_width_img, cmap='jet', vmin=0, vmax=100)
+    ax.set_title('Width')
+    ax.axis('off')
+    plt.colorbar(plot)
+    fig.savefig('results/width.png')
+
+    fig.canvas.draw()
+    plt.close(fig)
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/run_owg_ui.py OWG-main/run_owg_ui.py
--- OWG-upstream/run_owg_ui.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/run_owg_ui.py	2025-11-21 22:31:45.145742057 +0800
@@ -0,0 +1,8 @@
+# run_owg_ui.py
+from owg_robot.ui import RobotEnvUI
+
+if __name__ == "__main__":
+    # è¿™é‡Œæ¢æˆä½  config ç›®å½•é‡Œå®é™…å­˜åœ¨çš„ yaml æ–‡ä»¶
+    cfg_path = "./config/pyb/env.yaml"
+    ui = RobotEnvUI(cfg_path)
+    ui.run()
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/run_ycb_multiobj.sh OWG-main/run_ycb_multiobj.sh
--- OWG-upstream/run_ycb_multiobj.sh	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/run_ycb_multiobj.sh	2025-11-27 18:42:23.289436164 +0800
@@ -0,0 +1,59 @@
+#!/bin/bash
+set -e
+
+OBJ_LIST=objects_ycb.txt
+SUMMARY=results/summary_ycb.csv
+
+mkdir -p data/pc data/grasps_raw data/grasps_val results
+
+# å¯é€‰ï¼šæ¸…ç©ºæ—§ summary
+rm -f "$SUMMARY"
+
+while read oid urdf_path target_h; do
+  # è·³è¿‡æ³¨é‡Šè¡Œæˆ–ç©ºè¡Œ
+  [[ "$oid" =~ ^#.*$ ]] && continue
+  [[ -z "$oid" ]] && continue
+
+  echo "==== Object: $oid  ($urdf_path, target_h=$target_h) ===="
+
+  pc_path=data/pc/${oid}_$(echo $target_h | sed 's/0\.//').ply
+  grasps_raw=data/grasps_raw/${oid}_geom_K256.json
+  grasps_val=data/grasps_val/${oid}_geom_val.json
+
+  # 1) URDF -> ç‚¹äº‘
+  python grasp_6dof/tools/make_pc_from_urdf.py \
+    --urdf "$urdf_path" \
+    --out "$pc_path" \
+    --target-h "$target_h" \
+    --table-z -0.004
+
+  # 2) ç‚¹äº‘ -> å‡ ä½• grasp å€™é€‰
+  python grasp_6dof/grasp_gen_open3d.py \
+    --pc "$pc_path" \
+    --out "$grasps_raw" \
+    --topk 256 \
+    --yaw-bins 12 \
+    --offset-mm 2 8 \
+    --table-z -0.004 \
+    --seed 1
+
+  # 3) Panda ä»¿çœŸéªŒè¯ + å†™ summary
+  python grasp_6dof/validate_grasps_panda.py \
+    --obj "$urdf_path" \
+    --cube-scale "$target_h" \
+    --grasps "$grasps_raw" \
+    --out "$grasps_val" \
+    --topk 64 \
+    --fast \
+    --fast-scale 0.9 \
+    --reset-each-trial 1 \
+    --seed 123 \
+    --descent-step 0.0015 \
+    --descend-clear 0.020 \
+    --vel-close 0.8 \
+    --pos-close 700 \
+    --squeeze 0.8 \
+    --summary-csv "$SUMMARY"
+
+done < "$OBJ_LIST"
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/run_ycb_random.sh OWG-main/run_ycb_random.sh
--- OWG-upstream/run_ycb_random.sh	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/run_ycb_random.sh	2025-11-29 11:26:06.976281374 +0800
@@ -0,0 +1,45 @@
+#!/bin/bash
+set -e
+
+OBJ_LIST=objects_ycb.txt
+SUMMARY=results/summary_ycb_random.csv
+
+mkdir -p data/pc data/grasps_raw data/grasps_val results
+
+# å¯é€‰ï¼šæ¸…ç©ºæ—§ summary
+rm -f "$SUMMARY"
+
+while read oid urdf_path target_h; do
+  # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
+  [[ "$oid" =~ ^#.*$ ]] && continue
+  [[ -z "$oid" ]] && continue
+
+  echo "==== [RANDOM] Object: $oid  ($urdf_path, target_h=$target_h) ===="
+
+  grasps_random=data/grasps_raw/${oid}_random_K256.json
+  grasps_val_rand=data/grasps_val/${oid}_random_val.json
+
+  if [ ! -f "$grasps_random" ]; then
+    echo "[WARN] Random grasps file not found: $grasps_random, skip."
+    continue
+  fi
+
+  python grasp_6dof/validate_grasps_panda.py \
+    --obj "$urdf_path" \
+    --cube-scale "$target_h" \
+    --grasps "$grasps_random" \
+    --out "$grasps_val_rand" \
+    --topk 64 \
+    --fast \
+    --fast-scale 0.9 \
+    --reset-each-trial 1 \
+    --seed 321 \
+    --descent-step 0.0015 \
+    --descend-clear 0.020 \
+    --vel-close 0.8 \
+    --pos-close 700 \
+    --squeeze 0.8 \
+    --summary-csv "$SUMMARY"
+
+done < "$OBJ_LIST"
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/semantic_pc_from_masks.py OWG-main/semantic_pc_from_masks.py
--- OWG-upstream/semantic_pc_from_masks.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/semantic_pc_from_masks.py	2025-11-25 11:55:32.345930839 +0800
@@ -0,0 +1,196 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+"""
+semantic_pc_from_masks.py
+
+ç¦»çº¿è„šæœ¬ï¼šè¯»å– grounding æ—¥å¿— .npzï¼ˆimage + depth + masks + K [+ pose_cam] [+ query]ï¼‰ï¼Œ
+æŠŠé€‰ä¸­çš„ 2D è¯­ä¹‰ mask å›æŠ•æˆ 3D è¯­ä¹‰ç‚¹äº‘ï¼Œä¿å­˜åˆ° semantic_pc/ ç›®å½•ã€‚
+
+ç”¨æ³•ç¤ºä¾‹ï¼š
+    cd ~/OWG-main
+    python tools/semantic_pc_from_masks.py \
+        --log-dir logs/grounding_examples \
+        --out-dir semantic_pc \
+        --min-depth 0.1 \
+        --max-depth 2.0 \
+        --stride 2
+"""
+
+import os
+import argparse
+import glob
+import numpy as np
+
+
+def backproject_mask_to_points(mask, depth, K, pose_cam=None,
+                               min_depth=0.0, max_depth=np.inf, stride=1):
+    """
+    å°† 2D mask + depth å›æŠ•æˆ 3D ç‚¹äº‘ã€‚
+    mask: (H, W) bool
+    depth: (H, W) float32, ä»¥ç±³ä¸ºå•ä½
+    K: (3, 3) ç›¸æœºå†…å‚çŸ©é˜µ
+    pose_cam: (4, 4) å¯é€‰ï¼Œä¸–ç•Œåæ ‡ç³»ä¸‹çš„ç›¸æœºä½å§¿ï¼Œ
+              å¦‚æœæä¾›ï¼Œåˆ™æŠŠç‚¹ä»ç›¸æœºåæ ‡ç³»å˜æ¢åˆ°ä¸–ç•Œåæ ‡ç³»
+    min_depth, max_depth: æ·±åº¦èŒƒå›´è¿‡æ»¤
+    stride: ä¸‹é‡‡æ ·æ­¥é•¿ï¼Œå‡å°ç‚¹æ•°ï¼ˆä¾‹å¦‚ stride=2ï¼‰
+    """
+    H, W = depth.shape
+    assert mask.shape == depth.shape, "mask å’Œ depth å°ºå¯¸ä¸ä¸€è‡´"
+
+    # å–å‡º mask èŒƒå›´å†…çš„åƒç´ 
+    ys, xs = np.nonzero(mask)
+    if stride > 1:
+        ys = ys[::stride]
+        xs = xs[::stride]
+
+    d = depth[ys, xs]
+    # æ·±åº¦è¿‡æ»¤
+    valid = (d > min_depth) & (d < max_depth)
+    if not np.any(valid):
+        return np.zeros((0, 3), dtype=np.float32)
+
+    xs = xs[valid].astype(np.float32)
+    ys = ys[valid].astype(np.float32)
+    d = d[valid].astype(np.float32)
+
+    fx = K[0, 0]
+    fy = K[1, 1]
+    cx = K[0, 2]
+    cy = K[1, 2]
+
+    # åƒç´ åæ ‡ â†’ ç›¸æœºåæ ‡ç³»
+    # X_cam = ( (u-cx)*z/fx, (v-cy)*z/fy, z )
+    X = (xs - cx) * d / fx
+    Y = (ys - cy) * d / fy
+    Z = d
+
+    pts_cam = np.stack([X, Y, Z], axis=1)  # (N, 3)
+
+    if pose_cam is None:
+        return pts_cam.astype(np.float32)
+
+    # æ‰©å±•æˆé½æ¬¡åæ ‡ï¼Œå¹¶ç”¨ pose_cam (4x4) å˜æ¢åˆ°ä¸–ç•Œåæ ‡
+    R = pose_cam[:3, :3]
+    t = pose_cam[:3, 3]
+    pts_world = (R @ pts_cam.T + t.reshape(3, 1)).T  # (N, 3)
+    return pts_world.astype(np.float32)
+
+
+def process_file(path, out_dir, args):
+    """
+    å¤„ç†å•ä¸ª grounding .npz æ–‡ä»¶ï¼Œç”Ÿæˆé€‰ä¸­å¯¹è±¡çš„ 3D ç‚¹äº‘å¹¶ä¿å­˜ã€‚
+    """
+    data = np.load(path, allow_pickle=True)
+    base = os.path.basename(path).replace(".npz", "")
+
+    image = data["image"]             # HxWx3
+    depth = data["depth"]             # HxW
+    masks = data["masks"]             # NxHxW
+    labels = data["labels"]           # (N,)
+    selected_ids = data["selected_ids"]  # (M,)
+
+    # âœ… 1) å¦‚æœæœ‰çœŸå® Kï¼Œå°±ç”¨çœŸå®çš„ï¼›å¦åˆ™æ„é€ ä¸€ä¸ªé»˜è®¤ K
+    if "K" in data.files:
+        K = data["K"]
+    else:
+        H, W = depth.shape
+        # å‡è®¾æ°´å¹³ FOV = 60Â°ï¼Œæ„é€  pinhole intrinsics
+        fov_deg = 60.0
+        f = W / (2.0 * np.tan(np.deg2rad(fov_deg) / 2.0))  # fx = fy = f
+        cx, cy = W / 2.0, H / 2.0
+        K = np.array([
+            [f, 0, cx],
+            [0, f, cy],
+            [0, 0, 1],
+        ], dtype=np.float32)
+        print(f"[WARN] {path} has no K, using default intrinsics with fov={fov_deg}Â°")
+
+    pose_cam = data["pose_cam"] if "pose_cam" in data.files else None
+    query = str(data["query"]) if "query" in data.files else ""
+
+    H, W, _ = image.shape
+    N = masks.shape[0]
+
+    print(f"[INFO] Processing {base}: image={H}x{W}, N_masks={N}, selected_ids={selected_ids}")
+
+    # ä¸ºäº†ç®€å•ï¼šåªå¤„ç† selected_ids ä¸­æ¯ä¸ª id å¯¹åº”çš„ mask
+    pcs = []
+    metas = []
+
+    for sid in selected_ids:
+        # æ‰¾åˆ° label == sid çš„é‚£ä¸€ä¸ª maskï¼ˆæœ‰å¯èƒ½æ²¡æœ‰ï¼Œåšä¸ªé˜²å¾¡ï¼‰
+        idxs = np.where(labels == sid)[0]
+        if len(idxs) == 0:
+            print(f"  âš ï¸ selected id {sid} not found in labels {labels}")
+            continue
+        idx = idxs[0]
+        mask = masks[idx].astype(bool)
+
+        pts = backproject_mask_to_points(
+            mask,
+            depth,
+            K,
+            pose_cam=pose_cam,
+            min_depth=args.min_depth,
+            max_depth=args.max_depth,
+            stride=args.stride,
+        )
+        if pts.shape[0] == 0:
+            print(f"  âš ï¸ No valid points for selected id {sid}")
+            continue
+
+        pcs.append(pts)
+        metas.append({
+            "sid": int(sid),
+            "label_index": int(idx),
+        })
+
+    if not pcs:
+        print(f"[WARN] No point cloud generated for {base}, skip.")
+        return
+
+    # æ‹¼æˆä¸€ä¸ªå¤§çš„ç‚¹äº‘ï¼Œæˆ–ä¹Ÿå¯ä»¥åˆ†å¼€ä¿å­˜
+    pc_all = np.concatenate(pcs, axis=0)  # (M_total, 3)
+
+    os.makedirs(out_dir, exist_ok=True)
+    out_path = os.path.join(out_dir, base + "_semantic_pc.npz")
+    np.savez_compressed(
+        out_path,
+        pc=pc_all,          # æ‰€æœ‰é€‰ä¸­ç›®æ ‡çš„ 3D ç‚¹
+        query=query,
+        metas=np.array(metas, dtype=object),
+    )
+    print(f"[OK] Saved semantic point cloud â†’ {out_path}, N_pts={pc_all.shape[0]}")
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--log-dir", type=str, default="logs/grounding_examples",
+                        help="grounding .npz æ—¥å¿—æ‰€åœ¨ç›®å½•")
+    parser.add_argument("--out-dir", type=str, default="semantic_pc",
+                        help="è¾“å‡º 3D è¯­ä¹‰ç‚¹äº‘ä¿å­˜ç›®å½•")
+    parser.add_argument("--min-depth", type=float, default=0.05,
+                        help="æœ€å°æœ‰æ•ˆæ·±åº¦ï¼ˆç±³ï¼‰")
+    parser.add_argument("--max-depth", type=float, default=2.0,
+                        help="æœ€å¤§æœ‰æ•ˆæ·±åº¦ï¼ˆç±³ï¼‰")
+    parser.add_argument("--stride", type=int, default=1,
+                        help="åƒç´ ä¸‹é‡‡æ ·æ­¥é•¿ï¼ˆ>1 å¯å‡å°‘ç‚¹æ•°ï¼‰")
+
+    args = parser.parse_args()
+
+    paths = sorted(glob.glob(os.path.join(args.log_dir, "*.npz")))
+    if not paths:
+        print(f"[WARN] No .npz found in {args.log_dir}")
+        return
+
+    print(f"[INFO] Found {len(paths)} grounding logs in {args.log_dir}")
+    for p in paths:
+        try:
+            process_file(p, args.out_dir, args)
+        except Exception as e:
+            print(f"[ERROR] Failed to process {p}: {e}")
+
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/train_lggsn.py OWG-main/train_lggsn.py
--- OWG-upstream/train_lggsn.py	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/train_lggsn.py	2025-11-23 16:11:43.480763355 +0800
@@ -0,0 +1,147 @@
+import pandas as pd
+import torch
+from torch.utils.data import Dataset, DataLoader
+from sklearn.model_selection import train_test_split
+from lggsn_model import LGGSN  # ä»ç„¶å¤ç”¨ä½ åŸæ¥çš„æ¨¡å‹
+
+CSV_PATH = "grasp_6dof/dataset/all_lggsn.csv"
+
+
+class LGGSNDataset(Dataset):
+    """
+    å‡ ä½• + è´¨é‡ç‰¹å¾ â†’ æŠ“å–å¥½å(label) çš„æ•°æ®é›†ã€‚
+
+    è¿™é‡Œä¸å†ä½¿ç”¨çœŸå® queryï¼Œè€Œæ˜¯ç»™ä¸€ä¸ªå›ºå®šçš„ dummy query id = 0ï¼Œ
+    è¿™æ ·å¯ä»¥æ²¿ç”¨ LGGSN(geom, query_id) è¿™ä¸ªæ¥å£ã€‚
+    """
+    def __init__(self, df, feature_cols):
+        self.df = df.reset_index(drop=True)
+        self.feature_cols = feature_cols
+
+    def __len__(self):
+        return len(self.df)
+
+    def __getitem__(self, idx):
+        row = self.df.iloc[idx]
+
+        geom = torch.tensor(
+            row[self.feature_cols].values.astype("float32"),
+            dtype=torch.float32,
+        )
+        # å•ä¸€è™šæ‹Ÿ queryï¼šå…¨æ˜¯ 0
+        q_id = torch.tensor(0, dtype=torch.long)
+
+        # label åˆ—ï¼š0 = bad grasp, 1 = good grasp
+        y = torch.tensor(float(row["label"]), dtype=torch.float32)
+
+        return geom, q_id, y
+
+
+def main():
+    # 1) è¯»å– CSV
+    df = pd.read_csv(CSV_PATH)
+
+    if "label" not in df.columns:
+        raise ValueError(
+            "CSV é‡Œæ‰¾ä¸åˆ° 'label' åˆ—ï¼Œè¯·ç¡®è®¤ convert_to_lggsn.py å·²ç»ç”Ÿæˆ labelã€‚"
+        )
+
+    print("Label distribution from CSV:", df["label"].value_counts().to_dict())
+
+    # å‡ ä½• + è´¨é‡ç‰¹å¾åˆ—ï¼Œæ ¹æ®ä½ çš„ all_lggsn.csv æ¥é€‰
+    feature_cols = [
+        "x", "y", "z",
+        "roll", "pitch", "yaw",
+        "width", "score",
+        "dz", "dz_lift", "need_dz", "H",
+    ]
+    for c in feature_cols:
+        if c not in df.columns:
+            raise ValueError(f"CSV é‡Œç¼ºå°‘ç‰¹å¾åˆ—: {c}")
+
+    # 2) åˆ¤æ–­èƒ½ä¸èƒ½åšåˆ†å±‚åˆ’åˆ†
+    class_counts = df["label"].value_counts()
+    if len(class_counts) < 2 or class_counts.min() < 2:
+        print("[WARN] è‡³å°‘æœ‰ä¸€ä¸ªç±»åˆ«æ ·æœ¬æ•° < 2ï¼Œä¸èƒ½åš stratified splitï¼Œå°†ä¸ä½¿ç”¨ stratifyã€‚")
+        stratify = None
+    else:
+        stratify = df["label"]
+
+    # 3) æŒ‰è¡Œæ‹†åˆ† train / valï¼Œç„¶åæ„å»º Dataset
+    train_df, val_df = train_test_split(
+        df,
+        test_size=0.2,
+        random_state=42,
+        stratify=stratify,
+    )
+
+    train_ds = LGGSNDataset(train_df, feature_cols)
+    val_ds = LGGSNDataset(val_df, feature_cols)
+
+    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
+    val_loader = DataLoader(val_ds, batch_size=8, shuffle=False)
+
+    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    print("Using device:", device)
+
+    # 4) æ„å»ºæ¨¡å‹
+    # æˆ‘ä»¬åªæœ‰ 1 ä¸ª dummy queryï¼Œæ‰€ä»¥ n_queries = 1
+    model = LGGSN(
+        n_queries=1,
+        geom_dim=len(feature_cols),  # æ ¹æ®å½“å‰ç‰¹å¾åˆ—è‡ªåŠ¨è®¾ç½®
+        query_dim=0,                 # å…ˆä¸ç”¨ query embedding
+        hidden_dim=40,               # ä½ ä¹Ÿå¯ä»¥æ”¹å¤§ä¸€ç‚¹ï¼Œæ¯”å¦‚ 64
+    ).to(device)
+    optim = torch.optim.Adam(model.parameters(), lr=1e-3)
+    criterion = torch.nn.BCEWithLogitsLoss()
+
+    def run_epoch(loader, train=True):
+        if train:
+            model.train()
+        else:
+            model.eval()
+
+        total_loss, total_correct, total_n = 0.0, 0, 0
+
+        for geom, q_id, y in loader:
+            geom = geom.to(device)
+            q_id = q_id.to(device)
+            y = y.to(device)
+
+            with torch.set_grad_enabled(train):
+                logit = model(geom, q_id).squeeze(-1)  # [B] or [B,1] â†’ [B]
+                loss = criterion(logit, y)
+                if train:
+                    optim.zero_grad()
+                    loss.backward()
+                    optim.step()
+
+            total_loss += loss.item() * y.size(0)
+            pred = (torch.sigmoid(logit) > 0.5).float()
+            total_correct += (pred == y).sum().item()
+            total_n += y.size(0)
+
+        avg_loss = total_loss / max(total_n, 1)
+        avg_acc = total_correct / max(total_n, 1)
+        return avg_loss, avg_acc
+
+    # 5) è®­ç»ƒè‹¥å¹²ä¸ª epoch
+    for epoch in range(20):
+        tr_loss, tr_acc = run_epoch(train_loader, train=True)
+        va_loss, va_acc = run_epoch(val_loader, train=False)
+        print(
+            f"Epoch {epoch:02d} | "
+            f"train loss={tr_loss:.4f} acc={tr_acc:.2f} | "
+            f"val  loss={va_loss:.4f} acc={va_acc:.2f}"
+        )
+    # è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹æƒé‡
+    ckpt_path = "grasp_6dof/models/lggsn_geom_only.pt"
+    import os
+    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)
+    torch.save(model.state_dict(), ckpt_path)
+    print(f"Saved LGGSN checkpoint to {ckpt_path}")
+
+
+if __name__ == "__main__":
+    main()
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' OWG-upstream/tune_params_ycb.sh OWG-main/tune_params_ycb.sh
--- OWG-upstream/tune_params_ycb.sh	1970-01-01 07:30:00.000000000 +0730
+++ OWG-main/tune_params_ycb.sh	2025-11-29 15:24:16.991231236 +0800
@@ -0,0 +1,65 @@
+#!/bin/bash
+set -e
+
+OBJ_LIST=objects_ycb.txt
+SUMMARY=results/summary_ycb_tune.csv
+
+mkdir -p results
+
+# å¯é€‰ï¼šå…ˆæ¸…ç©ºæ—§çš„ tune summary
+rm -f "$SUMMARY"
+
+# å‚æ•°ç½‘æ ¼ï¼šæ¯ä¸ªå­—ç¬¦ä¸²æ˜¯ "descent_step vel_close squeeze"
+PARAMS=(
+  "0.0010 0.6 0.6"
+  "0.0010 0.6 0.8"
+  "0.0010 0.8 0.6"
+  "0.0010 0.8 0.8"
+  "0.0015 0.6 0.6"
+  "0.0015 0.6 0.8"
+  "0.0015 0.8 0.6"
+  "0.0015 0.8 0.8"   # è¿™ä¸€ç»„å°±æ˜¯å½“å‰é»˜è®¤å‚æ•°
+)
+
+while read oid urdf_path target_h; do
+  # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
+  [[ "$oid" =~ ^#.*$ ]] && continue
+  [[ -z "$oid" ]] && continue
+
+  echo "==== [TUNE] Object: $oid  ($urdf_path, target_h=$target_h) ===="
+
+  grasps_raw=data/grasps_raw/${oid}_geom_K256.json
+  if [ ! -f "$grasps_raw" ]; then
+    echo "[WARN] Geometric grasps not found: $grasps_raw, skip."
+    continue
+  fi
+
+  for p in "${PARAMS[@]}"; do
+    # æ‹†å‡ºä¸‰ä¸ªå‚æ•°
+    read ds vc sq <<< "$p"
+    echo "  -> ds=$ds, vel_close=$vc, squeeze=$sq"
+
+    # ä¸ºé¿å…è¦†ç›–ç»“æœï¼Œout æ–‡ä»¶åå¸¦ä¸Šå‚æ•°
+    grasps_val=data/grasps_val/${oid}_ds${ds}_vc${vc}_sq${sq}_val.json
+
+    python grasp_6dof/validate_grasps_panda.py \
+      --obj "$urdf_path" \
+      --cube-scale "$target_h" \
+      --grasps "$grasps_raw" \
+      --out "$grasps_val" \
+      --topk 32 \
+      --fast \
+      --fast-scale 0.9 \
+      --reset-each-trial 1 \
+      --seed 456 \
+      --descent-step "$ds" \
+      --descend-clear 0.020 \
+      --vel-close "$vc" \
+      --pos-close 700 \
+      --squeeze "$sq" \
+      --summary-csv "$SUMMARY"
+
+  done
+
+done < "$OBJ_LIST"
+
diff -ruN '--exclude=.git' '--exclude=grasp_6dof' '--exclude=scripts' '--exclude=data' '--exclude=logs' '--exclude=results' '--exclude=semantic_pc' '--exclude=YCB_Dataset' '--exclude=third_party' '--exclude=notebooks' '--exclude=media' '--exclude=__pycache__' '--exclude=*.pyc' '--exclude=*.ipynb' "OWG-upstream/\345\221\275\344\273\244\346\214\207\347\244\272" "OWG-main/\345\221\275\344\273\244\346\214\207\347\244\272"
--- "OWG-upstream/\345\221\275\344\273\244\346\214\207\347\244\272"	1970-01-01 07:30:00.000000000 +0730
+++ "OWG-main/\345\221\275\344\273\244\346\214\207\347\244\272"	2025-11-25 12:46:12.537848658 +0800
@@ -0,0 +1,157 @@
+
+
+
+1. The tennis ball is marked with ID 6.
+2. The hammer is marked with ID 11.
+3. The blue container is marked with ID 8.
+4. The green clamp is marked with ID 13.
+5. The red and white eraser is marked with ID 14.
+6. The can of Pringles is marked with ID 16.
+7. The scissors are marked with ID 15.
+8. The mustard bottle is marked with ID 10.
+9. The Campbell's soup can is marked with ID 9.
+10. The box of Cheez-It is marked with ID 12.
+11. The rectangular tin is marked with ID 7.
+
+
+
+
+5
+
+
+
+
+ID 12 â€“ box of Cheez-It
+
+âœ… the box of Cheez-It
+
+âœ… the red box of crackers
+
+the cracker box
+
+the red snack box
+
+the box of cheese crackers
+
+ID 7 â€“ rectangular tinï¼ˆç»¿è‰²é“ç›’ï¼‰
+
+âœ… the rectangular tin
+
+âœ… the green tin box
+
+the small green tin
+
+the green metal box
+
+the green rectangular box
+ID 6 â€“ tennis ball
+
+âœ… tennis ball
+
+âœ… the green tennis ball
+
+the small tennis ball
+
+the round tennis ball
+
+the green ball
+
+the tennis ball on the table
+
+ID 11 â€“ hammer
+
+âœ… hammer
+
+âœ… the hammer with wooden handle
+
+the hammer on the left
+
+the metal hammer
+
+the tool for hitting nails
+
+ID 8 â€“ blue containerï¼ˆè“è‰²åœ†ç½ï¼‰
+
+âœ… the blue can
+
+âœ… blue container
+
+the blue round can
+
+the blue cylinder
+
+the blue soda can
+
+ID 13 â€“ green clampï¼ˆç»¿è‰²å¤¹å­ï¼‰
+
+âœ… the clamp
+
+âœ… the green clamp
+
+the green clip
+
+the green tool for gripping
+
+the small green clamp in the middle
+
+ID 14 â€“ red and white eraser
+
+âœ… the red and white eraser
+
+âœ… the eraser
+
+the red and white rectangular eraser
+
+the eraser on the table
+
+the small red and white block
+
+ID 16 â€“ can of Pringlesï¼ˆè–¯ç‰‡ç­’ï¼‰
+
+âœ… the can of Pringles
+
+âœ… the tube of chips
+
+the red chips can
+
+the Pringles can
+
+the tall red snack can
+
+ID 15 â€“ scissors
+
+âœ… the scissors
+
+âœ… the pair of scissors
+
+the gray scissors
+
+the scissors on the table
+
+the cutting tool
+
+ID 10 â€“ mustard bottleï¼ˆé»„èŠ¥æœ«ç“¶ï¼‰
+
+âœ… the mustard bottle
+
+âœ… the yellow bottle
+
+the yellow squeeze bottle
+
+the yellow condiment bottle
+
+the yellow plastic bottle
+
+ID 9 â€“ Campbellâ€™s soup canï¼ˆç•ªèŒ„æ±¤ç½ï¼‰
+
+âœ… campbell's soup can
+
+âœ… the soup can
+
+the red and white soup can
+
+the can of tomato soup
+
+the Campbell's can
+
+
